% Encoding: UTF-8
Scopus
EXPORT DATE: 10 May 2021

@Article{Rani2021,
  author          = {Rani, S. and Suri, B.},
  journal         = {International Journal of Software Engineering and Knowledge Engineering},
  title           = {Investigating Different Metrics for Evaluation and Selection of Mutation Operators for Java},
  year            = {2021},
  note            = {cited By 0},
  number          = {3},
  pages           = {311-336},
  volume          = {31},
  abstract        = {Mutation testing is a successful and powerful technique, specifically designed for injecting the artificial faults. Although it is effective at revealing the faults, test suite assessment and its reduction, however, suffer from the expense of executing a large number of mutants. The researchers have proposed different types of cost reduction techniques in the literature. These techniques highly depend on the inspection of mutation operators. Several metrics have been evolved for the same. The selective mutation technique is most frequently used by the researchers. In this paper, the authors investigate different metrics for evaluating the traditional mutation operators for Java. Results on 13 Java programs indicate how grouping few operators can impact the effectiveness of an adequate and minimal test suite, and how this could provide several cost benefits. © 2021 World Scientific Publishing Company.},
  author_keywords = {cost-benefit analysis; mutation operator metrics; Mutation testing; resistant mutant; selective mutation; weak mutant},
  doi             = {10.1142/S021819402150011X},
  keywords        = {Computer software; Cost reduction, Cost benefits; Java program; Mutation operators; Mutation testing; Selective mutation, Java programming language},
  url             = {https://doi.org/10.1142/S021819402150011X},
}

@Article{Sykora2021,
  author          = {Sykora, K. and Ahmed, B.S. and Bures, M.},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title           = {Code Coverage Aware Test Generation Using Constraint Solver},
  year            = {2021},
  note            = {cited By 0},
  pages           = {58-66},
  volume          = {12524 LNCS},
  abstract        = {Code coverage has been used in the software testing context mostly as a metric to assess a generated test suite’s quality. Recently, code coverage analysis is used as a white-box testing technique for test optimization. Most of the research activities focus on using code coverage for test prioritization and selection within automated testing strategies. Less effort has been paid in the literature to use code coverage for test generation. This paper introduces a new Code Coverage-based Test Case Generation (CCTG) concept that changes the current practices by utilizing the code coverage analysis in the test generation process. CCTG uses the code coverage data to calculate the input parameters’ impact for a constraint solver to automate the generation of effective test suites. We applied this approach to a few real-world case studies. The results showed that the new test generation approach could generate effective test cases and detect new faults. © 2021, Springer Nature Switzerland AG.},
  author_keywords = {Automated test generation; Code coverage; Constrained interaction testing; Software testing; Test case augmentation},
  doi             = {10.1007/978-3-030-67220-1_5},
  keywords        = {Application programs; Embedded systems; Formal methods; Logic programming, Automated testing; Constraint solvers; Current practices; Research activities; Test case generation; Test optimization; Test prioritization; White-box testing, Software testing},
  url             = {https://doi.org/10.1007/978-3-030-67220-1_5},
}

@Conference{Virginio2020,
  author          = {Virginio, T. and Martins, L. and Rocha, L. and Santana, R. and Cruz, A. and Costa, H. and Machado, I.},
  title           = {JNose: Java Test Smell Detector},
  year            = {2020},
  note            = {cited By 0},
  pages           = {564-569},
  abstract        = {Several strategies have been proposed for test quality measurement and analysis. Code coverage is likely the most widely used one. It enables to verify the ability of a test case to cover as many source code branches as possible. Although code coverage has been widely used, novel strategies have been recently employed. It is the case of test smells analysis, which has been introduced as an affordable strategy to evaluate the quality of test code. Test smells are poor design choices in implementation, and their occurrence in test code might reduce the quality of test suites. Test smells identification is clearly dependent on tool support, otherwise it could become a cost-ineffective strategy. However, as far as we know, there is no tool that combines code coverage and test smells to address test quality measurement. In this work, we present the JNose Test, a tool aimed to analyze test suite quality in the perspective of test smells. JNose Test detects code coverage and software evolution metrics and a set of test smells throughout software versions. © 2020 ACM.},
  author_keywords = {Code Coverage; Quality of Tests; Test Smells; Test Suite Evolution},
  doi             = {10.1145/3422392.3422499},
  journal         = {ACM International Conference Proceeding Series},
  keywords        = {Java programming language; Odors; Quality control, Code coverage; Novel strategies; Software Evolution; Software versions; Source codes; Test case; Test quality; Tool support, Software testing},
  url             = {https://dl.acm.org/doi/10.1145/3422392.3422499},
}

@Article{Andrade2020,
  author          = {Andrade, L. and Machado, P. and Andrade, W.},
  journal         = {Software Testing Verification and Reliability},
  title           = {Can operational profile coverage explain post-release bug detection?},
  year            = {2020},
  note            = {cited By 0},
  number          = {4-5},
  volume          = {30},
  abstract        = {To deliver reliable software, developers may rely on the fault detection capability of test suites. To evaluate this capability, they can apply code coverage metrics before a software release. However, recent research results have shown that these metrics may not provide a solid basis for this evaluation. Moreover, the fixing of a fault has a cost, and not all faults have the same impact regarding software reliability. In this sense, operational testing aims at assessing parts of the system that are more valuable for users. The goal of this work is to investigate whether traditional code coverage and code coverage merged with operational information can be related to post-release bug detection. We focus on the scope of proprietary software under continuous delivery. We performed an exploratory case study where code branch and statement coverage metrics were collected for each version of a proprietary software together with real usage data of the system. We then measured the ability to explain the bug-fixing activity after version release using code coverage levels. We found that traditional statement coverage has a moderate negative correlation with bug-fixing activities, whereas statement coverage merged with the operational profile has a large negative correlation with higher confidence. Developers can consider operational information as an important factor of influence that should be analysed, among other factors, together with code coverage to assess the fault detection capability of a test suite. © 2020 John Wiley & Sons, Ltd.},
  art_number      = {e1735},
  author_keywords = {fault detection capability; operational profile; test coverage},
  doi             = {10.1002/stvr.1735},
  keywords        = {Codes (symbols); Fault detection; Software reliability, Detection capability; Exploratory case studies; Negative correlation; Operational profile; Operational testing; Proprietary software; Recent researches; Statement coverage, Software testing},
  url             = {https://doi.org/10.1002/stvr.1735},
}

@Conference{Krotkov2020,
  author          = {Krotkov, V. and Danilenko, A.},
  title           = {Experience of creating a library for testing c# and c++ console applications},
  year            = {2020},
  note            = {cited By 0},
  pages           = {280-283},
  volume          = {2667},
  abstract        = {The present work provides a description of functionality of library aimed at simplifying creation of console applications in and C++ languages by granting opportunities to use built-in adjusted menu of any level of enclosure, special means for control over variables of the program and tracking of a condition of program entities. The library contains functions for manual and automatic testing and the multilevel analysis of console application's performance. The library also includes auxiliary functionality of random or sample generation of user-defined or standardtyped test data and gives opportunities for exact identification of mistakes made by programmer during development. The library is powered with modern technologies of object-oriented programming and developed according to the advanced architectural and algorithmic concepts. Copyright © 2020 for this paper by its authors.},
  author_keywords = {C#; C++; Console applications; Data generation; Library of tools; Manual and automatic testing; Parser},
  journal         = {CEUR Workshop Proceedings},
  keywords        = {Application programs; Automatic testing; C++ (programming language); Data Science; Nanotechnology, C++ language; Modern technologies; Multi-level analysis; Sample generations; Test data, Object oriented programming},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092240595&partnerID=40&md5=23ff88b8a725b238efa9cbb3acf404fb},
}

@Article{Magalhaes2020,
  author          = {Magalhães, C. and Andrade, J. and Perrusi, L. and Mota, A. and Barros, F. and Maia, E.},
  journal         = {Journal of Systems and Software},
  title           = {HSP: A hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study},
  year            = {2020},
  note            = {cited By 0},
  volume          = {159},
  abstract        = {The usual way to guarantee quality of software products is via testing. This paper presents a novel strategy for selection and prioritisation of Test Cases (TC) for Regression testing. In the lack of code artifacts from where to derive Test Plans, this work uses information conveyed by textual documents maintained by Industry, such as Change Requests. The proposed process is based on Information Retrieval techniques combined with indirect code coverage measures to select and prioritise TCs. The aim is to provide a high coverage Test Plan which would maximise the number of bugs found. This process was implemented as a prototype tool which was used in a case study with our industrial partner (Motorola Mobility). Experiments results revealed that the combined strategy provides better results than the use of information retrieval and code coverage independently. Yet, it is worth mentioning that any of these automated options performed better than the previous manual process deployed by our industrial partner to create test plans. © 2019},
  art_number      = {110430},
  author_keywords = {Code coverage; Information retrieval; Regression testing; Static analysis; Test cases selection and prioritisation},
  doi             = {10.1016/j.jss.2019.110430},
  keywords        = {Codes (symbols); Information retrieval; Information use; Regression analysis; Static analysis; Testing, Code coverage; Industrial case study; Industrial partners; Novel strategies; Quality of softwares; Regression testing; Selection and prioritisation; Textual documents, Software testing},
  url             = {https://doi.org/10.1016/j.jss.2019.110430},
}

@Article{GomezAbajo2020,
  author          = {Gómez-Abajo, P. and Guerra, E. and de Lara, J. and Merayo, M.G.},
  journal         = {Journal of Object Technology},
  title           = {Systematic Engineering of Mutation Operators},
  year            = {2020},
  note            = {cited By 0},
  number          = {3},
  pages           = {1-15},
  volume          = {19},
  abstract        = {In the context of software engineering, mutation consists in injecting small changes in artefacts – like models, programs, or data – for purposes like (mutation) testing, test data generation, and all sorts of search-based methods. These tasks typically require defining sets of mutation operators, which are often built ad-hoc because there is currently poor support for their development and testing. To improve this situation, we propose a methodology and corresponding tool support for the proper engineering of mutation operators. Our proposal is model-based, representing the artefacts to be mutated as models. It includes a domain-specific language to describe the mutation operators, facilities to synthesize models that can be used to test the operators, different metrics to analyse operator coverage, and services to generate operators when the coverage is insufficient. We show automated support atop the WODEL tool, and illustrate its use by defining mutation operators for UML Class Diagrams. © 2020, Journal of Object Technology. All Rights Reserved.},
  author_keywords = {class diagrams; metrics; model mutation; model synthesis; Model-driven engineering; WODEL},
  doi             = {10.5381/jot.2020.19.3.a5},
  url             = {https://doi.org/10.5381/jot.2020.19.3.a5},
}

@Conference{Godio2019,
  author          = {Godio, A. and Bengolea, V. and Ponzio, P. and Aguirre, N. and Frias, M.F.},
  title           = {Efficient test generation guided by field coverage criteria},
  year            = {2019},
  note            = {cited By 0},
  pages           = {91-101},
  abstract        = {Field-exhaustive testing is a testing criterion suitable for object-oriented code over complex, heap-allocated, data structures. It requires test suites to contain enough test inputs to cover all feasible values for the object's fields within a certain scope (input-size bound). While previous work shows that field-exhaustive suites can be automatically generated, the generation technique required a formal specification of the inputs that can be subject to SAT-based analysis. Moreover, the restriction of producing all feasible values for inputs' fields makes test generation costly. In this paper, we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score, by examining to what extent the values of inputs' fields are covered. In particular, we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field-exhaustive suites, using the Symbolic Pathfinder tool. To underapproximate these suites we use tranScoping, a technique that estimates characteristics of yet to be run analyses for large scopes, based on data obtained from analyses performed in small scopes. This provides us with a suitable condition to prematurely stop the symbolic execution. As we show, tranScoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time. All this while retaining the effectiveness of field exhaustive suites in terms of test suite quality. © 2019 IEEE.},
  art_number      = {8952481},
  author_keywords = {Field-based testing; Field-exhaustive testing; Symbolic execution; Transcoping},
  doi             = {10.1109/ASE.2019.00019},
  journal         = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
  keywords        = {Model checking; Object oriented programming, Automatically generated; Exhaustive testing; Generation techniques; Object-oriented code; Sat-based analysis; Suitable conditions; Symbolic execution; Transcoping, Testing},
  url             = {https://doi.org/10.1109/ASE.2019.00019},
}

@Article{Gergely2019,
  author          = {Gergely, T. and Balogh, G. and Horváth, F. and Vancsics, B. and Beszédes, Á. and Gyimóthy, T.},
  journal         = {Software Quality Journal},
  title           = {Differences between a static and a dynamic test-to-code traceability recovery method},
  year            = {2019},
  note            = {cited By 3},
  number          = {2},
  pages           = {797-822},
  volume          = {27},
  abstract        = {Recovering test-to-code traceability links may be required in virtually every phase of development. This task might seem simple for unit tests thanks to two fundamental unit testing guidelines: isolation (unit tests should exercise only a single unit) and separation (they should be placed next to this unit). However, practice shows that recovery may be challenging because the guidelines typically cannot be fully followed. Furthermore, previous works have already demonstrated that fully automatic test-to-code traceability recovery for unit tests is virtually impossible in a general case. In this work, we propose a semi-automatic method for this task, which is based on computing traceability links using static and dynamic approaches, comparing their results and presenting the discrepancies to the user, who will determine the final traceability links based on the differences and contextual information. We define a set of discrepancy patterns, which can help the user in this task. Additional outcomes of analyzing the discrepancies are structural unit testing issues and related refactoring suggestions. For the static test-to-code traceability, we rely on the physical code structure, while for the dynamic, we use code coverage information. In both cases, we compute combined test and code clusters which represent sets of mutually traceable elements. We also present an empirical study of the method involving 8 non-trivial open source Java systems. © 2018, The Author(s).},
  author_keywords = {Code coverage; Refactoring; Structural test smells; Test-to-code traceability; Traceability link recovery; Unit testing},
  doi             = {10.1007/s11219-018-9430-x},
  keywords        = {Codes (symbols); Open source software; Recovery; Software testing, Code coverage; Refactorings; Structural tests; Traceability links; Unit testing, Open systems},
  url             = {https://doi.org/10.1007/s11219-018-9430-x},
}

@Article{McMinn2019,
  author          = {McMinn, P. and Wright, C.J. and McCurdy, C.J. and Kapfhammer, G.M.},
  journal         = {IEEE Transactions on Software Engineering},
  title           = {Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas},
  year            = {2019},
  note            = {cited By 8},
  number          = {5},
  pages           = {427-463},
  volume          = {45},
  abstract        = {Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding strength of a test suite. As with program mutation, however, relational database schema mutation results in many ineffective mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems-HyperSQL, PostgreSQL, and SQLite-the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with heavyweight DBMSs like PostgreSQL. © 1976-2012 IEEE.},
  art_number      = {8240964},
  author_keywords = {relational databases; software quality; software testing; software tools},
  doi             = {10.1109/TSE.2017.2786286},
  keywords        = {Computer software; Relational database systems; Software testing; Taxonomies; Testing, Algorithm design and analysis; Automatic Detection; Equivalent Mutants; Google; Mutation analysis; Relational Database; Relational database management systems; Relational database schemata, Quality control},
  url             = {https://doi.org/10.1109/TSE.2017.2786286},
}

@Conference{Wang2019,
  author          = {Wang, P. and Bai, G.R. and Stolee, K.T.},
  title           = {Exploring Regular Expression Evolution},
  year            = {2019},
  note            = {cited By 7},
  pages           = {502-513},
  abstract        = {Although there are tools to help developers understand the matching behaviors between a regular expression and a string, regular-expression related faults are still common. Learning developers' behavior through the change history of regular expressions can identify common edit patterns, which can inform the creation of mutation and repair operators to assist with testing and fixing regular expressions. In this work, we explore how regular expressions evolve over time, focusing on the characteristics of regular expression edits, the syntactic and semantic difference of the edits, and the feature changes of edits. Our exploration uses two datasets. First, we look at GitHub projects that have a regular expression in their current version and look back through the commit logs to collect the regular expressions' edit history. Second, we collect regular expressions composed by study participants during problem-solving tasks. Our results show that 1) 95% of the regular expressions from GitHub are not edited, 2) most edited regular expressions have a syntactic distance of 4-6 characters from their predecessors, 3) over 50% of the edits in GitHub tend to expand the scope of regular expression, and 4) the number of features used indicates the regular expression language usage increases over time. This work has implications for supporting regular expression repair and mutation to ensure test suite quality. © 2019 IEEE.},
  art_number      = {8667972},
  author_keywords = {empirical studies; evolution; Regular expressions},
  doi             = {10.1109/SANER.2019.8667972},
  journal         = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
  keywords        = {Pattern matching; Problem solving; Reengineering; Repair; Semantics; Syntactics, Change history; Empirical studies; evolution; Feature changes; Matching behavior; Regular expressions; Repair operator; Semantic difference, Computer programming languages},
  url             = {https://doi.org/10.1109/SANER.2019.8667972},
}

@Conference{Vancsics2019,
  author          = {Vancsics, B.},
  title           = {NFL: Neighbor-Based Fault Localization Technique},
  year            = {2019},
  note            = {cited By 1},
  pages           = {17-22},
  abstract        = {Fault localization (FL) is a much-researched area, there are a lot of techniques that help programmers to find the location of the error (or bug) as accurately as possible. However, these automatic procedures are either quite costly or, in some cases, inaccurate. There is no general method that would be recognized as the generally accepted method of FL.The aim of our research was to create an automatic FL algorithm that helps the user with good and balanced results in effective error detection. The presented Neighbor-based FL (NFL) is a graph-based algorithm which transposes the coverage matrix into a graph and prioritizes the methods based on their connection to the passed and failed tests. It also uses this information to specify the location of the bug as precisely as possible.We did an empirical evaluation on Defects4J and 6 additional fault localization metrics were used for quantification. Thus, the results obtained were objectively judged and comparisons could be made.The results show that, on average, NFL found the location of bugs most accurately, and the results compared to other metrics proved to be satisfactory. © 2019 IEEE.},
  art_number      = {8665491},
  author_keywords = {fault localization; program debugging; program spectra; software testing},
  doi             = {10.1109/IBF.2019.8665491},
  journal         = {IBF 2019 - 2019 IEEE 1st International Workshop on Intelligent Bug Fixing},
  keywords        = {Graphic methods; Location; Software testing, Automatic procedures; Empirical evaluations; Fault localization; General method; Graph-based algorithms; Program spectra, Program debugging},
  url             = {https://doi.org/10.1109/IBF.2019.8665491},
}

@Article{Vancsics2019a,
  author          = {Vancsics, B.},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title           = {Graph-Based Fault Localization},
  year            = {2019},
  note            = {cited By 0},
  pages           = {372-387},
  volume          = {11622 LNCS},
  abstract        = {The subject of fault localization (FL) is a much-researched area, it has large literature. There are plenty of algorithms that try to identify the location of the bugs using different approaches. Debugging has a large resource requirement, therefore the bug’s location’s reflective identifiicaton greatly helps developers and testers to maintain the quality and reliability of the software. Our goal is to implement a graph-based Fl GFL approach that effectively finds the location of the bugs in the source code. In our research we performed an empirical evaluation using the Defects4J and the results were compared with six other algorithms accepted by the literature. The results show that our method finds the errors more effectively than the other presented procedures, thus speeding up the bug fixes. © 2019, Springer Nature Switzerland AG.},
  author_keywords = {Fault localization; Program debugging; Program spectra; Software testing},
  doi             = {10.1007/978-3-030-24305-0_28},
  keywords        = {Graphic methods; Location; Software reliability; Software testing, Bug fixes; Empirical evaluations; Fault localization; Graph-based; Program spectra; Resource requirements; Source codes, Program debugging},
  url             = {https://doi.org/10.1007/978-3-030-24305-0_28},
}

@Conference{Vercammen2018,
  author          = {Vercammen, S. and Ghafari, M. and Demeyer, S. and Borg, M.},
  title           = {Goal-oriented mutation testing with focal methods},
  year            = {2018},
  note            = {cited By 1},
  pages           = {23-30},
  abstract        = {Mutation testing is the state-of-the-art technique for assessing the fault-detection capacity of a test suite. Unfortunately, mutation testing consumes enormous computing resources because it runs the whole test suite for each and every injected mutant. In this paper we explore fine-grained traceability links at method level (named focal methods), to reduce the execution time of mutation testing and to verify the quality of the test cases for each individual method, instead of the usually verified overall test suite quality. Validation of our approach on the open source Apache Ant project shows a speed-up of 573.5x for the mutants located in focal methods with a quality score of 80%. © 2018 Copyright held by the owner/author(s)..},
  author_keywords = {Feasibility study; Focal methods; Mutation testing; Software testing},
  doi             = {10.1145/3278186.3278190},
  journal         = {A-TEST 2018 - Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, Co-located with FSE 2018},
  keywords        = {Fault detection; Open source software; Testing, Computing resource; Feasibility studies; Focal methods; Goal-oriented; Mutation testing; Open sources; State-of-the-art techniques; Traceability links, Software testing},
  url             = {https://doi.org/10.1145/3278186.3278190},
}

@Conference{Zhu2018,
  author     = {Zhu, Q. and Zaidman, A.},
  title      = {Mutation testing for physical computing},
  year       = {2018},
  note       = {cited By 1},
  pages      = {289-300},
  abstract   = {Physical computing, which builds interactive systems between the physical world and computers, has been widely used in a wide variety of domains and applications, e.g., the Internet of Things (IoT). Although physical computing has witnessed enormous realisations, testing these physical computing systems still face many challenges, such as potential circuit related bugs which are not part of the software problems, the timing issue which decreasing the testability, etc.; therefore, we proposed a mutation testing approach for physical computing systems to enable engineers to judge the quality of their tests in a more accurate way. The main focus is the communication between the software and peripherals. More particular, we first defined a set of mutation operators based on the common communication errors between the software and peripherals that could happen in the software. We conducted a preliminary experiment on nine physical computing projects based on the Raspberry Pi and Arduino platforms. The results show that our mutation testing method can assess the test suite quality effectively in terms of weakness and inadequacy. © 2018 IEEE.},
  art_number = {8424980},
  doi        = {10.1109/QRS.2018.00042},
  journal    = {Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018},
  keywords   = {Computer software selection and evaluation; Internet of things; Program debugging; Software reliability; Testing, Arduino platforms; Communication errors; Interactive system; Internet of thing (IOT); Mutation operators; Physical computing; Physical computing systems; Software problems, Software testing},
  url        = {https://doi.org/10.1109/QRS.2018.00042},
}

@Book{Laemmel2018,
  author   = {Lämmel, R.},
  title    = {Software languages: Syntax, semantics, and metaprogramming},
  year     = {2018},
  note     = {cited By 10},
  abstract = {This book identifies, defines and illustrates the fundamental concepts and engineering techniques relevant to applications of software languages in software development. It presents software languages primarily from a software engineering perspective, i.e., it addresses how to parse, analyze, transform, generate, format, and otherwise process software artifacts in different software languages, as they appear in software development. To this end, it covers a wide range of software languages - most notably programming languages, domain-specific languages, modeling languages, exchange formats, and specifically also language definition languages. Further, different languages are leveraged to illustrate software language engineering concepts and techniques. The functional programming language Haskell dominates the book, while the mainstream programming languages Python and Java are additionally used for illustration. By doing this, the book collects and organizes scattered knowledge from software language engineering, focusing on application areas such as software analysis (software reverse engineering), software transformation (software re-engineering), software composition (modularity), and domain-specific languages. It is designed as a textbook for independent study as well as for bachelor's (advanced level) or master's university courses in Computer Science. An additional website provides complementary material, for example, lecture slides and videos. This book is a valuable resource for anyone wanting to understand the fundamental concepts and important engineering principles underlying software languages, allowing them to acquire much of the operational intelligence needed for dealing with software languages in software development practice. This is an important skill set for software engineers, as languages are increasingly permeating software development. © Springer International Publishing AG, part of Springer Nature 2018. All right reserved.},
  doi      = {10.1007/978-3-319-90800-7},
  journal  = {Software Languages: Syntax, Semantics, and Metaprogramming},
  pages    = {1-424},
  url      = {https://doi.org/10.1007/978-3-319-90800-7},
}

@Conference{Minhas2018,
  author          = {Minhas, N.M. and Petersen, K. and Ali, N.B. and Wnuk, K.},
  title           = {Regression testing goals-view of practitioners and researchers},
  year            = {2018},
  note            = {cited By 5},
  pages           = {25-31},
  volume          = {2018-January},
  abstract        = {Context: Regression testing is a well-researched area. However,the majority regression testing techniques proposed by theresearchers are not getting the attention of the practitioners. Communicationgaps between industry and academia and disparity in theregression testing goals are the main reasons. Close collaboration canhelp in bridging the communication gaps and resolving the disparities.Objective: The study aims at exploring the views of academicsand practitioners about the goals of regression testing. The purpose isto investigate the commonalities and differences in their viewpointsand defining some common goals for the success of regression testing.Method: We conducted a focus group study, with 7 testingexperts from industry and academia. 4 testing practitioners from 2companies and 3 researchers from 2 universities participated in thestudy. We followed GQM approach, to elicit the regression testinggoals, information needs, and measures.Results: 43 regression testing goals were identified by theparticipants, which were reduced to 10 on the basis of similarityamong the identified goals. Later during the priority assignmentprocess, 5 goals were discarded, because the priority assigned tothese goals was very low. Participants identified 47 informationneeds/questions required to evaluate the success of regression testingwith reference to goal G5 (confidence). Which were then reduced to10 on the basis of similarity. Finally, we identified measures to gaugethose information needs/questions, which were corresponding to thegoal (G5).Conclusions: We observed that participation level ofpractitioners and researchers during the elicitation of goals andquestions was same. We found a certain level of agreement betweenthe participants regarding the regression testing definitions and goals.But there was some level of disagreement regarding the prioritiesof the goals. We also identified the need to implement a regressiontesting evaluation framework in the participating companies. © 2017 IEEE.},
  author_keywords = {Focus group; GQM; Regression testing; Regression testing goals},
  doi             = {10.1109/APSECW.2017.23},
  journal         = {Proceedings - 2017 24th Asia-Pacific Software Engineering Conference Workshops, APSECW 2017},
  keywords        = {Software testing, Communication gaps; Evaluation framework; Focus group studies; Focus groups; Regression testing; Regression testing techniques, Regression analysis},
  url             = {https://doi.org/10.1109/APSECW.2017.23},
}

@Conference{Bruenink2018,
  author          = {Brünink, M. and Rosenblum, D.S.},
  title           = {Using Branch Frequency Spectra to Evaluate Operational Coverage},
  year            = {2018},
  note            = {cited By 0},
  pages           = {150-159},
  volume          = {2017-December},
  abstract        = {Coverage metrics try to quantify how well a software artifact is tested. High coverage numbers instill confidence in the software and might even be necessary to obtain certification. Unfortunately, achieving high coverage numbers does not imply high quality of the test suite. One shortcoming is that coverage metrics do not measure how well test suites cover systems in production. We look at coverage from an operational perspective. We evaluate test suite quality by comparing runs executed during testing with runs executed in production. Branch frequency spectra are employed to capture the behavior during runtime. Differences in the branch frequency spectra between field executions and testing runs indicate test suite deficiencies. This post-release test suite quality assurance mechanism can be used to (1) build confidence by pooling coverage information from many execution sites and (2) guide test suite augmentation in order to prepare the test suite for the next release cycle. © 2017 IEEE.},
  author_keywords = {Code coverage; Operational coverage; Program spectra},
  doi             = {10.1109/APSEC.2017.21},
  journal         = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
  keywords        = {Quality assurance; Quality control; Spectroscopy; Well testing, Code coverage; Coverage metrics; Frequency spectra; High quality; Operational coverage; Program spectra; Release cycles; Software artifacts, Software testing},
  url             = {https://doi.org/10.1109/APSEC.2017.21},
}

@Article{RehmanKhan2018,
  author          = {Rehman Khan, S.U. and Lee, S.P. and Javaid, N. and Abdul, W.},
  journal         = {IEEE Access},
  title           = {A Systematic Review on Test Suite Reduction: Approaches, Experiment's Quality Evaluation, and Guidelines},
  year            = {2018},
  note            = {cited By 9},
  pages           = {11816-11841},
  volume          = {6},
  abstract        = {Regression testing aims at testing a system under test (SUT) in the presence of changes. As a SUT changes, the number of test cases increases to handle the modifications, and ultimately, it becomes practically impossible to execute all of them within limited testing budget. Test suite reduction (TSR) approaches are widely used to improve the regression testing costs by selecting representative test suite without compromising effectiveness, such as fault-detection capability, within allowed time budget. The aim of this systematic review is to identify state-of-the-art TSR approaches categories, assess the quality of experiments reported on this subject, and provide a set of guidelines for conducting future experiments in this area of research. After applying a two-facet study selection procedure, we finalized 113 most relevant studies from an initial pool of 4230 papers published in the field of TSR between 1993 and 2016. The TSR approaches are broadly classified into four main categories based on the literature including greedy, clustering, search, and hybrid approaches. It is noted that majority of the experiments in TSR do not follow any specific guidelines for planning, conducting, and reporting the experiments, which may pose validity threats related to their results. Thus, we recommend conducting experiments that are better designed for the future. In this direction, an initial set of recommendations is provided that are useful for performing well-designed experiments in the field of TSR. Furthermore, we provide a number of future research directions based on current trends in this field of research. © 2013 IEEE.},
  author_keywords = {experiments; guidelines; regression testing; Software testing; test suite reduction},
  doi             = {10.1109/ACCESS.2018.2809600},
  keywords        = {Budget control; Clustering algorithms; Experiments; Fault detection; Optimization; Reduction; Regression analysis; Testing; Web services, Guidelines; Regression testing; Software systems; Systematics; Test suite reduction, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042848923&doi=10.1109%2fACCESS.2018.2809600&partnerID=40&md5=3060a13d850e9ea051d23ca67200e0ab},
}

@Article{Gergely2018,
  author          = {Gergely, T. and Balogh, G. and Horváth, F. and Vancsics, B. and Beszédes, Á. and Gyimóthy, T.},
  journal         = {Acta Cybernetica},
  title           = {Analysis of static and dynamic test-to-code traceability information},
  year            = {2018},
  note            = {cited By 0},
  number          = {3},
  pages           = {903-919},
  volume          = {23},
  abstract        = {Unit test development has some widely accepted guidelines. Two of them concern the test and code relationship, namely isolation (unit tests should examine only a single unit) and separation (they should be placed next to this unit). These guidelines are not always kept by the developers. They can however be checked by investigating the relationship between tests and the source code, which is described by test-to-code traceability links. Still, these links perhaps cannot be inferred unambiguously from the test and production code. We developed a method that is based on the computation of traceability links for different aspects and report Structural Unit Test Smells where the traceability links for the different aspects do not match. The two aspects are the static structure of the code that reflects the intentions of the developers and testers and the dynamic coverage which reveals the actual behavior of the code during test execution. In this study, we investigated this method on real programs. We manually checked the reported Structural Unit Test Smells to find out whether they are real violations of the unit testing rules. Furthermore, the smells were analyzed to determine their root causes and possible ways of correction. Copyright © 2018 Institute of Informatics, University of Szeged. All rights reserved.},
  author_keywords = {Code coverage; Refactoring; Test smells; Test-to-code traceability; Unit testing},
  doi             = {10.14232/actacyb.23.3.2018.11},
  keywords        = {Codes (symbols); Odors, Code coverage; Dynamic coverages; Refactorings; Static and dynamic tests; Static structures; Traceability information; Traceability links; Unit testing, Testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052407712&doi=10.14232%2factacyb.23.3.2018.11&partnerID=40&md5=dcf86298b9df8ef6ed836326105f4340},
}

@Conference{Bach2017,
  author          = {Bach, T. and Andrzejak, A. and Pannemans, R. and Lo, D.},
  title           = {The Impact of Coverage on Bug Density in a Large Industrial Software Project},
  year            = {2017},
  note            = {cited By 5},
  pages           = {307-313},
  volume          = {2017-November},
  abstract        = {Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing. To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project. © 2017 IEEE.},
  author_keywords = {bug densitiy; coverage; empirical research; industry project; large real world project; software quality},
  doi             = {10.1109/ESEM.2017.44},
  journal         = {International Symposium on Empirical Software Engineering and Measurement},
  keywords        = {Codes (symbols); Computer software selection and evaluation; Software engineering, bug densitiy; coverage; Empirical research; Industry project; Real world projects; Software Quality, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042384798&doi=10.1109%2fESEM.2017.44&partnerID=40&md5=926786df8be5dda473f3fd8671108953},
}

@Conference{Magalhaes2017,
  author          = {Magalhães, C. and Andrade, J. and Perrusi, L. and Mota, A.},
  title           = {Evaluating an automatic text-based test case selection using a non-instrumented code coverage analysis},
  year            = {2017},
  note            = {cited By 2},
  volume          = {Part F130656},
  abstract        = {During development, systems may be tested several times. In general, a system evolves from change requests, aiming at improving its behavior in terms of new features as well as fixing failures. Thus, selecting the best test plan in terms of the closeness between test cases and the changed code and its dependencies is pursued by industry and academia. In this paper we measure the coverage achieved by an automatic test case selection based on information retrieval that relates change requests and test cases. But instead of using off-the-shelf coverage tools, like JaCoCo, we propose a way of obtaining code coverage of Android apk's without instrumentation. This was a basic requirement of our industrial partner. We performed some experiments on this industrial partner and promising results were obtained. © 2017 Association for Computing Machinery.},
  art_number      = {5},
  author_keywords = {Code coverage; Information Retrieval; Test case selection and prioritization},
  doi             = {10.1145/3128473.3128478},
  journal         = {ACM International Conference Proceeding Series},
  keywords        = {Codes (symbols); Information retrieval, Code coverage; Industrial partners; Instrumented code; Prioritization; Test case; Test case selection; Test plan, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030473731&doi=10.1145%2f3128473.3128478&partnerID=40&md5=c23e6344b323e41b01849f1952a60f15},
}

@Article{Gopinath2017,
  author          = {Gopinath, R. and Ahmed, I. and Alipour, M.A. and Jensen, C. and Groce, A.},
  journal         = {Software Quality Journal},
  title           = {Does choice of mutation tool matter?},
  year            = {2017},
  note            = {cited By 5},
  number          = {3},
  pages           = {871-920},
  volume          = {25},
  abstract        = {Though mutation analysis is the primary means of evaluating the quality of test suites, it suffers from inadequate standardization. Mutation analysis tools vary based on language, when mutants are generated (phase of compilation), and target audience. Mutation tools rarely implement the complete set of operators proposed in the literature and mostly implement at least a few domain-specific mutation operators. Thus different tools may not always agree on the mutant kills of a test suite. Few criteria exist to guide a practitioner in choosing the right tool for either evaluating effectiveness of a test suite or for comparing different testing techniques. We investigate an ensemble of measures for evaluating efficacy of mutants produced by different tools. These include the traditional difficulty of detection, strength of minimal sets, and the diversity of mutants, as well as the information carried by the mutants produced. We find that mutation tools rarely agree. The disagreement between scores can be large, and the variation due to characteristics of the project—even after accounting for difference due to test suites—is a significant factor. However, the mean difference between tools is very small, indicating that no single tool consistently skews mutation scores high or low for all projects. These results suggest that experiments yielding small differences in mutation score, especially using a single tool, or a small number of projects may not be reliable. There is a clear need for greater standardization of mutation analysis. We propose one approach for such a standardization. © 2016, Springer Science+Business Media New York.},
  author_keywords = {Empirical analysis; Mutation analysis; Software testing},
  doi             = {10.1007/s11219-016-9317-7},
  keywords        = {Quality control; Standardization; Testing, Domain specific; Empirical analysis; Mutation analysis; Mutation operators; Mutation score; Target audience; Testing technique, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966667381&doi=10.1007%2fs11219-016-9317-7&partnerID=40&md5=b0e1bc973b3ce995d298e1c11603f2ba},
}

@Conference{Bowes2017,
  author          = {Bowes, D. and Hall, T. and Petrić, J. and Shippey, T. and Turhan, B.},
  title           = {How Good Are My Tests?},
  year            = {2017},
  note            = {cited By 16},
  pages           = {9-14},
  abstract        = {Background: Test quality is a prerequisite for achieving production system quality. While the concept of quality is multidimensional, most of the effort in testing context hasbeen channelled towards measuring test effectiveness. Objective: While effectiveness of tests is certainly important, we aim to identify a core list of testing principles that also address other quality facets of testing, and to discuss how they can be quantified as indicators of test quality. Method: We have conducted a two-day workshop with our industry partners to come up with a list of relevant principles and best practices expected to result in high quality tests. We then utilised our academic and industrial training materials together with recommendations in practitioner oriented testing books to refine the list. We surveyed existing literature for potential metrics to quantify identified principles. Results: We have identified a list of 15 testing principles to capture the essence of testing goals and best practices from quality perspective. Eight principles do not map toexisting test smells and we propose metrics for six of those. Further, we have identified additional potential metrics for the seven principles that partially map to test smells. Conclusion: We provide a core list of testing principles along with a discussion of possible ways to quantify them for assessing goodness of tests. We believe that our work would be useful for practitioners in assessing the quality of their tests from multiple perspectives including but not limited to maintainability, comprehension and simplicity. © 2017 IEEE.},
  art_number      = {7968009},
  author_keywords = {metrics; test quality; unit testing},
  doi             = {10.1109/WETSoM.2017.2},
  journal         = {International Workshop on Emerging Trends in Software Metrics, WETSoM},
  keywords        = {Materials testing; Odors, Best practices; High Quality Test; Industrial training; metrics; Production system; Test effectiveness; Test quality; Unit testing, Testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026860594&doi=10.1109%2fWETSoM.2017.2&partnerID=40&md5=34561c928d5976168d8bb9cc29353f13},
}

@Conference{Wang2017,
  author          = {Wang, Q. and Brun, Y. and Orso, A.},
  title           = {Behavioral Execution Comparison: Are Tests Representative of Field Behavior?},
  year            = {2017},
  note            = {cited By 12},
  pages           = {321-332},
  abstract        = {Software testing is the most widely used approach for assessing and improving software quality, but it is inherently incomplete and may not be representative of how the software is used in the field. This paper addresses the questions of to what extent tests represent how real users use software, and how to measure behavioral differences between test and field executions. We study four real-world systems, one used by endusers and three used by other (client) software, and compare test suites written by the systems' developers to field executions using four models of behavior: statement coverage, method coverage, mutation score, and a temporal-invariant-based model we developed. We find that developer-written test suites fail to accurately represent field executions: the tests, on average, miss 6.2% of the statements and 7.7% of the methods exercised in the field, the behavior exercised only in the field kills an extra 8.6% of the mutants, finally, the tests miss 52.6% of the behavioral invariants that occur in the field. In addition, augmenting the in-house test suites with automatically-generated tests by a tool targeting high code coverage only marginally improves the tests' behavioral representativeness. These differences between field and test executions - and in particular the finer-grained and more sophisticated ones that we measured using our invariantbased model - can provide insight for developers and suggest a better method for measuring test suite quality. © 2017 IEEE.},
  art_number      = {7927986},
  author_keywords = {Field data; Model inference; Software testing},
  doi             = {10.1109/ICST.2017.36},
  journal         = {Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
  keywords        = {Automatic test pattern generation; Computer software selection and evaluation; Online systems; Testing; Verification, Automatically generated; Field data; Model inference; Real-world system; Software Quality; Statement coverage; Temporal invariants; Test execution, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020753963&doi=10.1109%2fICST.2017.36&partnerID=40&md5=1e7452cb6ae8582674c935686b54d63d},
}

@Conference{Felbinger2017,
  author          = {Felbinger, H. and Wotawa, F. and Nica, M.},
  title           = {Mutation Score, Coverage, Model Inference: Quality Assessment for T-Way Combinatorial Test-Suites},
  year            = {2017},
  note            = {cited By 5},
  pages           = {171-180},
  abstract        = {In this paper we assess and evaluate the quality of t-way combinatorial test-suites using three different test-suite quality assessment methods. As t-way combinatorial test-suites reduce the input space of a program under test, we investigate how an increasing t affects the quality of the test-suite. There are some limitations of existing test-suite quality assessment methods e.g. the number of mutants is limited by execution time and code coverage measurement might be intrusive due to changes of the behavior of the program under test when instrumenting the code. Here we generate t-way combinatorial test-suites for Java programs of different size. We compute mutation score and code coverage for the generated test-suites, and apply additionally a new model inference based approach, that does not require to execute the program under test, to compare the generated test-suites with each other and assign a quality valuation to the test-suites. Our results show that an increasing t generally raises test-suite quality in terms of mutation score, coverage, and model inference. However, the model inference approach is only applicable, if the outcomes of the programs under test are discrete values, and if the number of discrete values is less than the test-suite size. © 2017 IEEE.},
  art_number      = {7899053},
  author_keywords = {Combinatorial testing; Coverage; Model inference; Mutation score},
  doi             = {10.1109/ICSTW.2017.36},
  journal         = {Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2017},
  keywords        = {Codes (symbols); Computer software; Testing; Verification, Code coverage; Combinatorial testing; Coverage; Different sizes; Discrete values; Model inference; Mutation score; Quality assessment, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018371142&doi=10.1109%2fICSTW.2017.36&partnerID=40&md5=d483a7a148df34b3a0338941e623e3d7},
}

@CONFERENCE{VanBeckhoven2017,
author={Van Beckhoven, P. and Oprescu, A. and Bruntink, M.},
title={Assessing test suite eectiveness using static metrics},
journal={CEUR Workshop Proceedings},
year={2017},
volume={2070},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045417698&partnerID=40&md5=454fa40a88fc0151a9da3597e418c942},
abstract={With the increasing amount of automated tests, we need ways to measure the test eectiveness. The state-of-the-art technique for assessing test eectiveness, mutation testing, is too slow and cumbersome to be used in large scale evolution studies or code audits by external companies. In this paper we investigated two alternatives, namely code coverage and assertion count. We discovered that code coverage outperforms assertion count by showing a relation with test suite eec-tiveness for all analysed project. Assertion count only displays such a relation in only one of the analysed projects. Further analysing this relationship between assertion count coverage and test eectiveness would allow to circumvent some of the problems of mutation testing. Copyright c by the paper's authors. Copying permitted for private and academic purposes.},
keywords={Codes (symbols);  Software testing, Automated test;  Code Audit;  Code coverage;  Large-scale evolution;  Mutation testing;  State-of-the-art techniques;  Static metrics;  Test suites, Testing},
}

@Conference{Ahmed2016,
  author          = {Ahmed, I. and Gopinath, R. and Brindescu, C. and Groce, A. and Jensen, C.},
  title           = {Can testedness be effectively measured?},
  year            = {2016},
  note            = {cited By 22},
  pages           = {547-558},
  volume          = {13-18-November-2016},
  abstract        = {Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and decid-ing when to stop testing. Test the least-Tested code, and stop when all code is well-Tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite qual-ity. The first measure is statement coverage, the simplest and best-known code coverage measure. The second mea-sure is mutation score, a supposedly more powerful, though expensive, measure. We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer fu-ture bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring tested-ness. Using a large number of open source Java programs from Github and Apache, we show that both statement cov-erage and mutation score have only a weak negative corre-lation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Pro-gram elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.},
  author_keywords = {Coverage criteria; Mutation testing; Sta-Tistical analysis; Test suite evaluation},
  doi             = {10.1145/2950290.2950324},
  journal         = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
  keywords        = {Codes (symbols); Computer software; Java programming language; Open source software; Software engineering, Coverage criteria; Mutation testing; Program elements; Research evaluation; Software engineering practices; Sta-Tistical analysis; Statement coverage; Strong correlation, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418602&doi=10.1145%2f2950290.2950324&partnerID=40&md5=dda5f661050c3d8d817424775170a2d3},
}

@Conference{Parsai2016,
  author          = {Parsai, A. and Murgia, A. and Demeyer, S.},
  title           = {A Model to Estimate First-Order Mutation Coverage from Higher-Order Mutation Coverage},
  year            = {2016},
  note            = {cited By 8},
  pages           = {365-373},
  abstract        = {The test suite is essential for fault detection during software development. First-order mutation coverage is an accurate metric to quantify the quality of the test suite. However, it is computationally expensive. Hence, the adoption of this metric is limited. In this study, we address this issue by proposing a realistic model able to estimate first-order mutation coverage using only higher-order mutation coverage. Our study shows how the estimation evolves along with the order of mutation. We validate the model with an empirical study based on 17 open-source projects. © 2016 IEEE.},
  art_number      = {7589816},
  author_keywords = {Higher-order Mutation Testing; Model; Mutation Testing; Software Testing; Test Suite Quality},
  doi             = {10.1109/QRS.2016.48},
  journal         = {Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016},
  keywords        = {Computer software selection and evaluation; Fault detection; Models; Open source software; Software design; Software reliability, Empirical studies; First order; Higher-order; Mutation testing; Open source projects; Realistic model, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995519530&doi=10.1109%2fQRS.2016.48&partnerID=40&md5=25ea0c780048802f0ff532d40d293b5b},
}

@Conference{Tengeri2016,
  author          = {Tengeri, D. and Vidács, L. and Beszédes, A. and Jász, J. and Balogh, G. and Vancsics, B. and Gyimóthy, T.},
  title           = {Relating Code Coverage, Mutation Score and Test Suite Reducibility to Defect Density},
  year            = {2016},
  note            = {cited By 10},
  pages           = {174-179},
  abstract        = {Assessing the overall quality (adequacy for a particular purpose) of existing test suites is a complex task. Their code coverage is a simple yet powerful attribute for this purpose, so the additional benefits of mutation analysis may not always justify the comparably much higher costs and complexity of the computation. Mutation testing methods and tools slowly start to reach a maturity level at which their use in everyday industrial practice becomes possible, yet it is still not completely clear in which situations they provide additional insights into various quality attributes of the test suites. This paper reports on an experiment conducted on four open source systems' test suites to compare them from the viewpoints of code coverage, mutation score and test suite reducibility (the amount test adequacy is degraded in a reduced test suite). The purpose of the comparison is to find out when the different attributes provide additional insights with respect to defect density, a separately computed attribute for the estimation of real faults. We demonstrate that in some situations code coverage might be a sufficient indicator of the expected defect density, but mutation and reducibility are better in most of the cases. © 2016 IEEE.},
  art_number      = {7528960},
  author_keywords = {code coverage; defect density; Mutation analysis; test adequacy criteria; test suite reduction},
  doi             = {10.1109/ICSTW.2016.25},
  journal         = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
  keywords        = {Codes (symbols); Defect density; Defects; Open source software; Open systems; Testing; Verification, Code coverage; Industrial practices; Mutation analysis; Open source system; Quality attributes; Reduced test suites; Test adequacy criteria; Test suite reduction, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992222115&doi=10.1109%2fICSTW.2016.25&partnerID=40&md5=f7a1ebc4a978afcb286b920b20848b39},
}

@Conference{Holling2016,
  author          = {Holling, D. and Banescu, S. and Probst, M. and Petrovska, A. and Pretschner, A.},
  title           = {Nequivack: Assessing Mutation Score Confidence},
  year            = {2016},
  note            = {cited By 7},
  pages           = {152-161},
  abstract        = {The mutation score is defined as the number of killed mutants divided by the number of non-equivalent mutants. However, whether a mutant is equivalent to the original program is undecidable in general. Thus, even when improving a test suite, a mutant score assessing this test suite may become worse during the development of a system, because of equivalent mutants introduced during mutant creation. This is a fundamental problem. Using static analysis and symbolic execution, we show how to establish non-equivalence or "don't know" among mutants. If the number of don't knows is small, this is a good indicator that a computed mutation score actually reflects its above definition. We can therefore have an increased confidence that mutation score trends correspond to actual improvements of a test suite's quality, and are not overly polluted by equivalent mutants. Using a set of 14 representative unit size programs, we show that for some, but not all, of these programs, the above confidence can indeed be established. We also evaluate the reproducibility, efficiency and effectiveness of our Nequivack tool. Our findings are that reproducibility is completely given. A single mutant analysis can be performed within 3 seconds on average, which is efficient for practical and industrial applications. © 2016 IEEE.},
  art_number      = {7528957},
  author_keywords = {equivalent mutant; mutant score confidence; mutation score; non-equivalence checking},
  doi             = {10.1109/ICSTW.2016.29},
  journal         = {Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
  keywords        = {Static analysis; Verification, Equivalence checking; Equivalent Mutants; mutant score confidence; Mutation score; Reproducibilities; Single mutant; Symbolic execution, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992188057&doi=10.1109%2fICSTW.2016.29&partnerID=40&md5=69b302a57f396d9853d1c46c6ab910cc},
}

@Conference{Zhang2016,
  author          = {Zhang, J. and Wang, Z. and Zhang, L. and Hao, D. and Zang, L. and Cheng, S. and Zhang, L.},
  title           = {Predictive mutation testing},
  year            = {2016},
  note            = {cited By 31},
  pages           = {342-353},
  abstract        = {Mutation testing is a powerful methodology for evaluating test suite quality. In mutation testing, a large number of mutants are generated and executed against the test suite to check the ratio of killed mutants. Therefore, mutation testing is widely believed to be a computationally expensive technique. To alleviate the efficiency concern of mutation testing, in this paper, we propose predictive mutation testing (PMT), the first approach to predicting mutation testing results without mutant execution. In particular, the proposed approach constructs a classification model based on a series of features related to mutants and tests, and uses the classification model to predict whether a mutant is killed or survived without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (i.e., cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss when predicting mutant execution results, indicating a good tradeoff between efficiency and effectiveness of mutation testing. © 2016 ACM.},
  author_keywords = {Machine learning; Mutation testing; Software testing},
  doi             = {10.1145/2931037.2931038},
  journal         = {ISSTA 2016 - Proceedings of the 25th International Symposium on Software Testing and Analysis},
  keywords        = {Artificial intelligence; Efficiency; Forecasting; Learning systems, Accuracy loss; Application scenario; Classification models; Mutation testing; Real world projects, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984865403&doi=10.1145%2f2931037.2931038&partnerID=40&md5=b1a090f70fa5481b41dd0c8289130599},
}

@Conference{Felbinger2016,
  author          = {Felbinger, H. and Wotawa, F. and Nica, M.},
  title           = {Empirical study of correlation between mutation score and model inference based test suite adequacy assessment},
  year            = {2016},
  note            = {cited By 5},
  pages           = {43-49},
  abstract        = {In this paper we investigate a method for test suite evaluation that is based on an inferred model from the test suite. The idea is to use the similarity between the inferred model and the system under test as a measure of test suite adequacy, which is the ability of a test suite to expose errors in the system under test. We define similarity using the root mean squared error computed from the differences of the system under test output and the model output for certain inputs not used for model inference. In the paper we introduce the approach and provide results of an experimental evaluation where we compare the similarity with the mutation score. We used the Pearson Correlation coefficient to calculate whether a linear correlation between mutation score and root mean squared error exists. As a result we obtain that in certain cases the computed similarity strongly correlates with the mutation score. © 2016 ACM.},
  author_keywords = {Machine learning; Mutation score; Software test},
  doi             = {10.1145/2896921.2896923},
  journal         = {Proceedings - 11th International Workshop on Automation of Software Test, AST 2016},
  keywords        = {Artificial intelligence; Correlation methods; Errors; Learning algorithms; Learning systems; Mean square error, Empirical studies; Experimental evaluation; Linear correlation; Model inference; Mutation score; Pearson correlation coefficients; Root mean squared errors; System under test, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974652937&doi=10.1145%2f2896921.2896923&partnerID=40&md5=2e13e72dfaa17188d2617877d087c312},
}

@Article{Falah2015,
  author          = {Falah, B. and Akour, M. and Marchoum, N.E.},
  journal         = {International Review on Computers and Software},
  title           = {Testing patterns in action: Designing a test-pattern-based suite},
  year            = {2015},
  note            = {cited By 0},
  number          = {5},
  pages           = {489-494},
  volume          = {10},
  abstract        = {Design patterns constitute a revolution in the field of software engineering, as they emphasize the importance of reuse and its impact on the software process and the software product quality. A special type of design patterns is testing patters; these can be used in the testing phase to reduce redundancy, save time and resources and provide an effective reuse mechanism for more coverage and better quality of service at the same time. Many design patterns exist to test different aspects of the implemented functionality separately. However, in this paper, we will suggest a new concept, which consists of incorporating different testing patterns into the same test suite to test different aspects through running one single test exactly once. It will also allow the users to track the performance of their test suite quality attributes using a simple representation. © 2015, Praise Worthy Prize S.r.l. - All rights reserved.},
  author_keywords = {Design patterns; Quality metrics; Software testing; Test patterns; Testing quality},
  doi             = {10.15866/irecos.v10i5.5963},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938498296&doi=10.15866%2firecos.v10i5.5963&partnerID=40&md5=077e9f4fde9e21a4db49d19865f63ac6},
}

@Article{Fawaz2015,
  author          = {Fawaz, K. and Zaraket, F. and Masri, W. and Harkous, H.},
  journal         = {Software Quality Journal},
  title           = {PBCOV: a property-based coverage criterion},
  year            = {2015},
  note            = {cited By 4},
  number          = {1},
  pages           = {171-202},
  volume          = {23},
  abstract        = {Coverage criteria aim at satisfying test requirements and compute metrics values that quantify the adequacy of test suites at revealing defects in programs. Typically, a test requirement is a structural program element, and the coverage metric value represents the percentage of elements covered by a test suite. Empirical studies show that existing criteria might characterize a test suite as highly adequate, while it does not actually reveal some of the existing defects. In other words, existing structural coverage criteria are not always sensitive to the presence of defects. This paper presents PBCOV, a Property-Based COVerage criterion, and empirically demonstrates its effectiveness. Given a program with properties therein, static analysis techniques, such as model checking, leverage formal properties to find defects. PBCOV is a dynamic analysis technique that also leverages properties and is characterized by the following: (a) It considers the state space of first-order logic properties as the test requirements to be covered; (b) it uses logic synthesis to compute the state space; and (c) it is practical, i.e., computable, because it considers an over-approximation of the reachable state space using a cut-based abstraction.We evaluated PBCOV using programs with test suites comprising passing and failing test cases. First, we computed metrics values for PBCOV and structural coverage using the full test suites. Second, in order to quantify the sensitivity of the metrics to the absence of failing test cases, we computed the values for all considered metrics using only the passing test cases. In most cases, the structural metrics exhibited little or no decrease in their values, while PBCOV showed a considerable decrease. This suggests that PBCOV is more sensitive to the absence of failing test cases, i.e., it is more effective at characterizing test suite adequacy to detect defects, and at revealing deficiencies in test suites. © 2014, Springer Science+Business Media New York.},
  author_keywords = {Coverage criteria; Logic synthesis; Property-based coverage; Reachability analysis; Software testing; Specification-based coverage; State space coverage; Test suite evaluation},
  doi             = {10.1007/s11219-014-9237-3},
  keywords        = {C (programming language); Computer circuits; Defects; Logic Synthesis; Model checking; State space methods; Static analysis; Structural analysis; Structural properties; Testing, Analysis techniques; Coverage criteria; Dynamic analysis techniques; First order logic; Property-based; Reachability analysis; Structural metrics; Structural programs, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924222369&doi=10.1007%2fs11219-014-9237-3&partnerID=40&md5=db439c6545c725201184a9ba7af9d6d4},
}

@Conference{Horvath2015,
  author          = {Horváth, F. and Vancsics, B. and Vidács, L. and Beszédes, Á. and Tengeri, D. and Gergely, T. and Gyimóthy, T.},
  title           = {Test suite evaluation using code coverage based metrics},
  year            = {2015},
  note            = {cited By 7},
  pages           = {46-60},
  volume          = {1525},
  abstract        = {Regression test suites of evolving software systems are often crucial to maintaining software quality in the long term. They have to be effective in terms of detecting faults and helping their localization. However, to gain knowledge of such capabilities of test suites is usually difficult. We propose a method for deeper understanding of a test suite and its relation to the program code it is intended to test. The basic idea is to decompose the test suite and the program code into coherent logical groups which are easier to analyze and understand. Coverage and partition metrics are then extracted directly from code coverage information to characterize a test suite and its constituents. We also use heat-map tables for test suite assessment both at the system level and at the level of logical groups. We employ these metrics to analyze and evaluate the regression test suite of the WebKit system, an industrial size browser engine with an extensive set of 27,000 tests.},
  author_keywords = {Code coverage; Regression testing; Test metrics; Test suite evaluation},
  journal         = {CEUR Workshop Proceedings},
  keywords        = {Codes (symbols); Computational linguistics; Computer software; Computer software selection and evaluation; Regression analysis; Testing, Code coverage; Industrial size; Regression testing; Regression tests; Software Quality; Software systems; System levels; Test metrics, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962535014&partnerID=40&md5=0afbed6c96650e33de141304e883885a},
}

@CONFERENCE{Tengeri2015,
author={Tengeri, D. and Beszédes, A. and Gergely, T. and Vidács, L. and Hávas, D. and Gyimóthy, T.},
title={Beyond code coverage - An approach for test suite assessment and improvement},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2015 - Proceedings},
year={2015},
doi={10.1109/ICSTW.2015.7107476},
art_number={7107476},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934301413&doi=10.1109%2fICSTW.2015.7107476&partnerID=40&md5=edf4f61994fcb6c29d60ef2f3d37a202},
abstract={Code coverage is successfully used to guide white box test design and evaluate the respective test completeness. However, simple overall coverage ratios are often not precise enough to effectively help when a (regression) test suite needs to be reassessed and evolved after software change. We present an approach for test suite assessment and improvement that utilizes code coverage information, but on a more detailed level and adds further evaluation aspects derived from the coverage. The main use of the method is to aid various test suite evolution situations such as removal, refactoring and extension of test cases as a result of code change or test suite efficiency enhancement. We define various metrics to express different properties of test suites beyond simple code coverage ratios, and present the assessment and improvement process as an iterative application of different improvement goals and more specific sub-activities. The method is demonstrated by applying it to improve the tests of one of our experimental systems. © 2015 IEEE.},
author_keywords={code coverage;  regression testing;  test suite evolution;  test suite quality;  test suite refactoring;  white box testing metrics},
}

@CONFERENCE{Felbinger2015,
author={Felbinger, H.},
title={Test suite quality assessment using model inference techniques},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015 - Proceedings},
year={2015},
doi={10.1109/ICST.2015.7102617},
art_number={7102617},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935093475&doi=10.1109%2fICST.2015.7102617&partnerID=40&md5=6aae7422e3dc3fd266a7744612bbebd2},
abstract={To state whether a System Under Test is sufficiently tested requires an assessment of the test suite quality. Existing methods to assess the quality of a test suite either are based on the structure of an implementation or determine the quality using mutation score. In this paper we introduce a method, which is based on inductive inference to assess the quality of a test suite and propose a method to augment a test suite depending on the quality assessment result. In this paper we provide a short glimpse on our objectives and show preliminary results of model inference of a test suite. © 2015 IEEE.},
keywords={Testing;  Verification, Inductive inference;  Model inference;  Mutation score;  Quality assessment;  System under test, Software testing},
}

@Conference{Inozemtseva2014,
  author          = {Inozemtseva, L. and Holmes, R.},
  title           = {Coverage is not strongly correlated with test suite effectiveness},
  year            = {2014},
  note            = {cited By 219},
  number          = {1},
  pages           = {435-445},
  abstract        = {The coverage of a test suite is often used as a proxy for its ability to detect faults. However, previous studies that investigated the correlation between code coverage and test suite effectiveness have failed to reach a consensus about the nature and strength of the relationship between these test suite characteristics. Moreover, many of the studies were done with small or synthetic programs, making it unclear whether their results generalize to larger programs, and some of the studies did not account for the confounding influence of test suite size. In addition, most of the studies were done with adequate suites, which are are rare in practice, so the results may not generalize to typical test suites. We have extended these studies by evaluating the relationship between test suite size, coverage, and effectiveness for large Java programs. Our study is the largest to date in the literature: we generated 31,000 test suites for five systems consisting of up to 724,000 lines of source code. We measured the statement coverage, decision coverage, and modified condition coverage of these suites and used mutation testing to evaluate their fault detection effectiveness. We found that there is a low to moderate correlation between coverage and effectiveness when the number of test cases in the suite is controlled for. In addition, we found that stronger forms of coverage do not provide greater insight into the effectiveness of the suite. Our results suggest that coverage, while useful for identifying under-tested parts of a program, should not be used as a quality target because it is not a good indicator of test suite effectiveness. © 2014 ACM.},
  author_keywords = {Coverage; test suite effectiveness; test suite quality},
  doi             = {10.1145/2568225.2568271},
  journal         = {Proceedings - International Conference on Software Engineering},
  keywords        = {Computer software; Fault detection; Java programming language; Software engineering, Code coverage; Coverage; Decision coverage; Fault detection effectiveness; Modified conditions; Mutation testing; Quality targets; Statement coverage, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994086611&doi=10.1145%2f2568225.2568271&partnerID=40&md5=8124f10f05fc8239822e7289468833fe},
}

@Article{Schuler2013,
  author          = {Schuler, D. and Zeller, A.},
  journal         = {Software Testing Verification and Reliability},
  title           = {Checked coverage: An indicator for oracle quality},
  year            = {2013},
  note            = {cited By 16},
  number          = {7},
  pages           = {531-551},
  volume          = {23},
  abstract        = {known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality and even more sensitive than mutation testing. Copyright © 2013 John Wiley & Sons, Ltd.},
  author_keywords = {coverage metrics; dynamic slicing; mutation testing; test suite quality},
  doi             = {10.1002/stvr.1497},
  keywords        = {Coverage metrics; Dynamic slicing; Mutation testing; Open source projects, Media streaming; Software engineering, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885947629&doi=10.1002%2fstvr.1497&partnerID=40&md5=791a61c243449cccbe12f39c7768b843},
}

@Conference{Inozemtseva2013,
  author          = {Inozemtseva, L. and Hemmati, H. and Holmes, R.},
  title           = {Using fault history to improve mutation reduction},
  year            = {2013},
  note            = {cited By 8},
  pages           = {639-642},
  abstract        = {Mutation testing can be used to measure test suite quality in two ways: by treating the kill score as a quality metric, or by treating each surviving, non-equivalent mutant as an indicator of an inadequacy in the test suite. The first technique relies on the assumption that the mutation score is highly correlated with the suite's real fault detection rate, which is not well supported by the literature. The second technique relies only on the weaker assumption that the "interesting" mutants (i.e., the ones that indicate an inadequacy in the suite) are in the set of surviving mutants. Using the second technique also makes improving the suite straightforward. Unfortunately, mutation testing has a performance problem. At least part of the test suite must be run on every mutant, meaning mutation testing can be too slow for practical use. Previous work has addressed this by reducing the number of mutants to evaluate in various ways, including selecting a random subset of them. However, reducing the set of mutants by random reduction is suboptimal for developers using the second technique described above, since random reduction will eliminate many of the interesting mutants. We propose a new reduction method that supports the use of the second technique by reducing the set of mutants to those generated by altering files that have contained many faults in the past. We performed a pilot study that suggests that this reduction method preferentially chooses mutants that will survive mutation testing; that is, it preserves a greater number of interesting mutants than random reduction does.},
  author_keywords = {Fault history; Mutant reduction; Mutation testing; Test suite quality},
  doi             = {10.1145/2491411.2494586},
  journal         = {2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
  keywords        = {Fault detection rate; Highly-correlated; Mutation score; Mutation testing; Performance problems; Quality metrices; Random subsets; Reduction method, Fault detection; Software testing, Software engineering},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883722857&doi=10.1145%2f2491411.2494586&partnerID=40&md5=4445c5597ca0ac7d8c98983944c2f41b},
}

@Conference{Zhi2013,
  author          = {Zhi, J. and Garousi, V.},
  title           = {On adequacy of assertions in automated test suites: An empirical investigation},
  year            = {2013},
  note            = {cited By 7},
  pages           = {382-391},
  abstract        = {An integral part of test case is the verification phase (also called 'test oracle'), which verifies program's state, output or behavior. In automated testing, the verification phase is often implemented using test assertions which are usually developed manually by testers. More precisely, assertions are used for checking the unit or the system's behavior (or output) which is reflected by the changes in the data fields of the class under test, or the output of the function under test. Originated from human (testers') error, test suites are prone to having inadequate assertions. The paper reports an empirical study on the Inadequate-Assertion (IA) problem in the context of automated test suites developed for open-source projects. In this study, test suites of three active open-source projects have been chosen. To investigate IA problem occurrence among the sampled test suites, we performed mutation analysis and coverage analysis. The results indicate that: (1) the IA problem is common among the sampled open-source projects, and the occurrence varies from project to project and from package to package, and (2) the occurrence rate of the IA problem is positively co-related with the complexity of test code. © 2013 IEEE.},
  art_number      = {6571656},
  author_keywords = {adequacy of test oracles; automated testing; empirical case-study; mutation testing; quality of test oracles; state coverage; Test assertions; test-suite quality},
  doi             = {10.1109/ICSTW.2013.49},
  journal         = {Proceedings - IEEE 6th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2013},
  keywords        = {Automated testing; empirical case-study; Mutation testing; State coverage; Test assertions; Test oracles, Testing, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883330428&doi=10.1109%2fICSTW.2013.49&partnerID=40&md5=cbd09eac0a1c9ef41b8ab2ca883dc411},
}

@Article{RealesMateo2013,
  author          = {Reales Mateo, P. and Polo Usaola, M. and Fernández Alemán, J.L.},
  journal         = {IEEE Transactions on Software Engineering},
  title           = {Validating second-order mutation at system level},
  year            = {2013},
  note            = {cited By 25},
  number          = {4},
  pages           = {570-587},
  volume          = {39},
  abstract        = {Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like (n)-order mutation where each mutant contains (n) artificial faults instead of one. There are two approaches to (n)-order mutation: increasing the effectiveness of mutation by searching for good (n)-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into (n)-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of (n)-order mutation at system level. © 1976-2012 IEEE.},
  art_number      = {6216382},
  author_keywords = {Empirical evaluation; high-order mutation; mutation testing},
  doi             = {10.1109/TSE.2012.39},
  keywords        = {Combination strategies; Empirical evaluations; Empirical studies; High-order; Mutation operators; Mutation testing; Program statements; Software testing techniques, Cost reduction; Risk perception; Software testing; Testing, Cost benefit analysis},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875720693&doi=10.1109%2fTSE.2012.39&partnerID=40&md5=5c4441f6cefbaa3689a83c6e2e63d765},
}

@Conference{Dulz2011,
  author          = {Dulz, W.},
  title           = {A comfortable TestPlayer for analyzing statistical usage testing strategies},
  year            = {2011},
  note            = {cited By 4},
  pages           = {36-42},
  abstract        = {We will first give a brief introduction to a versatile modeling environment that allows the early validation of system specifications by a test-driven agile simulation approach. The main focus of our paper is on providing techniques for automated test case generation relying on statistical usage models. Based on the open source software R for statistical computing and graphics the easy to handle test case generation and analyzing TestPlayer© tool was developped. Starting from customer-specific graphical Markov chain usage models test cases are automatically generated and visualized by highlighting selected nodes and arcs in the model. In addition, various metrics and corresponding diagrams offer analytical techniques to assess the quality of the derived test suite. © 2011 ACM.},
  author_keywords = {graphical test suite evaluation; markov chain usage model; model-driven test case generation; testplayer tool},
  doi             = {10.1145/1982595.1982604},
  journal         = {Proceedings - International Conference on Software Engineering},
  keywords        = {Automated test case generation; Graphical test; Markov Chain; Model-driven; Modeling environments; Open Source Software; Simulation approach; Statistical computing; Statistical usage testing; System specification; Test case generation; testplayer tool; Usage models, Computer software selection and evaluation; Health care; Markov processes; Open systems; Software engineering; Specifications; Testing, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959443087&doi=10.1145%2f1982595.1982604&partnerID=40&md5=6f14247b755c8a92a95da75662293727},
}

@Conference{Schuler2011,
  author          = {Schuler, D. and Zeller, A.},
  title           = {Assessing oracle quality with checked coverage},
  year            = {2011},
  note            = {cited By 44},
  pages           = {90-99},
  abstract        = {A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative. © 2011 IEEE.},
  art_number      = {5770598},
  author_keywords = {coverage metrics; dynamic slicing; mutation testing; test suite quality},
  doi             = {10.1109/ICST.2011.32},
  journal         = {Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011},
  keywords        = {Coverage metrics; dynamic slicing; Mutation testing; Open source projects; test suite quality, Computer software selection and evaluation; Verification, Software testing},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958743339&doi=10.1109%2fICST.2011.32&partnerID=40&md5=a3e9f7781fffc1efe9a94d69fe4a1d39},
}

@Article{Popovic2010,
  author          = {Popovic, M. and Basicevic, I.},
  journal         = {Information and Software Technology},
  title           = {Test case generation for the task tree type of architecture},
  year            = {2010},
  note            = {cited By 7},
  number          = {6},
  pages           = {697-706},
  volume          = {52},
  abstract        = {Context: Emerging multicores and clusters of multicores that may operate in parallel have set a new challenge - development of massively parallel software composed of thousands of loosely coupled or even completely independent threads/processes, such as MapReduce and Java 3.0 workers, or Erlang processes, respectively. Testing and verification is a critical phase in the development of such software products. Objective: Generating test cases based on operational profiles and certifying declared operational reliability figure of the given software product is a well-established process for the sequential type of software. This paper proposes an adaptation of that process for a class of massively parallel software - large-scale task trees. Method: The proposed method uses statistical usage testing and operational reliability estimation based on operational profiles and novel test suite quality indicators, namely the percentage of different task trees and the percentage of different paths. Results: As an example, the proposed method is applied to operational reliability certification of a parallel software infrastructure named the TaskTreeExecutor. The paper proposes an algorithm for generating random task trees to enable that application. Test runs in the experiments involved hundreds and thousands of Win32/Linux threads thus demonstrating scalability of the proposed approach. For practitioners, the most useful result presented is the method for determining the number of task trees and the number of paths, which are needed to certify the given operational reliability of a software product. The practitioners may also use the proposed coverage metrics to measure the quality of automatically generated test suite. Conclusion: This paper provides a useful solution for the test case generation that enables the operational reliability certification process for a class of massively parallel software called the large-scale task trees. The usefulness of this solution was demonstrated by a case study - operational reliability certification of the real parallel software product. © 2010 Elsevier B.V. All rights reserved.},
  author_keywords = {Massively parallel software; Operational reliability; Statistical usage testing; Test case generation},
  doi             = {10.1016/j.infsof.2010.03.001},
  keywords        = {Automatically generated; Certification process; Coverage metrics; Erlang process; Operational profile; Operational reliability; Parallel software; Quality indicators; Software products; Statistical usage testing; Task tree; Test case; Test case generation; Test runs, Automatic test pattern generation; Computer software selection and evaluation; Quality assurance; Testing; Trees (mathematics); Verification, Software reliability},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950458077&doi=10.1016%2fj.infsof.2010.03.001&partnerID=40&md5=43e01c9367f30a797b20115200a96cd4},
}

@InProceedings{Cordova2021,
  author    = {Cordova, Lucas and Carver, Jeffrey and Gershmel, Noah and Walia, Gursimran},
  booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
  title     = {A Comparison of Inquiry-Based Conceptual Feedback vs. Traditional Detailed Feedback Mechanisms in Software Testing Education: An Empirical Investigation},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {87–93},
  publisher = {Association for Computing Machinery},
  series    = {SIGCSE '21},
  abstract  = {The feedback provided by current testing education tools about the deficiencies in a student's test suite either mimics industry code coverage tools or lists specific instructor test cases that are missing from the student's test suite. While useful in some sense, these types of feedback are akin to revealing the solution to the problem, which can inadvertently encourage students to pursue a trial-and-error approach to testing, rather than using a more systematic approach that encourages learning. In addition to not teaching students why their test suite is inadequate, this type of feedback may motivate students to become dependent on the feedback rather than thinking for themselves. To address this deficiency, there is an opportunity to investigate alternative feedback mechanisms that include a positive reinforcement of testing concepts. We argue that using an inquiry-based learning approach is better than simply providing the answers. To facilitate this type of learning, we present Testing Tutor, a web-based assignment submission platform that supports different levels of testing pedagogy via a customizable feedback engine. We evaluated the impact of the different types of feedback through an empirical study in two sophomore-level courses. We use Testing Tutor to provide students with different types of feedback, either traditional detailed code coverage feedback or inquiry-based learning conceptual feedback, and compare the effects. The results show that students that receive conceptual feedback had higher code coverage (by different measures), fewer redundant test cases, and higher programming grades than the students who receive traditional code coverage feedback.},
  doi       = {10.1145/3408877.3432417},
  isbn      = {9781450380621},
  keywords  = {education, testing, pedagogy, tools},
  location  = {Virtual Event, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3408877.3432417},
}

@InProceedings{Clegg2021,
  author    = {Clegg, Benjamin Simon and McMinn, Phil and Fraser, Gordon},
  booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
  title     = {An Empirical Study to Determine If Mutants Can Effectively Simulate Students' Programming Mistakes to Increase Tutors' Confidence in Autograding},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1055–1061},
  publisher = {Association for Computing Machinery},
  series    = {SIGCSE '21},
  abstract  = {Automated grading often requires automated test suites to identify students' faults. However, tests may not detect some faults, limiting feedback, and providing inaccurate grades. This issue can be mitigated by first ensuring that tests can detect faults. Mutation analysis is a technique that generates artificial faulty variants of a program for this purpose, called mutants. Mutants that are not detected by tests reveal their inadequacies, providing knowledge on how they can be improved. By using mutants to improve test suites, tutors can gain the confidence that: a) generated grades will not be biased by unidentified faults, and b) students will receive appropriate feedback for their mistakes. Existing work has shown that mutants are suitable substitutes for faults in real world software, but no work has shown that this holds for students' faults. In this paper, we investigate whether mutants are capable of replicating mistakes made by students. We conducted a quantitative study on 197 Java classes written by students across three introductory programming assignments, and mutants generated from the assignments' model solutions. We found that generated mutants capture the observed faulty behaviour of students' solutions. We also found that mutants better assess test adequacy than code coverage in some cases. Our results indicate that tutors can use mutants to identify and remedy deficiencies in grading test suites.},
  doi       = {10.1145/3408877.3432411},
  isbn      = {9781450380621},
  keywords  = {mutation analysis, autograding, introductory programming},
  location  = {Virtual Event, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3408877.3432411},
}

@InProceedings{Wang2020,
  author    = {Wang, Zan and Yan, Ming and Chen, Junjie and Liu, Shuang and Zhang, Dongdi},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title     = {Deep Learning Library Testing via Effective Model Generation},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {788–799},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2020},
  abstract  = {Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice. However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains. Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems. In this work, we propose a novel approach, LEMON, to testing DL libraries. In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries. We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet. The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers. Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.},
  doi       = {10.1145/3368089.3409761},
  isbn      = {9781450370431},
  keywords  = {Library Testing, Deep Learning Testing, Search-based Software Testing, Mutation, Model Generation},
  location  = {Virtual Event, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3368089.3409761},
}

@InProceedings{Wong2020,
  author    = {Wong, Chu-Pan and Meinicke, Jens and Chen, Leo and Diniz, Jo\~{a}o P. and K\"{a}stner, Christian and Figueiredo, Eduardo},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title     = {Efficiently Finding Higher-Order Mutants},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {1165–1177},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2020},
  abstract  = {Higher-order mutation has the potential for improving major drawbacks of traditional first-order mutation, such as by simulating more realistic faults or improving test-optimization techniques. Despite interest in studying promising higher-order mutants, such mutants are difficult to find due to the exponential search space of mutation combinations. State-of-the-art approaches rely on genetic search, which is often incomplete and expensive due to its stochastic nature. First, we propose a novel way of finding a complete set of higher-order mutants by using variational execution, a technique that can, in many cases, explore large search spaces completely and often efficiently. Second, we use the identified complete set of higher-order mutants to study their characteristics. Finally, we use the identified characteristics to design and evaluate a new search strategy, independent of variational execution, that is highly effective at finding higher-order mutants even in large codebases.},
  doi       = {10.1145/3368089.3409713},
  isbn      = {9781450370431},
  keywords  = {higher-order mutant, mutation analysis, variational execution},
  location  = {Virtual Event, USA},
  numpages  = {13},
  url       = {https://doi.org/10.1145/3368089.3409713},
}

@InProceedings{HarelCanada2020,
  author    = {Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung},
  booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title     = {Is Neuron Coverage a Meaningful Measure for Testing Deep Neural Networks?},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {851–862},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2020},
  abstract  = {Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.},
  doi       = {10.1145/3368089.3409754},
  isbn      = {9781450370431},
  keywords  = {Neuron Coverage, Software Engineering, Machine Learning, Adversarial Attack, Testing},
  location  = {Virtual Event, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3368089.3409754},
}

@InProceedings{Souza2020,
  author    = {Souza, Beatriz and Machado, Patr\'{\i}cia},
  booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
  title     = {A Large Scale Study On the Effectiveness of Manual and Automatic Unit Test Generation},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {253–262},
  publisher = {Association for Computing Machinery},
  series    = {SBES '20},
  abstract  = {Recently, an increasingly large amount of effort has been devoted to implementing tools to generate unit test suites automatically. Previous studies have investigated the effectiveness of these tools by comparing automatically generated test suites (ATSs) to manually written test suites (MTSs). Most of these studies report that ATSs can achieve higher code coverage, or even mutation coverage, than MTSs, particularly when suites are generated from defective code. However, these studies usually consider a limited amount of classes or subject programs, while the adoption of such tools in the industry is still low. This work aims to compare the effectiveness of ATSs and MTSs when applied as regression test suites. We conduct an empirical study, using ten programs (1368 classes), written in Java, that already have MTSs and apply two sophisticated tools that automatically generate test cases: Randoop and EvoSuite. To evaluate the test suites' effectiveness, we use line and mutation coverage. Our results indicate that MTSs are, in general, more effective than ATSs regarding the investigated metrics. Moreover, the number of generated test cases may not indicate test suites' effectiveness. Furthermore, there are situations when ATSs are more effective, and even when ATSs and MTSs can be complementary.},
  doi       = {10.1145/3422392.3422407},
  isbn      = {9781450387538},
  keywords  = {mutation testing, automatic test generation, empirical studies},
  location  = {Natal, Brazil},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3422392.3422407},
}

@InProceedings{Soto2019,
  author    = {Soto, Mauricio},
  booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Improving Patch Quality by Enhancing Key Components of Automatic Program Repair},
  year      = {2019},
  address   = {San Diego, California},
  pages     = {1230–1233},
  publisher = {IEEE Press},
  series    = {ASE '19},
  abstract  = {The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes.},
  doi       = {10.1109/ASE.2019.00147},
  isbn      = {9781728125084},
  keywords  = {patch quality, Automatic Program Repair},
  numpages  = {4},
  url       = {https://doi.org/10.1109/ASE.2019.00147},
}

@InProceedings{EscobarVelasquez2019,
  author    = {Escobar-Velasquez, Camilo and Osorio-Ria\~{n}o, Michael and Linares-V\'{a}squez, Mario},
  booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
  title     = {MutAPK: Source-Codeless Mutant Generation for Android Apps},
  year      = {2019},
  address   = {San Diego, California},
  pages     = {1090–1093},
  publisher = {IEEE Press},
  series    = {ASE '19},
  abstract  = {The amount of Android application is having a tremendous increasing trend, exerting pressure over practitioners and researchers around application quality, frequent releases, and quick fixing of bugs. This pressure leads practitioners to make usage of automated approaches based on using source-code as input. Nevertheless, third-party services are not able to use these approaches due to privacy factors. In this paper we present MutAPK, an open source mutation testing tool that enables the usage of Android Application Packages (APKs) as input for this task. MutAPK generates mutants without the need of having access to source code, because the mutations are done in an intermediate representation of the code (i.e., SMALI) that does not require compilation. MutAPK is publicly available at GitHub: https://bit.ly/2KYvgP9 VIDEO: https://bit.ly/2WOjiyy},
  doi       = {10.1109/ASE.2019.00109},
  isbn      = {9781728125084},
  keywords  = {closed-source apps, Android, mutation testing},
  numpages  = {4},
  url       = {https://doi.org/10.1109/ASE.2019.00109},
}

@Article{Bertolino2019,
  author     = {Bertolino, Antonia and Angelis, Guglielmo De and Gallego, Micael and Garc\'{\i}a, Boni and Gort\'{a}zar, Francisco and Lonetti, Francesca and Marchetti, Eda},
  journal    = {ACM Comput. Surv.},
  title      = {A Systematic Review on Cloud Testing},
  year       = {2019},
  issn       = {0360-0300},
  month      = sep,
  number     = {5},
  volume     = {52},
  abstract   = {A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012--2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an active research field, although not all topics have received enough attention and conclude by presenting the most relevant open research challenges for each area of the classification framework.},
  address    = {New York, NY, USA},
  articleno  = {93},
  doi        = {10.1145/3331447},
  issue_date = {October 2019},
  keywords   = {systematic literature review, testing, Cloud computing},
  numpages   = {42},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3331447},
}

@InProceedings{Chen2019,
  author    = {Chen, Junjie and Han, Jiaqi and Sun, Peiyi and Zhang, Lingming and Hao, Dan and Zhang, Lu},
  booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  title     = {Compiler Bug Isolation via Effective Witness Test Program Generation},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {223–234},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2019},
  abstract  = {Compiler bugs are extremely harmful, but are notoriously difficult to debug because compiler bugs usually produce few debugging information. Given a bug-triggering test program for a compiler, hundreds of compiler files are usually involved during compilation, and thus are suspect buggy files. Although there are lots of automated bug isolation techniques, they are not applicable to compilers due to the scalability or effectiveness problem. To solve this problem, in this paper, we transform the compiler bug isolation problem into a search problem, i.e., searching for a set of effective witness test programs that are able to eliminate innocent compiler files from suspects. Based on this intuition, we propose an automated compiler bug isolation technique, DiWi, which (1) proposes a heuristic-based search strategy to generate such a set of effective witness test programs via applying our designed witnessing mutation rules to the given failing test program, and (2) compares their coverage to isolate bugs following the practice of spectrum-based bug isolation. The experimental results on 90 real bugs from popular GCC and LLVM compilers show that DiWi effectively isolates 66.67%/78.89% bugs within Top-10/Top-20 compiler files, significantly outperforming state-of-the-art bug isolation techniques.},
  doi       = {10.1145/3338906.3338957},
  isbn      = {9781450355728},
  keywords  = {Compiler Debugging, Bug Isolation, Test Program Generation},
  location  = {Tallinn, Estonia},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3338906.3338957},
}

@InProceedings{Ghanbari2019,
  author    = {Ghanbari, Ali and Benton, Samuel and Zhang, Lingming},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {Practical Program Repair via Bytecode Mutation},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {19–30},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2019},
  abstract  = {Automated Program Repair (APR) is one of the most recent advances in automated debugging, and can directly fix buggy programs with minimal human intervention. Although various advanced APR techniques (including search-based or semantic-based ones) have been proposed, they mainly work at the source-code level and it is not clear how bytecode-level APR performs in practice. Also, empirical studies of the existing techniques on bugs beyond what has been reported in the original papers are rather limited. In this paper, we implement the first practical bytecode-level APR technique, PraPR, and present the first extensive study on fixing real-world bugs (e.g., Defects4J bugs) using JVM bytecode mutation. The experimental results show that surprisingly even PraPR with only the basic traditional mutators can produce genuine fixes for 17 bugs; with simple additional commonly used APR mutators, PraPR is able to produce genuine fixes for 43 bugs, significantly outperforming state-of-the-art APR, while being over 10X faster. Furthermore, we performed an extensive study of PraPR and other recent APR tools on a large number of additional real-world bugs, and demonstrated the overfitting problem of recent advanced APR tools for the first time. Lastly, PraPR has also successfully fixed bugs for other JVM languages (e.g., for the popular Kotlin language), indicating PraPR can greatly complement existing source-code-level APR.},
  doi       = {10.1145/3293882.3330559},
  isbn      = {9781450362245},
  keywords  = {Program repair, JVM bytecode, Fault localization, Mutation testing},
  location  = {Beijing, China},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3293882.3330559},
}

@InProceedings{Degott2019,
  author    = {Degott, Christian and Borges Jr., Nataniel P. and Zeller, Andreas},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {Learning User Interface Element Interactions},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {296–306},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2019},
  abstract  = {When generating tests for graphical user interfaces, one central problem is to identify how individual UI elements can be interacted with—clicking, long- or right-clicking, swiping, dragging, typing, or more. We present an approach based on reinforcement learning that automatically learns which interactions can be used for which elements, and uses this information to guide test generation. We model the problem as an instance of the multi-armed bandit problem (MAB problem) from probability theory, and show how its traditional solutions work on test generation, with and without relying on previous knowledge. The resulting guidance yields higher coverage. In our evaluation, our approach shows improvements in statement coverage between 18% (when not using any previous knowledge) and 20% (when reusing previously generated models).},
  doi       = {10.1145/3293882.3330569},
  isbn      = {9781450362245},
  keywords  = {Android, Test generation, Multi-Armed Bandit problem, User interactions},
  location  = {Beijing, China},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3293882.3330569},
}

@InProceedings{Ardito2019,
  author    = {Ardito, Luca and Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
  booktitle = {Proceedings of the Evaluation and Assessment on Software Engineering},
  title     = {Espresso vs. EyeAutomate: An Experiment for the Comparison of Two Generations of Android GUI Testing},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {13–22},
  publisher = {Association for Computing Machinery},
  series    = {EASE '19},
  abstract  = {Context: Different approaches exist for automated GUI testing of Android applications, each with its peculiarities, advantages, and drawbacks. The most common are either based on the structure of the GUI or use visual recognition.Goal: In this paper, we present an empirical evaluation of two different GUI testing techniques with the use for each of a representative tool: (1) Visual GUI testing, with the use of EyeAutomate, and (2) Layout-based GUI testing, with the use of Espresso.Method: We conducted an experiment with a population of 78 graduate students. The participants of the study were asked to create the same test suite for a popular, open-source Android app (Omni-Notes) with both the tools, and to answer a survey about their preference to the one or the other, and the perceived difficulties when developing the test scripts.Results: By analyzing the outcomes of the delivered test suites (in terms of number of test scripts delivered and ratio of working ones) and the answers to the survey, we found that the participants showed similar productivity with both the tools, but the test suites developed with EyeAutomate were of higher quality (in terms of correctly working test scripts). The participants expressed a slight preference towards the EyeAutomate testing tool, reflecting a general complexity of Layout-based techniques -- represented by Espresso -- and some obstacles that may make the identification of components of the GUI quite a long and laborious task.Conclusions: The evidence we collected can provide useful hints for researchers aiming at making GUI testing techniques for mobile applications more usable and effective.},
  doi       = {10.1145/3319008.3319022},
  isbn      = {9781450371452},
  keywords  = {Mobile computing, Software Testing, Empirical Software Engineering},
  location  = {Copenhagen, Denmark},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3319008.3319022},
}

@InProceedings{Hilton2018,
  author    = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  title     = {A Large-Scale Study of Test Coverage Evolution},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {53–63},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2018},
  abstract  = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
  doi       = {10.1145/3238147.3238183},
  isbn      = {9781450359375},
  keywords  = {flaky tests, code coverage, Software testing, empirical study},
  location  = {Montpellier, France},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3238147.3238183},
}

@InProceedings{Yi2018,
  author    = {Yi, Jooyong and Tan, Shin Hwei and Mechtaev, Sergey and B\"{o}hme, Marcel and Roychoudhury, Abhik},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {A Correlation Study between Automated Program Repair and Test-Suite Metrics},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {24},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '18},
  abstract  = {Automated program repair has attracted attention due to its potential to reduce debugging cost. Prior works show the feasibility of automated repair, and the research focus is gradually shifting towards the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used. In this paper, 1we investigate the question: "Can traditional test-suite metrics used in software testing be used for automated program repair?". We empirically investigate the effectiveness of test-suite metrics (statement / branch coverage and mutation score) in controlling the reliability of repairs (the likelihood that repairs cause regressions). We conduct the largest-scale experiments to date with real-world software, and perform the first correlation study between test-suite metrics and the reliability of generated repairs. Our results show that by increasing test-suite metrics, the reliability of repairs tend to increase. Particularly, such trend is most strongly observed in statement coverage. This implies that traditional test-suite metrics used in software testing can also be used to improve the reliability of repairs in program repair.},
  doi       = {10.1145/3180155.3182517},
  isbn      = {9781450356381},
  location  = {Gothenburg, Sweden},
  numpages  = {1},
  url       = {https://doi.org/10.1145/3180155.3182517},
}

@InProceedings{Groce2018,
  author    = {Groce, Alex and Holmes, Josie and Marinov, Darko and Shi, August and Zhang, Lingming},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
  title     = {An Extensible, Regular-Expression-Based Tool for Multi-Language Mutant Generation},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {25–28},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '18},
  abstract  = {Mutation testing is widely used in research (even if not in practice). Mutation testing tools usually target only one programming language and rely on parsing a program to generate mutants, or operate not at the source level but on compiled bytecode. Unfortunately, developing a robust mutation testing tool for a new language in this paradigm is a difficult and time-consuming undertaking. Moreover, bytecode/intermediate language mutants are difficult for programmers to read and understand. This paper presents a simple tool, called universalmutator, based on regular-expression-defined transformations of source code. The primary drawback of such an approach is that our tool can generate invalid mutants that do not compile, and sometimes fails to generate mutants that a parser-based tool would have produced. Additionally, it is incompatible with some approaches to improving the efficiency of mutation testing. However, the regexp-based approach provides multiple compensating advantages. First, our tool is easy to adapt to new languages; e.g., we present here the first mutation tool for Apple's Swift programming language. Second, the method makes handling multi-language programs and systems simple, because the same tool can support every language. Finally, our approach makes it easy for users to add custom, project-specific mutations.},
  doi       = {10.1145/3183440.3183485},
  isbn      = {9781450356633},
  keywords  = {regular expressions, mutation testing, multi-language tools},
  location  = {Gothenburg, Sweden},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3183440.3183485},
}

@InProceedings{Tosun2018,
  author    = {Tosun, Ayse and Ahmed, Muzamil and Turhan, Burak and Juristo, Natalia},
  booktitle = {Proceedings of the 2018 International Conference on Software and System Process},
  title     = {On the Effectiveness of Unit Tests in Test-Driven Development},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {113–122},
  publisher = {Association for Computing Machinery},
  series    = {ICSSP '18},
  abstract  = {Background: Writing unit tests is one of the primary activities in test-driven development. Yet, the existing reviews report few evidence supporting or refuting the effect of this development approach on test case quality. Lack of ability and skills of developers to produce sufficiently good test cases are also reported as limitations of applying test-driven development in industrial practice. Objective: We investigate the impact of test-driven development on the effectiveness of unit test cases compared to an incremental test last development in an industrial context. Method: We conducted an experiment in an industrial setting with 24 professionals. Professionals followed the two development approaches to implement the tasks. We measure unit test effectiveness in terms of mutation score. We also measure branch and method coverage of test suites to compare our results with the literature. Results: In terms of mutation score, we have found that the test cases written for a test-driven development task have a higher defect detection ability than test cases written for an incremental test-last development task. Subjects wrote test cases that cover more branches on a test-driven development task compared to the other task. However, test cases written for an incremental test-last development task cover more methods than those written for the second task. Conclusion: Our findings are different from previous studies conducted at academic settings. Professionals were able to perform more effective unit testing with test-driven development. Furthermore, we observe that the coverage measure preferred in academic studies reveal different aspects of a development approach. Our results need to be validated in larger industrial contexts.},
  doi       = {10.1145/3202710.3203153},
  isbn      = {9781450364591},
  keywords  = {test-driven development, empirical study, mutation score, code coverage, unit testing},
  location  = {Gothenburg, Sweden},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3202710.3203153},
}

@InProceedings{Fellner2017,
  author    = {Fellner, Andreas and Krenn, Willibald and Schlick, Rupert and Tarrach, Thorsten and Weissenbacher, Georg},
  booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
  title     = {Model-Based, Mutation-Driven Test Case Generation via Heuristic-Guided Branching Search},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {56–66},
  publisher = {Association for Computing Machinery},
  series    = {MEMOCODE '17},
  abstract  = {This work introduces a heuristic-guided branching search algorithm for model-based, mutation-driven test case generation. The algorithm is designed towards the efficient and computationally tractable exploration of discrete, non-deterministic models with huge state spaces. Asynchronous parallel processing is a key feature of the algorithm. The algorithm is inspired by the successful path planning algorithm Rapidly exploring Random Trees (RRT). We adapt RRT in several aspects towards test case generation. Most notably, we introduce parametrized heuristics for start and successor state selection, as well as a mechanism to construct test cases from the data produced during search.We implemented our algorithm in the existing test case generation framework MoMuT. We present an extensive evaluation of our heuristics and parameters based on a diverse set of demanding models obtained in an industrial context. In total we continuously utilized 128 CPU cores on three servers for two weeks to gather the experimental data presented. Using statistical methods we determine which heuristics are performing well on all models. With our new algorithm, we are now able to process models consisting of over 2300 concurrent objects. To our knowledge there is no other mutation driven test case generation tool that is able to process models of this magnitude.},
  doi       = {10.1145/3127041.3127049},
  isbn      = {9781450350938},
  keywords  = {parallel search, mutation testing, model-based testing, heuristics, test case generation, search-based testing},
  location  = {Vienna, Austria},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3127041.3127049},
}

@InProceedings{Brown2017,
  author    = {Brown, David Bingham and Vaughn, Michael and Liblit, Ben and Reps, Thomas},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  title     = {The Care and Feeding of Wild-Caught Mutants},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {511–522},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2017},
  abstract  = {Mutation testing of a test suite and a program provides a way to measure the quality of the test suite. In essence, mutation testing is a form of sensitivity testing: by running mutated versions of the program against the test suite, mutation testing measures the suite's sensitivity for detecting bugs that a programmer might introduce into the program. This paper introduces a technique to improve mutation testing that we call wild-caught mutants; it provides a method for creating potential faults that are more closely coupled with changes made by actual programmers. This technique allows the mutation tester to have more certainty that the test suite is sensitive to the kind of changes that have been observed to have been made by programmers in real-world cases.},
  doi       = {10.1145/3106237.3106280},
  isbn      = {9781450351058},
  keywords  = {test suites, mutation testing, repository mining},
  location  = {Paderborn, Germany},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3106237.3106280},
}

@Article{Kazmi2017,
  author     = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
  journal    = {ACM Comput. Surv.},
  title      = {Effective Regression Test Case Selection: A Systematic Literature Review},
  year       = {2017},
  issn       = {0360-0300},
  month      = may,
  number     = {2},
  volume     = {50},
  abstract   = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
  address    = {New York, NY, USA},
  articleno  = {29},
  doi        = {10.1145/3057269},
  issue_date = {June 2017},
  keywords   = {Software testing, SLR, cost effectiveness, fault detection ability, coverage},
  numpages   = {32},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3057269},
}

@InProceedings{Gligoric2017,
  author    = {Gligoric, Milos and Khurshid, Sarfraz and Misailovic, Sasa and Shi, August},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track},
  title     = {Mutation Testing Meets Approximate Computing},
  year      = {2017},
  address   = {Buenos Aires, Argentina},
  pages     = {3–6},
  publisher = {IEEE Press},
  series    = {ICSE-NIER '17},
  abstract  = {One of the most widely studied techniques in software testing research is mutation testing - a technique for evaluating the quality of test suites. Despite over four decades of academic advances in this technique, mutation testing has not found its way to mainstream development. The key issue with mutation testing is its high computational cost: it requires running the test suite against not just the program under test but against typically thousands of mutants, i.e., syntactic variants, of the program. Our key insight is that exciting advances in the upcoming, yet unrelated, area of approximate computing allow us to define a principled approach that provides the benefits of traditional mutation testing at a fraction of its usually large cost.This paper introduces the idea of a novel approach, named ApproxiMut, that blends the power of mutation testing with the practicality of approximate computing. To demonstrate the potential of our approach, we present a concrete instantiation: rather than executing tests against each mutant on the exact program version, ApproxiMut obtains an approximate test/program version by applying approximate transformations and runs tests against each mutant on the approximated version. Our initial goal is to (1) measure the correlation between mutation scores on the exact and approximate program versions, (2) evaluate the relation among mutation operators and approximate transformations, (3) discover the best way to approximate a test and a program, and (4) evaluate the benefits of ApproxiMut. Our preliminary results show similar mutation scores on the exact and approximate program versions and uncovered a case when an approximated test was, to our surprise, better than the exact test.},
  doi       = {10.1109/ICSE-NIER.2017.15},
  isbn      = {9781538626757},
  numpages  = {4},
  url       = {https://doi.org/10.1109/ICSE-NIER.2017.15},
}

@InProceedings{Golagha2017,
  author    = {Golagha, Mojdeh and Pretschner, Alexander and Fisch, Dominik and Nagy, Roman},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
  title     = {Reducing Failure Analysis Time: An Industrial Evaluation},
  year      = {2017},
  address   = {Buenos Aires, Argentina},
  pages     = {293–302},
  publisher = {IEEE Press},
  series    = {ICSE-SEIP '17},
  abstract  = {Testing and debugging automotive cyber physical systems are challenging. Developing and integrating cyber and physical components require extensive testing to ensure reliable and safe releases. One important cost factor in the debugging process is the time required to analyze failures. Since large number of failures usually happen due to a few underlying faults, clustering failures based on the responsible faults helps reduce analysis time. We focus on the software-in-the-loop and hardware-in-the-loop levels of testing where test execution times are high. We devise a methodology for adapting existing clustering techniques to a real context. We augment an existing clustering approach by a method for selecting representative tests. To analyze failures, rather than investigating all failing tests one by one, testers inspect only these representatives. We report on the results of a large scale industrial case study. We ran experiments on ca. 850 KLOC. Results show that utilizing our clustering tool, testers can reduce failure analysis time by more than 80%.},
  doi       = {10.1109/ICSE-SEIP.2017.15},
  isbn      = {9781538627174},
  keywords  = {HiL testing, SiL testing, automotive CPS, failure clustering, failure analysis},
  numpages  = {10},
  url       = {https://doi.org/10.1109/ICSE-SEIP.2017.15},
}

@InProceedings{Rojas2017,
  author    = {Rojas, Jos\'{e} Miguel and White, Thomas D. and Clegg, Benjamin S. and Fraser, Gordon},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering},
  title     = {Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game},
  year      = {2017},
  address   = {Buenos Aires, Argentina},
  pages     = {677–688},
  publisher = {IEEE Press},
  series    = {ICSE '17},
  abstract  = {Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program; automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.},
  doi       = {10.1109/ICSE.2017.68},
  isbn      = {9781538638682},
  numpages  = {12},
  url       = {https://doi.org/10.1109/ICSE.2017.68},
}

@InProceedings{Jabbarvand2016,
  author    = {Jabbarvand, Reyhaneh and Sadeghi, Alireza and Bagheri, Hamid and Malek, Sam},
  booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
  title     = {Energy-Aware Test-Suite Minimization for Android Apps},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {425–436},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2016},
  abstract  = {The rising popularity of mobile apps deployed on battery-constrained devices has motivated the need for effective energy-aware testing techniques. Energy testing is generally more labor intensive and expensive than functional testing, as tests need to be executed in the deployment environment and specialized equipment needs to be used to collect energy measurements. Currently, there is a dearth of automatic mobile testing techniques that consider energy as a program property of interest. This paper presents an energy-aware test-suite minimization approach to significantly reduce the number of tests needed to effectively test the energy properties of an Android app. It relies on an energy-aware coverage criterion that indicates the degree to which energy-greedy segments of a program are tested. We describe and evaluate two complementary algorithms for test-suite minimization. Experiments over test suites provided for real-world apps have corroborated our ability to reduce the test suite size by 84% on average, while maintaining the effectiveness of test suite in revealing the great majority of energy bugs.},
  doi       = {10.1145/2931037.2931067},
  isbn      = {9781450343909},
  keywords  = {Green software engineering, Test-suite minimization, Coverage criterion, Android},
  location  = {Saarbr\"{u}cken, Germany},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2931037.2931067},
}

@InProceedings{Khalsa2016,
  author    = {Khalsa, Sunint Kaur and Labiche, Yvan and Nicoletta, Johanna},
  booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {The Power of Single and Error Annotations in Category Partition Testing: An Experimental Evaluation},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '16},
  abstract  = {Category Partition (CP) is a black box testing technique that formalizes the specification of the input domain in a CP specification for the system under test. A CP specification is driven by tester's expertise and bundles parameters, categories (characteristics of parameters) and choices (acceptable values for categories) required for extensively testing the system. For completeness the choices correspond to permitted input values as well as some values to account for boundaries or robustness. These choices are then combined to form test frames on the basis of various criteria such as each choice or pairwise. To ensure that the combinations of choices are feasible and account for valid sets of user requirements, constraints are introduced to specify permitted combinations among choices, and to specify single or error choices. In a typical development environment where testing is driven by stringent deadlines a tester might have to decide how many constraints are enough to attain the maximum level of test completeness. The present work will assist a test engineer in making this decision. We conclude, on the basis of our experimental evaluation on academic and industrial case studies, that an equally effective test suite can be attained by meticulously defining error and single annotations in a CP specification while ignoring other constraints among choices.},
  articleno = {28},
  doi       = {10.1145/2915970.2915999},
  isbn      = {9781450336918},
  keywords  = {category partition, constraints, error annotations, single annotations, each choice, pairwise},
  location  = {Limerick, Ireland},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2915970.2915999},
}

@InProceedings{Parsai2016a,
  author    = {Parsai, Ali and Murgia, Alessandro and Demeyer, Serge},
  booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Evaluating Random Mutant Selection at Class-Level in Projects with Non-Adequate Test Suites},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {EASE '16},
  abstract  = {Mutation testing is a standard technique to evaluate the quality of a test suite. Due to its computationally intensive nature, many approaches have been proposed to make this technique feasible in real case scenarios. Among these approaches, uniform random mutant selection has been demonstrated to be simple and promising. However, works on this area analyze mutant samples at project level mainly on projects with adequate test suites. In this paper, we fill this lack of empirical validation by analyzing random mutant selection at class level on projects with non-adequate test suites. First, we show that uniform random mutant selection underachieves the expected results. Then, we propose a new approach named weighted random mutant selection which generates more representative mutant samples. Finally, we show that representative mutant samples are larger for projects with high test adequacy.},
  articleno = {11},
  doi       = {10.1145/2915970.2915992},
  isbn      = {9781450336918},
  location  = {Limerick, Ireland},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2915970.2915992},
}

@Article{Gligoric2015,
  author     = {Gligoric, Milos and Groce, Alex and Zhang, Chaoqiang and Sharma, Rohan and Alipour, Mohammad Amin and Marinov, Darko},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {Guidelines for Coverage-Based Comparisons of Non-Adequate Test Suites},
  year       = {2015},
  issn       = {1049-331X},
  month      = sep,
  number     = {4},
  volume     = {24},
  abstract   = {A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the feasible requirements is called C-adequate. Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given two criteria C and C′, are C-adequate suites on average more effective than C′-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible.This article presents the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given two criteria C and C′, which one is better to use to compare test suites? Namely, if suites T1, T2,…,Tn have coverage values c1, c2,…,cn for C and c1′, c2′,…,cn′ for C′, is it better to compare suites based on c1, c2,…,cn or based on c1′, c2′,…,cn′? We evaluate a large set of plausible criteria, including basic criteria such as statement and branch coverage, as well as stronger criteria used in recent studies, including criteria based on program paths, equivalence classes of covered statements, and predicate states. The criteria are evaluated on a set of Java and C programs with both manually written and automatically generated test suites. The evaluation uses three correlation measures. Based on these experiments, two criteria perform best: branch coverage and an intraprocedural acyclic path coverage. We provide guidelines for testing researchers aiming to evaluate test suites using coverage criteria as well as for other researchers evaluating coverage criteria for research use.},
  address    = {New York, NY, USA},
  articleno  = {22},
  doi        = {10.1145/2660767},
  issue_date = {August 2015},
  keywords   = {non-adequate test suites, Coverage criteria},
  numpages   = {33},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2660767},
}

@InProceedings{Smith2015,
  author    = {Smith, Edward K. and Barr, Earl T. and Le Goues, Claire and Brun, Yuriy},
  booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
  title     = {Is the Cure Worse than the Disease? Overfitting in Automated Program Repair},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {532–543},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2015},
  abstract  = {Automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TrpAutoRepair, on a publicly available benchmark of bugs, each with a human-written patch. By evaluating patches using tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs that pass most tests, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair performs no worse than these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, and starting program quality, as well as the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with a test suite independent from the one used for patch generation.},
  doi       = {10.1145/2786805.2786825},
  isbn      = {9781450336758},
  keywords  = {GenProg, automated program repair, independent evaluation, IntroClass, TrpAutoRepair, empirical evaluation},
  location  = {Bergamo, Italy},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2786805.2786825},
}

@InProceedings{Clapp2015,
  author    = {Clapp, Lazaro and Anand, Saswat and Aiken, Alex},
  booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
  title     = {Modelgen: Mining Explicit Information Flow Specifications from Concrete Executions},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {129–140},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2015},
  abstract  = {We present a technique to mine explicit information flow specifications from concrete executions. These specifications can be consumed by a static taint analysis, enabling static analysis to work even when method definitions are missing or portions of the program are too difficult to analyze statically (e.g., due to dynamic features such as reflection). We present an implementation of our technique for the Android platform. When compared to a set of manually written specifications for 309 methods across 51 classes, our technique is able to recover 96.36% of these manual specifications and produces many more correct annotations that our manual models missed. We incorporate the generated specifications into an existing static taint analysis system, and show that they enable it to find additional true flows. Although our implementation is Android-specific, our approach is applicable to other application frameworks.},
  doi       = {10.1145/2771783.2771810},
  isbn      = {9781450336208},
  keywords  = {information flow, specification mining, Dynamic analysis},
  location  = {Baltimore, MD, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2771783.2771810},
}

@InProceedings{Zhang2015,
  author    = {Zhang, Jie},
  booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
  title     = {Scalability Studies on Selective Mutation Testing},
  year      = {2015},
  address   = {Florence, Italy},
  pages     = {851–854},
  publisher = {IEEE Press},
  series    = {ICSE '15},
  abstract  = {Mutation testing is a test method which is designed to evaluate a test suite's quality. Due to the expensive cost of mutation testing, selective mutation testing was first proposed in 1991 by Mathur, in which a subset of mutants are selected aiming to achieve the same effectiveness as the whole set of mutants in evaluating the quality of test suites. Though selective mutation testing has been widely investigated in recent years, many people still doubt if it can suit well for large programs. Realizing that none of the existing work has systematically studied the scalability of selective mutation testing, I plan to work on the scalability of selective mutation testing through several studies.},
  doi       = {10.1109/icse.2015.276},
  numpages  = {4},
}

@InProceedings{Hauptmann2015,
  author    = {Hauptmann, Benedikt and Juergens, Elmar and Woinke, Volkmar},
  booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
  title     = {Generating Refactoring Proposals to Remove Clones from Automated System Tests},
  year      = {2015},
  address   = {Florence, Italy},
  pages     = {115–124},
  publisher = {IEEE Press},
  series    = {ICPC '15},
  abstract  = {Automated system tests often have many clones, which make them complex to understand and costly to maintain. Unfortunately, removing clones is challenging as there are numerous possibilities of how to refactor them to reuse components such as subroutines. Additionally, clones often overlap partly which makes it particularly difficult to decide which parts to extract. If done wrongly, reuse potential is not leveraged optimally and structures between tests and reuse components will become unnecessarily complex. We present a method to support test engineers in extracting overlapping clones. Using grammar inference algorithms, we generate a refactoring proposal that demonstrates test engineers how overlapping clones can be extracted. Furthermore, we visualize the generated refactoring proposal to make it easily understandable for test engineers. An industrial case study demonstrates that our approach helps test engineers to gain information of the reuse potential of test suites and guides them to perform refactorings.},
  doi       = {10.1109/icpc.2015.20},
  keywords  = {test clones, refactoring, automated testing},
  numpages  = {10},
}

@InProceedings{Khalili2015,
  author    = {Khalili, Ali and Narizzano, Massimo and Tacchella, Armando and Giunchiglia, Enrico},
  booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
  title     = {Automatic Test-Pattern Generation for Grey-Box Programs},
  year      = {2015},
  address   = {Florence, Italy},
  pages     = {33–37},
  publisher = {IEEE Press},
  series    = {AST '15},
  abstract  = {In the context of structural testing, automatic test-pattern generation (ATPG) may fail to provide suites covering 100% of the testing requirements for grey-box programs, i.e., applications wherein source code is available for some parts (white-box), but not for others (black-box). Furthermore, test suites based on abstract models may elicit behaviors on the actual program that diverge from the intended ones. In this paper, we present a new ATPG methodology to reduce divergence without increasing manual effort. This is achieved by (i) learning models of black-box components as finite-state machines, and (ii) composing the learnt models with the white-box components to generate test-suites for the grey-box program. Experiments with a prototypical implementation of our methodology show that it yields measurable improvements over two comparable state-of-the-art solutions.},
  doi       = {10.1109/ast.2015.14},
  numpages  = {5},
}

@InProceedings{Daoudagh2015,
  author    = {Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda},
  booktitle = {Proceedings of the First International Workshop on TEchnical and LEgal Aspects of Data PRIvacy},
  title     = {Assessment of Access Control Systems Using Mutation Testing},
  year      = {2015},
  address   = {Florence, Italy},
  pages     = {8–13},
  publisher = {IEEE Press},
  series    = {TELERISE '15},
  abstract  = {In modern pervasive applications, it is important to validate access control mechanisms that are usually defined by means of the standard XACML language. Mutation analysis has been applied on access control policies for measuring the adequacy of a test suite. In this paper, we present a testing framework aimed at applying mutation analysis at the level of the Java based policy evaluation engine. A set of Java based mutation operators is selected and applied to the code of the Policy Decision Point (PDP). A first experiment shows the effectiveness of the proposed framework in assessing the fault detection of XACML test suites and confirms the efficacy of the application of code-based mutation operators to the PDP.},
  doi       = {10.1109/telerise.2015.10},
  numpages  = {6},
}

@InProceedings{Shams2015,
  author    = {Shams, Zalia and Edwards, Stephen H.},
  booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
  title     = {Checked Coverage and Object Branch Coverage: New Alternatives for Assessing Student-Written Tests},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {534–539},
  publisher = {Association for Computing Machinery},
  series    = {SIGCSE '15},
  abstract  = {Many educators currently use code coverage metrics to assess student-written software tests. While test adequacy criteria such as statement or branch coverage can also be used to measure the thoroughness of a test suite, they have limitations. Coverage metrics assess what percentage of code has been exercised, but do not depend on whether a test suite adequately checks that the expected behavior is achieved. This paper evaluates checked coverage, an alternative measure of test thoroughness aimed at overcoming this limitation, along with object branch coverage, a structure code coverage metric that has received little discussion in educational assessment. Checked coverage works backwards from behavioral assertions in test cases, measuring the dynamic slice of the executed code that actually influences the outcome of each assertion. Object branch coverage (OBC) is a stronger coverage criterion similar to weak variants of modified condition/decision coverage. We experimentally compare checked coverage and OBC against statement coverage, branch coverage, mutation analysis, and all-pairs testing to evaluate which is the best predictor of how likely a test suite is to detect naturally occurring defects. While checked coverage outperformed other coverage measures in our experiment, followed closely by OBC, both were only weakly correlated with a test suite's ability to detect naturally occurring defects produced by students in the final versions of their programs. Still, OBC appears to be an improved and practical alternative to existing statement and branch coverage measures, while achieving nearly the same benefits as checked coverage.},
  doi       = {10.1145/2676723.2677300},
  isbn      = {9781450329668},
  keywords  = {checked coverage, statement coverage, branch coverage, software testing, mutation testing, automated grading, programming assignments, automated assessment, modified condition/decision coverage, test quality, test metrics, test coverage},
  location  = {Kansas City, Missouri, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2676723.2677300},
}

@Article{Bokil2015,
  author     = {Bokil, Prasad and Krishnan, Padmanabhan and Venkatesh, R.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {Achieving Effective Test Suites for Reactive Systems Using Specification Mining and Test Suite Reduction Techniques},
  year       = {2015},
  issn       = {0163-5948},
  month      = feb,
  number     = {1},
  pages      = {1–8},
  volume     = {40},
  abstract   = {Failures in reactive embedded systems are often unacceptable. Moreover, effective testing of such systems to detect potential critical failures is a difficult task.We present an automated black box test suite generation technique for reactive systems. The technique is based on dynamic mining of specifications, in form of a finite state machine (FSM), from initial runs. The set of test cases thus produced contain several redundant test cases, many of which are eliminated by a simple greedy test suite reduction algorithm to give the final test suite. The effectiveness of tests generated by our technique was evaluated using five case studies from the reactive embedded domain. Results indicate that a test suite generated by our technique is promising in terms of effectiveness and scalability. While the test suite reduction algorithm removes redundant test cases, the change in effectiveness of test suites due to this reduction is examined in the experimentation.We present our specification mining based test suite generation technique, the test suite reduction technique and results on industrial case studies.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2693208.2693226},
  issue_date = {January 2015},
  keywords   = {test suite reduction, specification mining, black box testing},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2693208.2693226},
}

@Article{Li2014,
  author     = {Li, Kaituo and Reichenbach, Christoph and Csallner, Christoph and Smaragdakis, Yannis},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {Residual Investigation: Predictive and Precise Bug Detection},
  year       = {2014},
  issn       = {1049-331X},
  month      = dec,
  number     = {2},
  volume     = {24},
  abstract   = {We introduce the concept of residual investigation for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite that are pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when considered in conjunction with the static error report, increase confidence in the existence or severity of an error without needing to directly observe a fault resulting from the error.We enhance the static analyzer FindBugs with several residual investigations appropriately tuned to the static error patterns in FindBugs, and apply it to nine large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (false positives) while reporting several actual errors that would not have been detected by mere execution of a program's test suite.},
  address    = {New York, NY, USA},
  articleno  = {7},
  doi        = {10.1145/2656201},
  issue_date = {December 2014},
  keywords   = {False warnings, existing test cases, RFBI},
  numpages   = {32},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2656201},
}

@InProceedings{Just2014,
  author    = {Just, Ren\'{e} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
  booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  title     = {Are Mutants a Valid Substitute for Real Faults in Software Testing?},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {654–665},
  publisher = {Association for Computing Machinery},
  series    = {FSE 2014},
  abstract  = {A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults -- each one a simple syntactic variation -- that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite’s ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automatically-generated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations.},
  doi       = {10.1145/2635868.2635929},
  isbn      = {9781450330565},
  keywords  = {Test effectiveness, mutation analysis, code coverage, real faults},
  location  = {Hong Kong, China},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2635868.2635929},
}

@InProceedings{Groce2014,
  author    = {Groce, Alex and Alipour, Mohammad Amin and Gopinath, Rahul},
  booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
  title     = {Coverage and Its Discontents},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {255–268},
  publisher = {Association for Computing Machinery},
  series    = {Onward! 2014},
  abstract  = {Everyone wants to know one thing about a test suite: will it detect enough bugs? Unfortunately, in most settings that matter, answering this question directly is impractical or impossible. Software engineers and researchers therefore tend to rely on various measures of code coverage (where mutation testing is considered a form of syntactic coverage). A long line of academic research efforts have attempted to determine whether relying on coverage as a substitute for fault detection is a reasonable solution to the problems of test suite evaluation. This essay argues that the profusion of coverage-related literature is in part a sign of an underlying uncertainty as to what exactly it is that measuring coverage should achieve, as well as how we would know if it can, in fact, achieve it. We propose some solutions and mitigations, but the primary focus of this essay is to clarify the state of current confusions regarding this key problem for effective software testing.},
  doi       = {10.1145/2661136.2661157},
  isbn      = {9781450332101},
  keywords  = {testing, coverage, evaluation},
  location  = {Portland, Oregon, USA},
  numpages  = {14},
  url       = {https://doi.org/10.1145/2661136.2661157},
}

@InProceedings{Herzig2014,
  author    = {Herzig, Kim and Nagappan, Nachiappan},
  booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
  title     = {The Impact of Test Ownership and Team Structure on the Reliability and Effectiveness of Quality Test Runs},
  year      = {2014},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ESEM '14},
  abstract  = {Context: Software testing is a crucial step in most software development processes. Testing software is a key component to manage and assess the risk of shipping quality products to customers. But testing is also an expensive process and changes to the system need to be tested thoroughly which may take time. Thus, the quality of a software product depends on the quality of its underlying testing process and on the effectiveness and reliability of individual test cases.Goal: In this paper, we investigate the impact of the organizational structure of test owners on the reliability and effectiveness of the corresponding test cases. Prior empirical research on organizational structure has focused only on developer activity. We expand the scope of empirical knowledge by assessing the impact of organizational structure on testing activities.Method: We performed an empirical study on the Windows build verification test suites (BVT) and relate effectiveness and reliability measures of each test run to the complexity and size of the organizational sub-structure that enclose all owners of test cases executed.Results: Our results show, that organizational structure impacts both test effectiveness and test execution reliability. We are also able to predict effectiveness and reliability with fairly high precision and recall values.Conclusion: We suggest to review test suites with respect to their organizational composition. As indicated by the results of this study, this would increase the effectiveness and reliability, development speed and developer satisfaction.},
  articleno = {2},
  doi       = {10.1145/2652524.2652535},
  isbn      = {9781450327749},
  keywords  = {software testing, reliability, effectiveness, empirical software engineering, organizational structure},
  location  = {Torino, Italy},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2652524.2652535},
}

@InProceedings{Giannakopoulou2014,
  author    = {Giannakopoulou, Dimitra and Howar, Falk and Isberner, Malte and Lauderdale, Todd and Rakamari\'{c}, Zvonimir and Raman, Vishwanath},
  booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
  title     = {Taming Test Inputs for Separation Assurance},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {373–384},
  publisher = {Association for Computing Machinery},
  series    = {ASE '14},
  abstract  = {The Next Generation Air Transportation System (NextGen) advocates the use of innovative algorithms and software to address the increasing load on air-traffic control. AutoResolver [12] is a large, complex NextGen component that provides separation assurance between multiple airplanes up to 20 minutes ahead of time. Our work targets the development of a light-weight, automated testing environment for AutoResolver. The input space of AutoResolver consists of airplane trajectories, each trajectory being a sequence of hundreds of points in the three-dimensional space. Generating meaningful test cases for AutoResolver that cover its behavioral space to a satisfactory degree is a major challenge. We discuss how we tamed this input space to make it amenable to test case generation techniques, as well as how we developed and validated an extensible testing environment around AutoResolver.},
  doi       = {10.1145/2642937.2642940},
  isbn      = {9781450330138},
  keywords  = {test case generation, test coverage, air-traffic control},
  location  = {Vasteras, Sweden},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2642937.2642940},
}

@InProceedings{Just2014a,
  author    = {Just, Ren\'{e} and Ernst, Michael D. and Fraser, Gordon},
  booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
  title     = {Efficient Mutation Analysis by Propagating and Partitioning Infected Execution States},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {315–326},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2014},
  abstract  = {Mutation analysis evaluates a testing technique by measur- ing how well it detects seeded faults (mutants). Mutation analysis is hampered by inherent scalability problems — a test suite is executed for each of a large number of mutants. Despite numerous optimizations presented in the literature, this scalability issue remains, and this is one of the reasons why mutation analysis is hardly used in practice. Whereas most previous optimizations attempted to stati- cally reduce the number of executions or their computational overhead, this paper exploits information available only at run time to further reduce the number of executions. First, state infection conditions can reveal — with a single test execution of the unmutated program — which mutants would lead to a different state, thus avoiding unnecessary test executions. Second, determining whether an infected execution state propagates can further reduce the number of executions. Mutants that are embedded in compound expressions may infect the state locally without affecting the outcome of the compound expression. Third, those mutants that do infect the state can be partitioned based on the resulting infected state — if two mutants lead to the same infected state, only one needs to be executed as the result of the other can be inferred. We have implemented these optimizations in the Major mu- tation framework and empirically evaluated them on 14 open source programs. The optimizations reduced the mutation analysis time by 40% on average.},
  doi       = {10.1145/2610384.2610388},
  isbn      = {9781450326452},
  keywords  = {software testing, Mutation analysis, dynamic analysis},
  location  = {San Jose, CA, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2610384.2610388},
}

@InProceedings{Mirzaaghaei2014,
  author    = {Mirzaaghaei, Mehdi and Mesbah, Ali},
  booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
  title     = {DOM-Based Test Adequacy Criteria for Web Applications},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {71–81},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2014},
  abstract  = {To assess the quality of web application test cases, web developers currently measure code coverage. Although code coverage has traditionally been a popular test adequacy criterion, we believe it alone is not adequate for assessing the quality of web application test cases. We propose a set of novel DOM-based test adequacy criteria for web applications. These criteria aim at measuring coverage at two granularity levels, (1) the percentage of DOM states and transitions covered in the total state space of the web application under test, and (2) the percentage of elements covered in each particular DOM state. We present a technique and tool, called DomCovery, which automatically extracts and measures the proposed adequacy criteria and generates a visual DOM coverage report. Our evaluation shows that there is no correlation between code coverage and DOM coverage. A controlled experiment illustrates that participants using DomCovery completed coverage related tasks 22% more accurately and 66% faster.},
  doi       = {10.1145/2610384.2610406},
  isbn      = {9781450326452},
  keywords  = {web applications, coverage, Test adequacy criteria, DOM},
  location  = {San Jose, CA, USA},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2610384.2610406},
}

@InProceedings{Gopinath2014,
  author    = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  title     = {Code Coverage for Suite Evaluation by Developers},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {72–82},
  publisher = {Association for Computing Machinery},
  series    = {ICSE 2014},
  abstract  = {One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best.},
  doi       = {10.1145/2568225.2568278},
  isbn      = {9781450327565},
  keywords  = {evaluation of coverage criteria, statistical analysis, test frameworks},
  location  = {Hyderabad, India},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2568225.2568278},
}

@InProceedings{Edwards2014,
  author    = {Edwards, Stephen H. and Shams, Zalia},
  booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
  title     = {Comparing Test Quality Measures for Assessing Student-Written Tests},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {354–363},
  publisher = {Association for Computing Machinery},
  series    = {ICSE Companion 2014},
  abstract  = {Many educators now include software testing activities in programming assignments, so there is a growing demand for appropriate methods of assessing the quality of student-written software tests. While tests can be hand-graded, some educators also use objective performance metrics to assess software tests. The most common measures used at present are code coverage measures—tracking how much of the student’s code (in terms of statements, branches, or some combination) is exercised by the corresponding software tests. Code coverage has limitations, however, and sometimes it overestimates the true quality of the tests. Some researchers have suggested that mutation analysis may provide a better indication of test quality, while some educators have experimented with simply running every student’s test suite against every other student’s program—an “all-pairs” strategy that gives a bit more insight into the quality of the tests. However, it is still unknown which one of these measures is more accurate, in terms of most closely predicting the true bug revealing capability of a given test suite. This paper directly compares all three methods of measuring test quality in terms of how well they predict the observed bug revealing capabilities of student-written tests when run against a naturally occurring collection of student-produced defects. Experimental results show that all-pairs testing—running each student’s tests against every other student’s solution—is the most effective predictor of the underlying bug revealing capability of a test suite. Further, no strong correlation was found between bug revealing capability and either code coverage or mutation analysis scores.},
  doi       = {10.1145/2591062.2591164},
  isbn      = {9781450327688},
  keywords  = {test coverage, automated assessment, test metrics, automated grading, test quality, mutation testing, programming assignments, Software testing},
  location  = {Hyderabad, India},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2591062.2591164},
}

@InProceedings{Yao2014,
  author    = {Yao, Xiangjuan and Harman, Mark and Jia, Yue},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  title     = {A Study of Equivalent and Stubborn Mutation Operators Using Human Analysis of Equivalence},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {919–930},
  publisher = {Association for Computing Machinery},
  series    = {ICSE 2014},
  abstract  = {Though mutation testing has been widely studied for more than thirty years, the prevalence and properties of equivalent mutants remain largely unknown. We report on the causes and prevalence of equivalent mutants and their relationship to stubborn mutants (those that remain undetected by a high quality test suite, yet are non-equivalent). Our results, based on manual analysis of 1,230 mutants from 18 programs, reveal a highly uneven distribution of equivalence and stubbornness. For example, the ABS class and half UOI class generate many equivalent and almost no stubborn mutants, while the LCR class generates many stubborn and few equivalent mutants. We conclude that previous test effectiveness studies based on fault seeding could be skewed, while developers of mutation testing tools should prioritise those operators that we found generate disproportionately many stubborn (and few equivalent) mutants.},
  doi       = {10.1145/2568225.2568265},
  isbn      = {9781450327565},
  keywords  = {Mutation Testing, Stubborn Mutant, Equivalent Mutant},
  location  = {Hyderabad, India},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2568225.2568265},
}

@InProceedings{Zhang2013,
  author    = {Zhang, Lingming and Gligoric, Milos and Marinov, Darko and Khurshid, Sarfraz},
  booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Operator-Based and Random Mutant Selection: Better Together},
  year      = {2013},
  address   = {Silicon Valley, CA, USA},
  pages     = {92–102},
  publisher = {IEEE Press},
  series    = {ASE'13},
  abstract  = {Mutation testing is a powerful methodology for evaluating the quality of a test suite. However, the methodology is also very costly, as the test suite may have to be executed for each mutant. Selective mutation testing is a well-studied technique to reduce this cost by selecting a subset of all mutants, which would otherwise have to be considered in their entirety. Two common approaches are operator-based mutant selection, which only generates mutants using a subset of mutation operators, and random mutant selection, which selects a subset of mutants generated using all mutation operators. While each of the two approaches provides some reduction in the number of mutants to execute, applying either of the two to medium-sized, realworld programs can still generate a huge number of mutants, which makes their execution too expensive. This paper presents eight random sampling strategies defined on top of operatorbased mutant selection, and empirically validates that operatorbased selection and random selection can be applied in tandem to further reduce the cost of mutation testing. The experimental results show that even sampling only 5% of mutants generated by operator-based selection can still provide precise mutation testing results, while reducing the average mutation testing time to 6.54% (i.e., on average less than 5 minutes for this study).},
  doi       = {10.1109/ASE.2013.6693070},
  isbn      = {9781479902156},
  numpages  = {11},
  url       = {https://doi.org/10.1109/ASE.2013.6693070},
}

@InProceedings{Zhang2013a,
  author    = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
  booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
  title     = {Injecting Mechanical Faults to Localize Developer Faults for Evolving Software},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {765–784},
  publisher = {Association for Computing Machinery},
  series    = {OOPSLA '13},
  abstract  = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art FaultTracer technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.},
  doi       = {10.1145/2509136.2509551},
  isbn      = {9781450323741},
  keywords  = {fault localization, software evolution, mutation testing, regression testing},
  location  = {Indianapolis, Indiana, USA},
  numpages  = {20},
  url       = {https://doi.org/10.1145/2509136.2509551},
}

@Article{Zhang2013b,
  author     = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
  journal    = {SIGPLAN Not.},
  title      = {Injecting Mechanical Faults to Localize Developer Faults for Evolving Software},
  year       = {2013},
  issn       = {0362-1340},
  month      = oct,
  number     = {10},
  pages      = {765–784},
  volume     = {48},
  abstract   = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art FaultTracer technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.},
  address    = {New York, NY, USA},
  doi        = {10.1145/2544173.2509551},
  issue_date = {October 2013},
  keywords   = {software evolution, fault localization, mutation testing, regression testing},
  numpages   = {20},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2544173.2509551},
}

@InProceedings{Marinescu2013,
  author    = {Marinescu, Paul Dan and Cadar, Cristian},
  booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
  title     = {KATCH: High-Coverage Testing of Software Patches},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {235–245},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2013},
  abstract  = {One of the distinguishing characteristics of software systems is that they evolve: new patches are committed to software repositories and new versions are released to users on a continuous basis. Unfortunately, many of these changes bring unexpected bugs that break the stability of the system or affect its security. In this paper, we address this problem using a technique for automatically testing code patches. Our technique combines symbolic execution with several novel heuristics based on static and dynamic program analysis which allow it to quickly reach the code of the patch. We have implemented our approach in a tool called KATCH, which we have applied to all the patches written in a combined period of approximately six years for nineteen mature programs from the popular GNU diffutils, GNU binutils and GNU findutils utility suites, which are shipped with virtually all UNIX-based distributions. Our results show that KATCH can automatically synthesise inputs that significantly increase the patch coverage achieved by the existing manual test suites, and find bugs at the moment they are introduced.},
  doi       = {10.1145/2491411.2491438},
  isbn      = {9781450322379},
  keywords  = {Symbolic Execution, Patch Testing},
  location  = {Saint Petersburg, Russia},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2491411.2491438},
}

@InProceedings{Gligoric2013,
  author    = {Gligoric, Milos and Groce, Alex and Zhang, Chaoqiang and Sharma, Rohan and Alipour, Mohammad Amin and Marinov, Darko},
  booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
  title     = {Comparing Non-Adequate Test Suites Using Coverage Criteria},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {302–313},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2013},
  abstract  = {A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the (feasible) requirements is C-adequate.  Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given criteria C and C′, are C-adequate suites (on average) more effective than C′-adequate suites? However, in many realistic cases producing adequate suites is impractical or even impossible. We present the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given criteria C and C′, which one is better to use to compare test suites? Namely, if suites T1, T2 . . . Tn have coverage values c1, c2 . . . cn for C and c′1, c′2 . . . c′n for C′, is it better to compare suites based on c1, c2 . . . cn or based on c′1, c′ 2 . . . c′n?  We evaluate a large set of plausible criteria, including statement and branch coverage, as well as stronger criteria used in recent studies. Two criteria perform best: branch coverage and an intra-procedural acyclic path coverage.},
  doi       = {10.1145/2483760.2483769},
  isbn      = {9781450321594},
  keywords  = {non-adequate test suites, Coverage criteria},
  location  = {Lugano, Switzerland},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2483760.2483769},
}

@InProceedings{Zhang2013c,
  author    = {Zhang, Lingming and Marinov, Darko and Khurshid, Sarfraz},
  booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
  title     = {Faster Mutation Testing Inspired by Test Prioritization and Reduction},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {235–245},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2013},
  abstract  = {Mutation testing is a well-known but costly approach for determining test adequacy. The central idea behind the approach is to generate mutants, which are small syntactic transformations of the program under test, and then to measure for a given test suite how many mutants it kills. A test t is said to kill a mutant m of program p if the output of t on m is different from the output of t on p. The effectiveness of mutation testing in determining the quality of a test suite relies on the ability to apply it using a large number of mutants. However, running many tests against many mutants is time consuming. We present a family of techniques to reduce the cost of mutation testing by prioritizing and reducing tests to more quickly determine the sets of killed and non-killed mutants. Experimental results show the effectiveness and efficiency of our techniques.},
  doi       = {10.1145/2483760.2483782},
  isbn      = {9781450321594},
  keywords  = {Test Reduction, Mutation testing, Test Prioritization},
  location  = {Lugano, Switzerland},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2483760.2483782},
}

@InProceedings{Jehan2013,
  author    = {Jehan, Seema and Pill, Ingo and Wotawa, Franz},
  booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
  title     = {Functional SOA Testing Based on Constraints},
  year      = {2013},
  address   = {San Francisco, California},
  pages     = {33–39},
  publisher = {IEEE Press},
  series    = {AST '13},
  abstract  = {In the fierce competition on today's software market, Service-Oriented Architectures (SOAs) are an established design paradigm. Essential concepts like modularization, reuse, and the corresponding IP core business are inherently supported in the development and operation of SOAs that offer flexibility in many aspects and thus optimal conditions also for heterogeneous system developments. The intrinsics of large and complex SOA enterprises, however, require us to adopt and evolve our verification technology, in order to achieve expected software quality levels. In this paper, we contribute to this challenge by proposing a constraint based testing approach for SOAs. In our work, we augment a SOA's BPEL business model with pre- and postcondition contracts defining essential component traits, and derive a suite of feasible test cases to be executed after assessing its quality via corresponding coverage criteria. We illustrate our approach's viability via a running example as well as experimental results, and discuss current and envisioned automation levels in the context of a test and diagnosis workflow.},
  doi       = {10.1109/iwast.2013.6595788},
  isbn      = {9781467361613},
  numpages  = {7},
}

@InProceedings{Selim2012,
  author    = {Selim, Gehan M. K. and Cordy, James R. and Dingel, Juergen},
  booktitle = {Proceedings of the First Workshop on the Analysis of Model Transformations},
  title     = {Model Transformation Testing: The State of the Art},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {21–26},
  publisher = {Association for Computing Machinery},
  series    = {AMT '12},
  abstract  = {Model Driven Development (MDD) is a software engineering approach in which models constitute the basic units of software development. A key part of MDD is the notion of automated model transformation, in which models are stepwise refined into more detailed models, and eventually into code. The correctness of transformations is essential to the success of MDD, and while much research has concentrated on formal verification, testing remains the most efficient method of validation. Transformation testing is however different from testing code, and presents new challenges. In this paper, we survey the model transformation testing phases and the approaches proposed in the literature for each phase.},
  doi       = {10.1145/2432497.2432502},
  isbn      = {9781450318037},
  keywords  = {model driven development, test case generation, model transformation testing, contracts, mutation analysis},
  location  = {Innsbruck, Austria},
  numpages  = {6},
  url       = {https://doi.org/10.1145/2432497.2432502},
}

@InProceedings{Gopinath2012,
  author    = {Gopinath, Divya and Zaeem, Razieh Nokhbeh and Khurshid, Sarfraz},
  booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Improving the Effectiveness of Spectra-Based Fault Localization Using Specifications},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {40–49},
  publisher = {Association for Computing Machinery},
  series    = {ASE 2012},
  abstract  = {Fault localization i.e., locating faulty lines of code, is a key step in removing bugs and often requires substantial manual effort. Recent years have seen many automated localization techniques, specifically using the program’s passing and failing test runs, i.e., test spectra. However, the effectiveness of these approaches is sensitive to factors such as the type and number of faults, and the quality of the test-suite. This paper presents a novel technique that applies spectra-based localization in synergy with specification-based analysis to more accurately locate faults. Our insight is that unsatisfiability analysis of violated specifications, enabled by SAT technology, could be used to (1) compute unsatisfiable cores that contain likely faulty statements and (2) generate tests that help spectra-based localization. Our technique is iterative and driven by a feedback loop that enables more precise fault localization. SAT-TAR is a framework that embodies our technique for Java programs, including those with multiple faults. An experimental evaluation using a suite of widely-studied data structure programs, including the ANTLR and JTopas parser applications, shows that our technique localizes faults more accurately than state-of-the-art approaches.},
  doi       = {10.1145/2351676.2351683},
  isbn      = {9781450312042},
  keywords  = {Tarantula, Automated Debugging, Alloy, Minimal UNSAT cores, Kodkod, Fault Localization},
  location  = {Essen, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2351676.2351683},
}

@InProceedings{Zhang2012,
  author    = {Zhang, Lingming and Marinov, Darko and Zhang, Lu and Khurshid, Sarfraz},
  booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
  title     = {Regression Mutation Testing},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {331–341},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2012},
  abstract  = {Mutation testing is one of the most powerful approaches for evaluating quality of test suites. However, mutation testing is also one of the most expensive testing approaches. This paper presents Regression Mutation Testing (ReMT), a new technique to speed up mutation testing for evolving systems. The key novelty of ReMT is to incrementally calculate mutation testing results for the new program version based on the results from the old program version; ReMT uses a static analysis to check which results can be safely reused. ReMT also employs a mutation-specific test prioritization to further speed up mutation testing. We present an empirical study on six evolving systems, whose sizes range from 3.9KLoC to 88.8KLoC. The empirical results show that ReMT can substantially reduce mutation testing costs, indicating a promising future for applying mutation testing on evolving software systems.},
  doi       = {10.1145/2338965.2336793},
  isbn      = {9781450314541},
  location  = {Minneapolis, MN, USA},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2338965.2336793},
}

@InProceedings{Li2012,
  author    = {Li, Kaituo and Reichenbach, Christoph and Csallner, Christoph and Smaragdakis, Yannis},
  booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
  title     = {Residual Investigation: Predictive and Precise Bug Detection},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {298–308},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2012},
  abstract  = {We introduce the concept of “residual investigation” for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when taken in conjunction with the static error report, increase confidence in the existence of an error, as well as its severity, without needing to directly observe a fault resulting from the error.  We enhance the static analyzer FindBugs with several residual investigations, appropriately tuned to the static error patterns in FindBugs, and apply it to 7 large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (“false positives”) while reporting several actual errors that would not have been detected by mere execution of a program's test suite.},
  doi       = {10.1145/2338965.2336789},
  isbn      = {9781450314541},
  location  = {Minneapolis, MN, USA},
  numpages  = {11},
  url       = {https://doi.org/10.1145/2338965.2336789},
}

@Article{Shahriar2012,
  author     = {Shahriar, Hossain and Zulkernine, Mohammad},
  journal    = {ACM Comput. Surv.},
  title      = {Mitigating Program Security Vulnerabilities: Approaches and Challenges},
  year       = {2012},
  issn       = {0360-0300},
  month      = jun,
  number     = {3},
  volume     = {44},
  abstract   = {Programs are implemented in a variety of languages and contain serious vulnerabilities which might be exploited to cause security breaches. These vulnerabilities have been exploited in real life and caused damages to related stakeholders such as program users. As many security vulnerabilities belong to program code, many techniques have been applied to mitigate these vulnerabilities before program deployment. Unfortunately, there is no comprehensive comparative analysis of different vulnerability mitigation works. As a result, there exists an obscure mapping between the techniques, the addressed vulnerabilities, and the limitations of different approaches. This article attempts to address these issues. The work extensively compares and contrasts the existing program security vulnerability mitigation techniques, namely testing, static analysis, and hybrid analysis. We also discuss three other approaches employed to mitigate the most common program security vulnerabilities: secure programming, program transformation, and patching. The survey provides a comprehensive understanding of the current program security vulnerability mitigation approaches and challenges as well as their key characteristics and limitations. Moreover, our discussion highlights the open issues and future research directions in the area of program security vulnerability mitigation.},
  address    = {New York, NY, USA},
  articleno  = {11},
  doi        = {10.1145/2187671.2187673},
  issue_date = {June 2012},
  keywords   = {secure programming, program transformation, vulnerability testing, static analysis, Program security vulnerability mitigation, patching, hybrid analysis},
  numpages   = {46},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/2187671.2187673},
}

@InProceedings{Robinson2011,
  author    = {Robinson, Brian and Ernst, Michael D. and Perkins, Jeff H. and Augustine, Vinay and Li, Nuo},
  booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Scaling up Automated Test Generation: Automatically Generating Maintainable Regression Unit Tests for Programs},
  year      = {2011},
  address   = {USA},
  pages     = {23–32},
  publisher = {IEEE Computer Society},
  series    = {ASE '11},
  abstract  = {This paper presents an automatic technique for generating maintainable regression unit tests for programs. We found previous test generation techniques inadequate for two main reasons. First. they were designed for and evaluated upon libraries rather than applications. Second, they were designed to find bugs rather than to create maintainable regression test suites: the test suites that they generated were brittle and hard to understand. This paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system. In experiments using an industrial system, the generated tests achieved good coverage and mutation kill score, were readable by the product's developers, and required few edits as the system under test evolved. While our evaluation is in the context of one test generator, we are aware of many research systems that suffer similar limitations, so our approach and observations are more generally relevant.},
  doi       = {10.1109/ASE.2011.6100059},
  isbn      = {9781457716386},
  numpages  = {10},
  url       = {https://doi.org/10.1109/ASE.2011.6100059},
}

@InProceedings{Aaltonen2010,
  author    = {Aaltonen, Kalle and Ihantola, Petri and Sepp\"{a}l\"{a}, Otto},
  booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
  title     = {Mutation Analysis vs. Code Coverage in Automated Assessment of Students' Testing Skills},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {153–160},
  publisher = {Association for Computing Machinery},
  series    = {OOPSLA '10},
  abstract  = {Learning to program should include learning about proper software testing. Some automatic assessment systems, e.g. Web-CAT, allow assessing student-generated test suites using coverage metrics. While this encourages testing, we have observed that sometimes students can get rewarded from high coverage although their tests are of poor quality. Exploring alternative methods of assessment, we have tested mutation analysis to evaluate students' solutions. Initial results from applying mutation analysis to real course submissions indicate that mutation analysis could be used to fix some problems of code coverage in the assessment. Combining both metrics is likely to give more accurate feedback.},
  doi       = {10.1145/1869542.1869567},
  isbn      = {9781450302401},
  keywords  = {programming assignments, mutation testing, test coverage, mutation analysis, automated assessment},
  location  = {Reno/Tahoe, Nevada, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1869542.1869567},
}

@InProceedings{Dobolyi2010,
  author    = {Dobolyi, Kinga and Weimer, Westley},
  booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
  title     = {Modeling Consumer-Perceived Web Application Fault Severities for Testing},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {97–106},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA '10},
  abstract  = {Despite the growing usage of web applications, extreme resource constraints during their development frequently leave them inadequately tested. Because testing may be perceived as having a low return on investment for web applications, we believe that providing a consumer-perceived fault severity model could allow developers to prioritize faults according to their likelihood of impacting consumer retention, encouraging web application developers to test more effectively. In a study involving 386 humans and 800 web application faults, we observe that an arbitrary human judgment of fault severity is unreliable. We thus present two models of fault severity that outperform individual humans in terms of correctly predicting the average consumer-perceived severity of web application faults. Our first model uses human annotations of fault surface features, and is 87% accurate at identifying low-priority, non-severe faults. We also present a fully automated conservative model that correctly identifies 55% of non-severe faults without missing any severe faults. Both models outperform humans at flagging severe faults, and can substitute or reinforce humans by prioritizing faults encountered in web application development and testing.},
  doi       = {10.1145/1831708.1831720},
  isbn      = {9781605588230},
  keywords  = {fault, web application, severity},
  location  = {Trento, Italy},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1831708.1831720},
}

@InProceedings{Souza2016,
  author    = {Souza, Francisco Carlos M. and Papadakis, Mike and Le Traon, Yves and Delamaro, Márcio E.},
  booktitle = {2016 IEEE/ACM 9th International Workshop on Search-Based Software Testing (SBST)},
  title     = {Strong Mutation-Based Test Data Generation Using Hill Climbing},
  year      = {2016},
  month     = {May},
  pages     = {45-54},
  abstract  = {Mutation Testing is an effective test criterion for finding faults and assessing the quality of a test suite. Every test criterion requires the generation of test cases, which turns to be a manual and difficult task. In literature, search-based techniques are effective in generating structural-based test data. This fact motivates their use for mutation testing. Thus, if automatic test data generation can achieve an acceptable level of mutation score, it has the potential to greatly reduce the involved manual effort. This paper proposes an automated test generation approach, using hill climbing, for strong mutation. It incremental aims at strongly killing mutants, by focusing on mutants' propagation, i.e., how to kill mutants that are weakly killed but not strongly. Furthermore, the paper reports empirical results regarding the cost and effectiveness of the proposed approach on a set of 18 C programs. Overall, for the majority of the studied programs, the proposed approach achieved a higher strong mutation score than random testing, by 19,02% on average, and the previously proposed test generation techniques that ignore mutants' propagation, by 7,2% on average. Our results also demonstrate the improved efficiency of the proposed scheme over the previous methods.},
  doi       = {10.1145/2897010.2897012},
  keywords  = {Measurement;Manuals;Software testing;Computers;Security;Reliability;Mutation Testing;Search-Based Software Testing;Test Data Generation},
}

@InProceedings{Lindstroem2016,
  author    = {Lindström, Birgitta and Ḿrki, Andŕs},
  booktitle = {2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
  title     = {On strong mutation and subsuming mutants},
  year      = {2016},
  month     = {April},
  pages     = {112-121},
  abstract  = {Mutation analysis is a powerful technique for software testing but it is also known to be computationally expensive. The main reason for the high computational cost is that many of the mutants are redundant and thus, do not contribute to the quality of the test suite. One of the most promising approaches to avoid producing redundant mutants is to identify subsumption relations among mutants, preferably before these are generated. Such relations have for example, been identified at an operator level for mutants created by the ROR operator. This reduced set of non-redundant mutants has been used in several recent studies and is also the default option in at least one mutation testing tool that supports strong mutation. This raises questions on whether the identified subsumption relations between the mutants hold in a context of strong mutation or variants of weak mutation that require some limited error propagation (firm mutation). We have conducted an experimental study to investigate the subsumption relations in the context of strong or firm mutation. We observed that it is possible to create a test suite that is 100% adequate for the reduced set of mutants while not being 100% adequate for the complete set. This shows that the subsumption relations do not hold for strong or firm mutation. We provide several examples on this behavior and discuss the root causes. Our findings are important since strong and firm mutation both are frequently used to evaluate test suites and testing criteria. The choice of whether to use a reduced set of mutants or an entire set should however, not be made without consideration of the context in which they are used (i.e., strong, firm or weak mutation) since the subsumption relations between ROR mutants do not hold for strong or firm mutation. Just as redundant mutants can give an overestimation of the mutation score for a test suite, using the reduced set of mutants can give an underestimation if used together with strong or firm mutation. Results reported from such studies should therefore, be accompanied by information on whether the reduced or complete set of mutants was used and if the researchers used strong, firm or weak mutation.},
  doi       = {10.1109/ICSTW.2016.28},
  keywords  = {Context;Software;Conferences;Software testing;Informatics;Computational efficiency;mutation testing;redundant mutant;strong mutation;subsumption},
}

@InProceedings{Kracht2014,
  author    = {Kracht, Jeshua S. and Petrovic, Jacob Z. and Walcott-Justice, Kristen R.},
  booktitle = {2014 14th International Conference on Quality Software},
  title     = {Empirically Evaluating the Quality of Automatically Generated and Manually Written Test Suites},
  year      = {2014},
  month     = {Oct},
  pages     = {256-265},
  abstract  = {The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-the-art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing.},
  doi       = {10.1109/QSIC.2014.33},
  issn      = {2332-662X},
  keywords  = {Manuals;Complexity theory;Software;Testing;Writing;Standards;Java},
}

@InProceedings{Sherman2009,
  author    = {Sherman, Elena and Dwyer, Matthew B. and Elbaum, Sebastian},
  booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
  title     = {Saturation-Based Testing of Concurrent Programs},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {53–62},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE '09},
  abstract  = {Coverage measures help to determine whether a test suite exercises a program adequately according to a testing criterion. Many existing measures, however, are defined over coverage domains that cannot be precisely calculated, rendering them of limited value in assessing the extent of testing activities. To exploit the use of such measures, we formalize saturation-based test adequacy, a form of adequacy focused on the rate at which coverage increases during test suite execution. We define a family of coverage metrics for concurrent program testing that are well-suited to saturation-based adequacy and present a study that explores their cost and effectiveness. The results of this study suggest that saturation-based testing can serve as an effective complement to traditional notions of coverage-based testing.},
  doi       = {10.1145/1595696.1595706},
  isbn      = {9781605580012},
  keywords  = {test adequacy criteria, concurrent programs, coverage},
  location  = {Amsterdam, The Netherlands},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1595696.1595706},
}

@InProceedings{Halfond2009,
  author    = {Halfond, William G.J. and Anand, Saswat and Orso, Alessandro},
  booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
  title     = {Precise Interface Identification to Improve Testing and Analysis of Web Applications},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {285–296},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA '09},
  abstract  = {As web applications become more widespread, sophisticated, and complex, automated quality assurance techniques for such applications have grown in importance. Accurate interface identification is fundamental for many of these techniques, as the components of a web application communicate extensively via implicitly-defined interfaces to generate customized and dynamic content. However, current techniques for identifying web application interfaces can be incomplete or imprecise, which hinders the effectiveness of quality assurance techniques. To address these limitations, we present a new approach for identifying web application interfaces that is based on a specialized form of symbolic execution. In our empirical evaluation, we show that the set of interfaces identified by our approach is more accurate than those identified by other approaches. We also show that this increased accuracy leads to improvements in several important quality assurance techniques for web applications: test-input generation, penetration testing, and invocation verification.},
  doi       = {10.1145/1572272.1572305},
  isbn      = {9781605583389},
  keywords  = {interface identification, web application testing},
  location  = {Chicago, IL, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/1572272.1572305},
}

@InProceedings{Polikarpova2009,
  author    = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
  booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
  title     = {A Comparative Study of Programmer-Written and Automatically Inferred Contracts},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {93–104},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA '09},
  abstract  = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination?Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect.Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts.We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
  doi       = {10.1145/1572272.1572284},
  isbn      = {9781605583389},
  keywords  = {eiffel, dynamic contract inference},
  location  = {Chicago, IL, USA},
  numpages  = {12},
  url       = {https://doi.org/10.1145/1572272.1572284},
}

@InProceedings{Rajan2008,
  author    = {Rajan, Ajitha and Whalen, Michael W. and Heimdahl, Mats P.E.},
  booktitle = {Proceedings of the 30th International Conference on Software Engineering},
  title     = {The Effect of Program and Model Structure on Mc/Dc Test Adequacy Coverage},
  year      = {2008},
  address   = {New York, NY, USA},
  pages     = {161–170},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '08},
  abstract  = {In avionics and other critical systems domains, adequacy of test suites is currently measured using the MC/DC metric on source code (or on a model in model-based development). We believe that the rigor of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be misleading as a test adequacy criterion. We investigate this hypothesis by empirically studying the effect of program structure on MC/DC coverage.To perform this investigation, we use six realistic systems from the civil avionics domain and two toy examples. For each of these systems, we use two versions of their implementation-with and without expression folding (i.e., inlining). To assess the sensitivity of MC/DC to program structure, we first generate test suites that satisfy MC/DC over a non-inlined implementation. We then run the generated test suites over the inlined implementation and measure MC/DC achieved. For our realistic examples, the test suites yield an average reduction of 29.5% in MC/DC achieved over the inlined implementations at 5% statistical significance level.},
  doi       = {10.1145/1368088.1368111},
  isbn      = {9781605580791},
  keywords  = {structural coverage metrics},
  location  = {Leipzig, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1368088.1368111},
}

@InProceedings{Fraser2007,
  author    = {Fraser, Gordon and Wotawa, Franz},
  booktitle = {Proceedings of the 3rd International Workshop on Advances in Model-Based Testing},
  title     = {Using LTL Rewriting to Improve the Performance of Model-Checker Based Test-Case Generation},
  year      = {2007},
  address   = {New York, NY, USA},
  pages     = {64–74},
  publisher = {Association for Computing Machinery},
  series    = {A-MOST '07},
  abstract  = {Model-checkers have recently been suggested for automated software test-case generation. Several works have presented methods that create efficient test-suites using model-checkers. Ease of use and complete automation are major advantages of such approaches. However, the use of a model-checker comes at the price of potential performance problems. If the model used for test-case generation is complex, then model-checker based approaches can be very slow, or even not applicable at all. In this paper, we identify that unnecessary, redundant calls to the model-checker are one of the causes of bad performance. To overcome this problem, we suggest the use of temporal logic rewriting techniques, which originate from runtime verification research. This achieves a significant increase in the performance, and improves the applicability of model-checker based test-case generation approaches in general. At the same time, the suggested techniques achieve a reduction of the resulting test-suite sizes without degradation of the fault sensitivity. This helps to reduce the costs of the test-case execution.},
  doi       = {10.1145/1291535.1291542},
  isbn      = {9781595938503},
  keywords  = {test-case generation with model-checkers, LTL rewriting, automated software testing},
  location  = {London, United Kingdom},
  numpages  = {11},
  url       = {https://doi.org/10.1145/1291535.1291542},
}

@InProceedings{Xie2006,
  author    = {Xie, Tao and Zhao, Jianjun},
  booktitle = {Proceedings of the 5th International Conference on Aspect-Oriented Software Development},
  title     = {A Framework and Tool Supports for Generating Test Inputs of AspectJ Programs},
  year      = {2006},
  address   = {New York, NY, USA},
  pages     = {190–201},
  publisher = {Association for Computing Machinery},
  series    = {AOSD '06},
  abstract  = {Aspect-oriented software development is gaining popularity with the wider adoption of languages such as AspectJ. To reduce the manual effort of testing aspects in AspectJ programs, we have developed a framework, called Aspectra, that automates generation of test inputs for testing aspectual behavior, i.e., the behavior implemented in pieces of advice or intertype methods defined in aspects. To test aspects, developers construct base classes into which the aspects are woven to form woven classes. Our approach leverages existing test-generation tools to generate test inputs for the woven classes; these test inputs indirectly exercise the aspects. To enable aspects to be exercised during test generation, Aspectra automatically synthesizes appropriate wrapper classes for woven classes. To assess the quality of the generated tests, Aspectra defines and measures aspectual branch coverage (branch coverage within aspects). To provide guidance for developers to improve test coverage, Aspectra also defines interaction coverage. We have developed tools for automating Aspectra's wrapper synthesis and coverage measurement, and applied them on testing 12 subjects taken from a variety of sources. Our experience has shown that Aspectra effectively provides tool supports in enabling existing test-generation tools to generate test inputs for improving aspectual branch coverage.},
  doi       = {10.1145/1119655.1119681},
  isbn      = {159593300X},
  keywords  = {coverage criteria, coverage measurement, software testing, AspectJ, aspect-oriented programs, test generation, aspect-oriented software development},
  location  = {Bonn, Germany},
  numpages  = {12},
  url       = {https://doi.org/10.1145/1119655.1119681},
}

@InProceedings{Bradbury2005,
  author    = {Bradbury, Jeremy S. and Cordy, James R. and Dingel, Juergen},
  booktitle = {Proceedings of the 6th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
  title     = {An Empirical Framework for Comparing Effectiveness of Testing and Property-Based Formal Analysis},
  year      = {2005},
  address   = {New York, NY, USA},
  pages     = {2–5},
  publisher = {Association for Computing Machinery},
  series    = {PASTE '05},
  abstract  = {Today, many formal analysis tools are not only used to provide certainty but are also used to debug software systems - a role that has traditional been reserved for testing tools. We are interested in exploring the complementary relationship as well as tradeoffs between testing and formal analysis with respect to debugging and more specifically bug detection. In this paper we present an approach to the assessment of testing and formal analysis tools using metrics to measure the quantity and efficiency of each technique at finding bugs. We also present an assessment framework that has been constructed to allow for symmetrical comparison and evaluation of tests versus properties. We are currently beginning to conduct experiments and this paper presents a discussion of possible outcomes of our proposed empirical study.},
  doi       = {10.1145/1108792.1108795},
  isbn      = {1595932399},
  keywords  = {mutation testing, model checking, empirical software engineering, bug detection, static analysis},
  location  = {Lisbon, Portugal},
  numpages  = {4},
  url       = {https://doi.org/10.1145/1108792.1108795},
}

@Article{Bradbury2005a,
  author     = {Bradbury, Jeremy S. and Cordy, James R. and Dingel, Juergen},
  journal    = {SIGSOFT Softw. Eng. Notes},
  title      = {An Empirical Framework for Comparing Effectiveness of Testing and Property-Based Formal Analysis},
  year       = {2005},
  issn       = {0163-5948},
  month      = sep,
  number     = {1},
  pages      = {2–5},
  volume     = {31},
  abstract   = {Today, many formal analysis tools are not only used to provide certainty but are also used to debug software systems - a role that has traditional been reserved for testing tools. We are interested in exploring the complementary relationship as well as tradeoffs between testing and formal analysis with respect to debugging and more specifically bug detection. In this paper we present an approach to the assessment of testing and formal analysis tools using metrics to measure the quantity and efficiency of each technique at finding bugs. We also present an assessment framework that has been constructed to allow for symmetrical comparison and evaluation of tests versus properties. We are currently beginning to conduct experiments and this paper presents a discussion of possible outcomes of our proposed empirical study.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1108768.1108795},
  issue_date = {January 2006},
  keywords   = {static analysis, empirical software engineering, bug detection, mutation testing, model checking},
  numpages   = {4},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/1108768.1108795},
}

@InProceedings{Harder2003,
  author    = {Harder, Michael and Mellen, Jeff and Ernst, Michael D.},
  booktitle = {Proceedings of the 25th International Conference on Software Engineering},
  title     = {Improving Test Suites via Operational Abstraction},
  year      = {2003},
  address   = {USA},
  pages     = {60–71},
  publisher = {IEEE Computer Society},
  series    = {ICSE '03},
  abstract  = {This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text.The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications) from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults.This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.},
  doi       = {10.1109/icse.2003.1201188},
  isbn      = {076951877X},
  location  = {Portland, Oregon},
  numpages  = {12},
}

@Article{Fraser2009,
  author          = {Fraser, G. and Wotawa, F. and Ammann, P.},
  journal         = {Journal of Systems and Software},
  title           = {Issues in using model checkers for test case generation},
  year            = {2009},
  note            = {cited By 26},
  number          = {9},
  pages           = {1403-1418},
  volume          = {82},
  abstract        = {The use of model checkers for automated software testing has received some attention in the literature: It is convenient because it allows fully automated generation of test suites for many different test objectives. On the other hand, model checkers were not originally meant to be used this way but for formal verification, so using model checkers for testing is sometimes perceived as a "hack". Indeed, several drawbacks result from the use of model checkers for test case generation. If model checkers were designed or adapted to take into account the needs that result from the application to software testing, this could lead to significant improvements with regard to test suite quality and performance. In this paper we identify the drawbacks of current model checkers when used for testing. We illustrate techniques to overcome these problems, and show how they could be integrated into the model checking process. In essence, the described techniques can be seen as a general road map to turn model checkers into general purpose testing tools. © 2009 Elsevier Inc. All rights reserved.},
  author_keywords = {Automated software testing; Automated test case generation; Minimization; Performance; Testing with model checkers},
  doi             = {10.1016/j.jss.2009.05.016},
  keywords        = {Automated generation; Automated software testing; Automated test case generation; Current models; Formal verifications; General purpose; Minimization; Model checker; Performance; Road-maps; Test case generation; Testing tools; Turn model, Automation; Computer software selection and evaluation; Software testing; Testing, Model checking},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949100459&doi=10.1016%2fj.jss.2009.05.016&partnerID=40&md5=10cb02a3f89ebe55aa1497253d1fb57b},
}

@Article{Gopinath2017a,
  author          = {Gopinath, R. and Ahmed, I. and Alipour, M.A. and Jensen, C. and Groce, A.},
  journal         = {IEEE Transactions on Reliability},
  title           = {Mutation reduction strategies considered harmful},
  year            = {2017},
  note            = {cited By 19},
  number          = {3},
  pages           = {854-874},
  volume          = {66},
  abstract        = {Mutation analysis is a well known yet unfortunately costly method for measuring test suite quality. Researchers have proposed numerous mutation reduction strategies in order to reduce the high cost of mutation analysis, while preserving the representativeness of the original set of mutants. As mutation reduction is an area of active research, it is important to understand the limits of possible improvements. We theoretically and empirically investigate the limits of improvement in effectiveness from using mutation reduction strategies compared to random sampling. Using real-world open source programs as subjects, we find an absolute limit in improvement of effectiveness over random sampling - 13.078%. Given our findings with respect to absolute limits, one may ask: How effective are the extant mutation reduction strategies? We evaluate the effectiveness of multiple mutation reduction strategies in comparison to random sampling. We find that none of the mutation reduction strategies evaluated - many forms of operator selection, and stratified sampling (on operators or program elements) - produced an effectiveness advantage larger than 5% in comparison with random sampling. Given the poor performance of mutation selection strategies - they may have a negligible advantage at best, and often perform worse than random sampling - we caution practicing testers against applying mutation reduction strategies without adequate justification. © 1963-2012 IEEE.},
  art_number      = {7942146},
  author_keywords = {Mutation analysis; software testing},
  doi             = {10.1109/TR.2017.2705662},
  keywords        = {Software testing, Multiple mutations; Mutation analysis; Open source projects; Operator selections; Poor performance; Program elements; Reduction strategy; Stratified sampling, Quality control},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020385899&doi=10.1109%2fTR.2017.2705662&partnerID=40&md5=111ae5063220437a27e8ca4e1839db84},
}

@Comment{jabref-meta: databaseType:bibtex;}
