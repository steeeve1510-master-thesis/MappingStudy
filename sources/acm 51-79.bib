@inproceedings{10.1145/2568225.2568271,
author = {Inozemtseva, Laura and Holmes, Reid},
title = {Coverage is Not Strongly Correlated with Test Suite Effectiveness},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568271},
doi = {10.1145/2568225.2568271},
abstract = { The coverage of a test suite is often used as a proxy for its ability to detect faults. However, previous studies that investigated the correlation between code coverage and test suite effectiveness have failed to reach a consensus about the nature and strength of the relationship between these test suite characteristics. Moreover, many of the studies were done with small or synthetic programs, making it unclear whether their results generalize to larger programs, and some of the studies did not account for the confounding influence of test suite size. In addition, most of the studies were done with adequate suites, which are are rare in practice, so the results may not generalize to typical test suites.  We have extended these studies by evaluating the relationship between test suite size, coverage, and effectiveness for large Java programs. Our study is the largest to date in the literature: we generated 31,000 test suites for five systems consisting of up to 724,000 lines of source code. We measured the statement coverage, decision coverage, and modified condition coverage of these suites and used mutation testing to evaluate their fault detection effectiveness.  We found that there is a low to moderate correlation between coverage and effectiveness when the number of test cases in the suite is controlled for. In addition, we found that stronger forms of coverage do not provide greater insight into the effectiveness of the suite. Our results suggest that coverage, while useful for identifying under-tested parts of a program, should not be used as a quality target because it is not a good indicator of test suite effectiveness. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {435–445},
numpages = {11},
keywords = {test suite effectiveness, Coverage, test suite quality},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568278,
author = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
title = {Code Coverage for Suite Evaluation by Developers},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568278},
doi = {10.1145/2568225.2568278},
abstract = { One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {72–82},
numpages = {11},
keywords = {evaluation of coverage criteria, statistical analysis, test frameworks},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2591062.2591164,
author = {Edwards, Stephen H. and Shams, Zalia},
title = {Comparing Test Quality Measures for Assessing Student-Written Tests},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591164},
doi = {10.1145/2591062.2591164},
abstract = { Many educators now include software testing activities in programming assignments, so there is a growing demand for appropriate methods of assessing the quality of student-written software tests. While tests can be hand-graded, some educators also use objective performance metrics to assess software tests. The most common measures used at present are code coverage measures—tracking how much of the student’s code (in terms of statements, branches, or some combination) is exercised by the corresponding software tests. Code coverage has limitations, however, and sometimes it overestimates the true quality of the tests. Some researchers have suggested that mutation analysis may provide a better indication of test quality, while some educators have experimented with simply running every student’s test suite against every other student’s program—an “all-pairs” strategy that gives a bit more insight into the quality of the tests. However, it is still unknown which one of these measures is more accurate, in terms of most closely predicting the true bug revealing capability of a given test suite. This paper directly compares all three methods of measuring test quality in terms of how well they predict the observed bug revealing capabilities of student-written tests when run against a naturally occurring collection of student-produced defects. Experimental results show that all-pairs testing—running each student’s tests against every other student’s solution—is the most effective predictor of the underlying bug revealing capability of a test suite. Further, no strong correlation was found between bug revealing capability and either code coverage or mutation analysis scores. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {354–363},
numpages = {10},
keywords = {test coverage, automated assessment, test metrics, automated grading, test quality, mutation testing, programming assignments, Software testing},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2568225.2568265,
author = {Yao, Xiangjuan and Harman, Mark and Jia, Yue},
title = {A Study of Equivalent and Stubborn Mutation Operators Using Human Analysis of Equivalence},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568265},
doi = {10.1145/2568225.2568265},
abstract = { Though mutation testing has been widely studied for more than thirty years, the prevalence and properties of equivalent mutants remain largely unknown. We report on the causes and prevalence of equivalent mutants and their relationship to stubborn mutants (those that remain undetected by a high quality test suite, yet are non-equivalent). Our results, based on manual analysis of 1,230 mutants from 18 programs, reveal a highly uneven distribution of equivalence and stubbornness. For example, the ABS class and half UOI class generate many equivalent and almost no stubborn mutants, while the LCR class generates many stubborn and few equivalent mutants. We conclude that previous test effectiveness studies based on fault seeding could be skewed, while developers of mutation testing tools should prioritise those operators that we found generate disproportionately many stubborn (and few equivalent) mutants. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {919–930},
numpages = {12},
keywords = {Mutation Testing, Stubborn Mutant, Equivalent Mutant},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1109/ASE.2013.6693070,
author = {Zhang, Lingming and Gligoric, Milos and Marinov, Darko and Khurshid, Sarfraz},
title = {Operator-Based and Random Mutant Selection: Better Together},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693070},
doi = {10.1109/ASE.2013.6693070},
abstract = {Mutation testing is a powerful methodology for evaluating the quality of a test suite. However, the methodology is also very costly, as the test suite may have to be executed for each mutant. Selective mutation testing is a well-studied technique to reduce this cost by selecting a subset of all mutants, which would otherwise have to be considered in their entirety. Two common approaches are operator-based mutant selection, which only generates mutants using a subset of mutation operators, and random mutant selection, which selects a subset of mutants generated using all mutation operators. While each of the two approaches provides some reduction in the number of mutants to execute, applying either of the two to medium-sized, realworld programs can still generate a huge number of mutants, which makes their execution too expensive. This paper presents eight random sampling strategies defined on top of operatorbased mutant selection, and empirically validates that operatorbased selection and random selection can be applied in tandem to further reduce the cost of mutation testing. The experimental results show that even sampling only 5% of mutants generated by operator-based selection can still provide precise mutation testing results, while reducing the average mutation testing time to 6.54% (i.e., on average less than 5 minutes for this study).},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {92–102},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE'13}
}

@inproceedings{10.1145/2509136.2509551,
author = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
title = {Injecting Mechanical Faults to Localize Developer Faults for Evolving Software},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509551},
doi = {10.1145/2509136.2509551},
abstract = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art FaultTracer technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {765–784},
numpages = {20},
keywords = {fault localization, software evolution, mutation testing, regression testing},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@article{10.1145/2544173.2509551,
author = {Zhang, Lingming and Zhang, Lu and Khurshid, Sarfraz},
title = {Injecting Mechanical Faults to Localize Developer Faults for Evolving Software},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2544173.2509551},
doi = {10.1145/2544173.2509551},
abstract = {This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art FaultTracer technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.},
journal = {SIGPLAN Not.},
month = oct,
pages = {765–784},
numpages = {20},
keywords = {software evolution, fault localization, mutation testing, regression testing}
}

@inproceedings{10.1145/2491411.2494586,
author = {Inozemtseva, Laura and Hemmati, Hadi and Holmes, Reid},
title = {Using Fault History to Improve Mutation Reduction},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2494586},
doi = {10.1145/2491411.2494586},
abstract = { Mutation testing can be used to measure test suite quality in two ways: by treating the kill score as a quality metric, or by treating each surviving, non-equivalent mutant as an indicator of an inadequacy in the test suite. The first technique relies on the assumption that the mutation score is highly correlated with the suite's real fault detection rate, which is not well supported by the literature. The second technique relies only on the weaker assumption that the "interesting" mutants (i.e., the ones that indicate an inadequacy in the suite) are in the set of surviving mutants. Using the second technique also makes improving the suite straightforward.  Unfortunately, mutation testing has a performance problem. At least part of the test suite must be run on every mutant, meaning mutation testing can be too slow for practical use. Previous work has addressed this by reducing the number of mutants to evaluate in various ways, including selecting a random subset of them. However, reducing the set of mutants by random reduction is suboptimal for developers using the second technique described above, since random reduction will eliminate many of the interesting mutants.  We propose a new reduction method that supports the use of the second technique by reducing the set of mutants to those generated by altering files that have contained many faults in the past. We performed a pilot study that suggests that this reduction method preferentially chooses mutants that will survive mutation testing; that is, it preserves a greater number of interesting mutants than random reduction does. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {639–642},
numpages = {4},
keywords = {Mutation testing, test suite quality, fault history, mutant reduction},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2491411.2491438,
author = {Marinescu, Paul Dan and Cadar, Cristian},
title = {KATCH: High-Coverage Testing of Software Patches},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491438},
doi = {10.1145/2491411.2491438},
abstract = { One of the distinguishing characteristics of software systems is that they evolve: new patches are committed to software repositories and new versions are released to users on a continuous basis. Unfortunately, many of these changes bring unexpected bugs that break the stability of the system or affect its security. In this paper, we address this problem using a technique for automatically testing code patches. Our technique combines symbolic execution with several novel heuristics based on static and dynamic program analysis which allow it to quickly reach the code of the patch. We have implemented our approach in a tool called KATCH, which we have applied to all the patches written in a combined period of approximately six years for nineteen mature programs from the popular GNU diffutils, GNU binutils and GNU findutils utility suites, which are shipped with virtually all UNIX-based distributions. Our results show that KATCH can automatically synthesise inputs that significantly increase the patch coverage achieved by the existing manual test suites, and find bugs at the moment they are introduced. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {235–245},
numpages = {11},
keywords = {Symbolic Execution, Patch Testing},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2483760.2483769,
author = {Gligoric, Milos and Groce, Alex and Zhang, Chaoqiang and Sharma, Rohan and Alipour, Mohammad Amin and Marinov, Darko},
title = {Comparing Non-Adequate Test Suites Using Coverage Criteria},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483769},
doi = {10.1145/2483760.2483769},
abstract = { A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the (feasible) requirements is C-adequate.  Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given criteria C and C′, are C-adequate suites (on average) more effective than C′-adequate suites? However, in many realistic cases producing adequate suites is impractical or even impossible. We present the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given criteria C and C′, which one is better to use to compare test suites? Namely, if suites T1, T2 . . . Tn have coverage values c1, c2 . . . cn for C and c′1, c′2 . . . c′n for C′, is it better to compare suites based on c1, c2 . . . cn or based on c′1, c′ 2 . . . c′n?  We evaluate a large set of plausible criteria, including statement and branch coverage, as well as stronger criteria used in recent studies. Two criteria perform best: branch coverage and an intra-procedural acyclic path coverage. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {302–313},
numpages = {12},
keywords = {non-adequate test suites, Coverage criteria},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.1145/2483760.2483782,
author = {Zhang, Lingming and Marinov, Darko and Khurshid, Sarfraz},
title = {Faster Mutation Testing Inspired by Test Prioritization and Reduction},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483782},
doi = {10.1145/2483760.2483782},
abstract = { Mutation testing is a well-known but costly approach for determining test adequacy. The central idea behind the approach is to generate mutants, which are small syntactic transformations of the program under test, and then to measure for a given test suite how many mutants it kills. A test t is said to kill a mutant m of program p if the output of t on m is different from the output of t on p. The effectiveness of mutation testing in determining the quality of a test suite relies on the ability to apply it using a large number of mutants. However, running many tests against many mutants is time consuming. We present a family of techniques to reduce the cost of mutation testing by prioritizing and reducing tests to more quickly determine the sets of killed and non-killed mutants. Experimental results show the effectiveness and efficiency of our techniques. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {235–245},
numpages = {11},
keywords = {Test Reduction, Mutation testing, Test Prioritization},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.5555/2662413.2662421,
author = {Jehan, Seema and Pill, Ingo and Wotawa, Franz},
title = {Functional SOA Testing Based on Constraints},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {In the fierce competition on today's software market, Service-Oriented Architectures (SOAs) are an established design paradigm. Essential concepts like modularization, reuse, and the corresponding IP core business are inherently supported in the development and operation of SOAs that offer flexibility in many aspects and thus optimal conditions also for heterogeneous system developments. The intrinsics of large and complex SOA enterprises, however, require us to adopt and evolve our verification technology, in order to achieve expected software quality levels. In this paper, we contribute to this challenge by proposing a constraint based testing approach for SOAs. In our work, we augment a SOA's BPEL business model with pre- and postcondition contracts defining essential component traits, and derive a suite of feasible test cases to be executed after assessing its quality via corresponding coverage criteria. We illustrate our approach's viability via a running example as well as experimental results, and discuss current and envisioned automation levels in the context of a test and diagnosis workflow.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {33–39},
numpages = {7},
location = {San Francisco, California},
series = {AST '13}
}

@inproceedings{10.5555/2486788.2486981,
author = {Santelices, Raul and Zhang, Yiji and Jiang, Siyuan and Cai, Haipeng and Zhang, Ying-Jie},
title = {Quantitative Program Slicing: Separating Statements by Relevance},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Program slicing is a popular but imprecise technique for identifying which parts of a program affect or are affected by a particular value. A major reason for this imprecision is that slicing reports all program statements possibly affected by a value, regardless of how relevant to that value they really are. In this paper, we introduce quantitative slicing (q-slicing), a novel approach that quantifies the relevance of each statement in a slice. Q-slicing helps users and tools focus their attention first on the parts of slices that matter the most. We present two methods for quantifying slices and we show the promise of q-slicing for a particular application: predicting the impacts of changes. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1269–1272},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2486788.2486966,
author = {Hauptmann, Benedikt and Junker, Maximilian and Eder, Sebastian and Heinemann, Lars and Vaas, Rudolf and Braun, Peter},
title = {Hunting for Smells in Natural Language Tests},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Tests are central artifacts of software systems and play a crucial role for software quality. In system testing, a lot of test execution is performed manually using tests in natural language. However, those test cases are often poorly written without best practices in mind. This leads to tests which are not maintainable, hard to understand and inefficient to execute.  For source code and unit tests, so called code smells and test smells have been established as indicators to identify poorly written code. We apply the idea of smells to natural language tests by defining a set of common Natural Language Test Smells (NLTS). Furthermore, we report on an empirical study analyzing the extent in more than 2800 tests of seven industrial test suites. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1217–1220},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2432497.2432502,
author = {Selim, Gehan M. K. and Cordy, James R. and Dingel, Juergen},
title = {Model Transformation Testing: The State of the Art},
year = {2012},
isbn = {9781450318037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2432497.2432502},
doi = {10.1145/2432497.2432502},
abstract = {Model Driven Development (MDD) is a software engineering approach in which models constitute the basic units of software development. A key part of MDD is the notion of automated model transformation, in which models are stepwise refined into more detailed models, and eventually into code. The correctness of transformations is essential to the success of MDD, and while much research has concentrated on formal verification, testing remains the most efficient method of validation. Transformation testing is however different from testing code, and presents new challenges. In this paper, we survey the model transformation testing phases and the approaches proposed in the literature for each phase.},
booktitle = {Proceedings of the First Workshop on the Analysis of Model Transformations},
pages = {21–26},
numpages = {6},
keywords = {model driven development, test case generation, model transformation testing, contracts, mutation analysis},
location = {Innsbruck, Austria},
series = {AMT '12}
}

@inproceedings{10.1145/2351676.2351683,
author = {Gopinath, Divya and Zaeem, Razieh Nokhbeh and Khurshid, Sarfraz},
title = {Improving the Effectiveness of Spectra-Based Fault Localization Using Specifications},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351683},
doi = {10.1145/2351676.2351683},
abstract = { Fault localization i.e., locating faulty lines of code, is a key step in removing bugs and often requires substantial manual effort. Recent years have seen many automated localization techniques, specifically using the program’s passing and failing test runs, i.e., test spectra. However, the effectiveness of these approaches is sensitive to factors such as the type and number of faults, and the quality of the test-suite. This paper presents a novel technique that applies spectra-based localization in synergy with specification-based analysis to more accurately locate faults. Our insight is that unsatisfiability analysis of violated specifications, enabled by SAT technology, could be used to (1) compute unsatisfiable cores that contain likely faulty statements and (2) generate tests that help spectra-based localization. Our technique is iterative and driven by a feedback loop that enables more precise fault localization. SAT-TAR is a framework that embodies our technique for Java programs, including those with multiple faults. An experimental evaluation using a suite of widely-studied data structure programs, including the ANTLR and JTopas parser applications, shows that our technique localizes faults more accurately than state-of-the-art approaches. },
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {40–49},
numpages = {10},
keywords = {Tarantula, Automated Debugging, Alloy, Minimal UNSAT cores, Kodkod, Fault Localization},
location = {Essen, Germany},
series = {ASE 2012}
}

@inproceedings{10.1145/2338965.2336793,
author = {Zhang, Lingming and Marinov, Darko and Zhang, Lu and Khurshid, Sarfraz},
title = {Regression Mutation Testing},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336793},
doi = {10.1145/2338965.2336793},
abstract = { Mutation testing is one of the most powerful approaches for evaluating quality of test suites. However, mutation testing is also one of the most expensive testing approaches. This paper presents Regression Mutation Testing (ReMT), a new technique to speed up mutation testing for evolving systems. The key novelty of ReMT is to incrementally calculate mutation testing results for the new program version based on the results from the old program version; ReMT uses a static analysis to check which results can be safely reused. ReMT also employs a mutation-specific test prioritization to further speed up mutation testing. We present an empirical study on six evolving systems, whose sizes range from 3.9KLoC to 88.8KLoC. The empirical results show that ReMT can substantially reduce mutation testing costs, indicating a promising future for applying mutation testing on evolving software systems. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {331–341},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/2338965.2336789,
author = {Li, Kaituo and Reichenbach, Christoph and Csallner, Christoph and Smaragdakis, Yannis},
title = {Residual Investigation: Predictive and Precise Bug Detection},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336789},
doi = {10.1145/2338965.2336789},
abstract = { We introduce the concept of “residual investigation” for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when taken in conjunction with the static error report, increase confidence in the existence of an error, as well as its severity, without needing to directly observe a fault resulting from the error.  We enhance the static analyzer FindBugs with several residual investigations, appropriately tuned to the static error patterns in FindBugs, and apply it to 7 large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (“false positives”) while reporting several actual errors that would not have been detected by mere execution of a program's test suite. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {298–308},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@article{10.1145/2187671.2187673,
author = {Shahriar, Hossain and Zulkernine, Mohammad},
title = {Mitigating Program Security Vulnerabilities: Approaches and Challenges},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2187671.2187673},
doi = {10.1145/2187671.2187673},
abstract = {Programs are implemented in a variety of languages and contain serious vulnerabilities which might be exploited to cause security breaches. These vulnerabilities have been exploited in real life and caused damages to related stakeholders such as program users. As many security vulnerabilities belong to program code, many techniques have been applied to mitigate these vulnerabilities before program deployment. Unfortunately, there is no comprehensive comparative analysis of different vulnerability mitigation works. As a result, there exists an obscure mapping between the techniques, the addressed vulnerabilities, and the limitations of different approaches. This article attempts to address these issues. The work extensively compares and contrasts the existing program security vulnerability mitigation techniques, namely testing, static analysis, and hybrid analysis. We also discuss three other approaches employed to mitigate the most common program security vulnerabilities: secure programming, program transformation, and patching. The survey provides a comprehensive understanding of the current program security vulnerability mitigation approaches and challenges as well as their key characteristics and limitations. Moreover, our discussion highlights the open issues and future research directions in the area of program security vulnerability mitigation.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {11},
numpages = {46},
keywords = {secure programming, program transformation, vulnerability testing, static analysis, Program security vulnerability mitigation, patching, hybrid analysis}
}

@inproceedings{10.1109/ASE.2011.6100059,
author = {Robinson, Brian and Ernst, Michael D. and Perkins, Jeff H. and Augustine, Vinay and Li, Nuo},
title = {Scaling up Automated Test Generation: Automatically Generating Maintainable Regression Unit Tests for Programs},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100059},
doi = {10.1109/ASE.2011.6100059},
abstract = {This paper presents an automatic technique for generating maintainable regression unit tests for programs. We found previous test generation techniques inadequate for two main reasons. First. they were designed for and evaluated upon libraries rather than applications. Second, they were designed to find bugs rather than to create maintainable regression test suites: the test suites that they generated were brittle and hard to understand. This paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system. In experiments using an industrial system, the generated tests achieved good coverage and mutation kill score, were readable by the product's developers, and required few edits as the system under test evolved. While our evaluation is in the context of one test generator, we are aware of many research systems that suffer similar limitations, so our approach and observations are more generally relevant.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {23–32},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/1869542.1869567,
author = {Aaltonen, Kalle and Ihantola, Petri and Sepp\"{a}l\"{a}, Otto},
title = {Mutation Analysis vs. Code Coverage in Automated Assessment of Students' Testing Skills},
year = {2010},
isbn = {9781450302401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869542.1869567},
doi = {10.1145/1869542.1869567},
abstract = {Learning to program should include learning about proper software testing. Some automatic assessment systems, e.g. Web-CAT, allow assessing student-generated test suites using coverage metrics. While this encourages testing, we have observed that sometimes students can get rewarded from high coverage although their tests are of poor quality. Exploring alternative methods of assessment, we have tested mutation analysis to evaluate students' solutions. Initial results from applying mutation analysis to real course submissions indicate that mutation analysis could be used to fix some problems of code coverage in the assessment. Combining both metrics is likely to give more accurate feedback.},
booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
pages = {153–160},
numpages = {8},
keywords = {programming assignments, mutation testing, test coverage, mutation analysis, automated assessment},
location = {Reno/Tahoe, Nevada, USA},
series = {OOPSLA '10}
}

@inproceedings{10.1145/1831708.1831720,
author = {Dobolyi, Kinga and Weimer, Westley},
title = {Modeling Consumer-Perceived Web Application Fault Severities for Testing},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831720},
doi = {10.1145/1831708.1831720},
abstract = {Despite the growing usage of web applications, extreme resource constraints during their development frequently leave them inadequately tested. Because testing may be perceived as having a low return on investment for web applications, we believe that providing a consumer-perceived fault severity model could allow developers to prioritize faults according to their likelihood of impacting consumer retention, encouraging web application developers to test more effectively. In a study involving 386 humans and 800 web application faults, we observe that an arbitrary human judgment of fault severity is unreliable. We thus present two models of fault severity that outperform individual humans in terms of correctly predicting the average consumer-perceived severity of web application faults. Our first model uses human annotations of fault surface features, and is 87% accurate at identifying low-priority, non-severe faults. We also present a fully automated conservative model that correctly identifies 55% of non-severe faults without missing any severe faults. Both models outperform humans at flagging severe faults, and can substitute or reinforce humans by prioritizing faults encountered in web application development and testing.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {97–106},
numpages = {10},
keywords = {fault, web application, severity},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/1595696.1595706,
author = {Sherman, Elena and Dwyer, Matthew B. and Elbaum, Sebastian},
title = {Saturation-Based Testing of Concurrent Programs},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595706},
doi = {10.1145/1595696.1595706},
abstract = {Coverage measures help to determine whether a test suite exercises a program adequately according to a testing criterion. Many existing measures, however, are defined over coverage domains that cannot be precisely calculated, rendering them of limited value in assessing the extent of testing activities. To exploit the use of such measures, we formalize saturation-based test adequacy, a form of adequacy focused on the rate at which coverage increases during test suite execution. We define a family of coverage metrics for concurrent program testing that are well-suited to saturation-based adequacy and present a study that explores their cost and effectiveness. The results of this study suggest that saturation-based testing can serve as an effective complement to traditional notions of coverage-based testing.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {53–62},
numpages = {10},
keywords = {test adequacy criteria, concurrent programs, coverage},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/1572272.1572305,
author = {Halfond, William G.J. and Anand, Saswat and Orso, Alessandro},
title = {Precise Interface Identification to Improve Testing and Analysis of Web Applications},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572305},
doi = {10.1145/1572272.1572305},
abstract = {As web applications become more widespread, sophisticated, and complex, automated quality assurance techniques for such applications have grown in importance. Accurate interface identification is fundamental for many of these techniques, as the components of a web application communicate extensively via implicitly-defined interfaces to generate customized and dynamic content. However, current techniques for identifying web application interfaces can be incomplete or imprecise, which hinders the effectiveness of quality assurance techniques. To address these limitations, we present a new approach for identifying web application interfaces that is based on a specialized form of symbolic execution. In our empirical evaluation, we show that the set of interfaces identified by our approach is more accurate than those identified by other approaches. We also show that this increased accuracy leads to improvements in several important quality assurance techniques for web applications: test-input generation, penetration testing, and invocation verification.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {285–296},
numpages = {12},
keywords = {interface identification, web application testing},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/1572272.1572284,
author = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
title = {A Comparative Study of Programmer-Written and Automatically Inferred Contracts},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572284},
doi = {10.1145/1572272.1572284},
abstract = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination?Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect.Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts.We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {93–104},
numpages = {12},
keywords = {eiffel, dynamic contract inference},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/1368088.1368111,
author = {Rajan, Ajitha and Whalen, Michael W. and Heimdahl, Mats P.E.},
title = {The Effect of Program and Model Structure on Mc/Dc Test Adequacy Coverage},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368111},
doi = {10.1145/1368088.1368111},
abstract = {In avionics and other critical systems domains, adequacy of test suites is currently measured using the MC/DC metric on source code (or on a model in model-based development). We believe that the rigor of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be misleading as a test adequacy criterion. We investigate this hypothesis by empirically studying the effect of program structure on MC/DC coverage.To perform this investigation, we use six realistic systems from the civil avionics domain and two toy examples. For each of these systems, we use two versions of their implementation-with and without expression folding (i.e., inlining). To assess the sensitivity of MC/DC to program structure, we first generate test suites that satisfy MC/DC over a non-inlined implementation. We then run the generated test suites over the inlined implementation and measure MC/DC achieved. For our realistic examples, the test suites yield an average reduction of 29.5% in MC/DC achieved over the inlined implementations at 5% statistical significance level.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {161–170},
numpages = {10},
keywords = {structural coverage metrics},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/1291535.1291542,
author = {Fraser, Gordon and Wotawa, Franz},
title = {Using LTL Rewriting to Improve the Performance of Model-Checker Based Test-Case Generation},
year = {2007},
isbn = {9781595938503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1291535.1291542},
doi = {10.1145/1291535.1291542},
abstract = {Model-checkers have recently been suggested for automated software test-case generation. Several works have presented methods that create efficient test-suites using model-checkers. Ease of use and complete automation are major advantages of such approaches. However, the use of a model-checker comes at the price of potential performance problems. If the model used for test-case generation is complex, then model-checker based approaches can be very slow, or even not applicable at all. In this paper, we identify that unnecessary, redundant calls to the model-checker are one of the causes of bad performance. To overcome this problem, we suggest the use of temporal logic rewriting techniques, which originate from runtime verification research. This achieves a significant increase in the performance, and improves the applicability of model-checker based test-case generation approaches in general. At the same time, the suggested techniques achieve a reduction of the resulting test-suite sizes without degradation of the fault sensitivity. This helps to reduce the costs of the test-case execution.},
booktitle = {Proceedings of the 3rd International Workshop on Advances in Model-Based Testing},
pages = {64–74},
numpages = {11},
keywords = {test-case generation with model-checkers, LTL rewriting, automated software testing},
location = {London, United Kingdom},
series = {A-MOST '07}
}

@inproceedings{10.1145/1119655.1119681,
author = {Xie, Tao and Zhao, Jianjun},
title = {A Framework and Tool Supports for Generating Test Inputs of AspectJ Programs},
year = {2006},
isbn = {159593300X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1119655.1119681},
doi = {10.1145/1119655.1119681},
abstract = {Aspect-oriented software development is gaining popularity with the wider adoption of languages such as AspectJ. To reduce the manual effort of testing aspects in AspectJ programs, we have developed a framework, called Aspectra, that automates generation of test inputs for testing aspectual behavior, i.e., the behavior implemented in pieces of advice or intertype methods defined in aspects. To test aspects, developers construct base classes into which the aspects are woven to form woven classes. Our approach leverages existing test-generation tools to generate test inputs for the woven classes; these test inputs indirectly exercise the aspects. To enable aspects to be exercised during test generation, Aspectra automatically synthesizes appropriate wrapper classes for woven classes. To assess the quality of the generated tests, Aspectra defines and measures aspectual branch coverage (branch coverage within aspects). To provide guidance for developers to improve test coverage, Aspectra also defines interaction coverage. We have developed tools for automating Aspectra's wrapper synthesis and coverage measurement, and applied them on testing 12 subjects taken from a variety of sources. Our experience has shown that Aspectra effectively provides tool supports in enabling existing test-generation tools to generate test inputs for improving aspectual branch coverage.},
booktitle = {Proceedings of the 5th International Conference on Aspect-Oriented Software Development},
pages = {190–201},
numpages = {12},
keywords = {coverage criteria, coverage measurement, software testing, AspectJ, aspect-oriented programs, test generation, aspect-oriented software development},
location = {Bonn, Germany},
series = {AOSD '06}
}

@inproceedings{10.1145/1108792.1108795,
author = {Bradbury, Jeremy S. and Cordy, James R. and Dingel, Juergen},
title = {An Empirical Framework for Comparing Effectiveness of Testing and Property-Based Formal Analysis},
year = {2005},
isbn = {1595932399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1108792.1108795},
doi = {10.1145/1108792.1108795},
abstract = {Today, many formal analysis tools are not only used to provide certainty but are also used to debug software systems - a role that has traditional been reserved for testing tools. We are interested in exploring the complementary relationship as well as tradeoffs between testing and formal analysis with respect to debugging and more specifically bug detection. In this paper we present an approach to the assessment of testing and formal analysis tools using metrics to measure the quantity and efficiency of each technique at finding bugs. We also present an assessment framework that has been constructed to allow for symmetrical comparison and evaluation of tests versus properties. We are currently beginning to conduct experiments and this paper presents a discussion of possible outcomes of our proposed empirical study.},
booktitle = {Proceedings of the 6th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
pages = {2–5},
numpages = {4},
keywords = {mutation testing, model checking, empirical software engineering, bug detection, static analysis},
location = {Lisbon, Portugal},
series = {PASTE '05}
}

@article{10.1145/1108768.1108795,
author = {Bradbury, Jeremy S. and Cordy, James R. and Dingel, Juergen},
title = {An Empirical Framework for Comparing Effectiveness of Testing and Property-Based Formal Analysis},
year = {2005},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1108768.1108795},
doi = {10.1145/1108768.1108795},
abstract = {Today, many formal analysis tools are not only used to provide certainty but are also used to debug software systems - a role that has traditional been reserved for testing tools. We are interested in exploring the complementary relationship as well as tradeoffs between testing and formal analysis with respect to debugging and more specifically bug detection. In this paper we present an approach to the assessment of testing and formal analysis tools using metrics to measure the quantity and efficiency of each technique at finding bugs. We also present an assessment framework that has been constructed to allow for symmetrical comparison and evaluation of tests versus properties. We are currently beginning to conduct experiments and this paper presents a discussion of possible outcomes of our proposed empirical study.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {2–5},
numpages = {4},
keywords = {static analysis, empirical software engineering, bug detection, mutation testing, model checking}
}

@inproceedings{10.5555/776816.776824,
author = {Harder, Michael and Mellen, Jeff and Ernst, Michael D.},
title = {Improving Test Suites via Operational Abstraction},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text.The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications) from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults.This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {60–71},
numpages = {12},
location = {Portland, Oregon},
series = {ICSE '03}
}

