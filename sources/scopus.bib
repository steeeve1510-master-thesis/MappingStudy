Scopus
EXPORT DATE: 11 May 2021

@ARTICLE{Rani2021311,
author={Rani, S. and Suri, B.},
title={Investigating Different Metrics for Evaluation and Selection of Mutation Operators for Java},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2021},
volume={31},
number={3},
pages={311-336},
doi={10.1142/S021819402150011X},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103906332&doi=10.1142%2fS021819402150011X&partnerID=40&md5=28503884a356726b1ca27581af29a550},
abstract={Mutation testing is a successful and powerful technique, specifically designed for injecting the artificial faults. Although it is effective at revealing the faults, test suite assessment and its reduction, however, suffer from the expense of executing a large number of mutants. The researchers have proposed different types of cost reduction techniques in the literature. These techniques highly depend on the inspection of mutation operators. Several metrics have been evolved for the same. The selective mutation technique is most frequently used by the researchers. In this paper, the authors investigate different metrics for evaluating the traditional mutation operators for Java. Results on 13 Java programs indicate how grouping few operators can impact the effectiveness of an adequate and minimal test suite, and how this could provide several cost benefits. © 2021 World Scientific Publishing Company.},
author_keywords={cost-benefit analysis;  mutation operator metrics;  Mutation testing;  resistant mutant;  selective mutation;  weak mutant},
keywords={Computer software;  Cost reduction, Cost benefits;  Java program;  Mutation operators;  Mutation testing;  Selective mutation, Java programming language},
}

@ARTICLE{Sykora202158,
author={Sykora, K. and Ahmed, B.S. and Bures, M.},
title={Code Coverage Aware Test Generation Using Constraint Solver},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12524 LNCS},
pages={58-66},
doi={10.1007/978-3-030-67220-1_5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101580102&doi=10.1007%2f978-3-030-67220-1_5&partnerID=40&md5=835b6c40d8b4012afa9e9515eb50c6b3},
abstract={Code coverage has been used in the software testing context mostly as a metric to assess a generated test suite’s quality. Recently, code coverage analysis is used as a white-box testing technique for test optimization. Most of the research activities focus on using code coverage for test prioritization and selection within automated testing strategies. Less effort has been paid in the literature to use code coverage for test generation. This paper introduces a new Code Coverage-based Test Case Generation (CCTG) concept that changes the current practices by utilizing the code coverage analysis in the test generation process. CCTG uses the code coverage data to calculate the input parameters’ impact for a constraint solver to automate the generation of effective test suites. We applied this approach to a few real-world case studies. The results showed that the new test generation approach could generate effective test cases and detect new faults. © 2021, Springer Nature Switzerland AG.},
author_keywords={Automated test generation;  Code coverage;  Constrained interaction testing;  Software testing;  Test case augmentation},
keywords={Application programs;  Embedded systems;  Formal methods;  Logic programming, Automated testing;  Constraint solvers;  Current practices;  Research activities;  Test case generation;  Test optimization;  Test prioritization;  White-box testing, Software testing},
}

@CONFERENCE{Virginio2020564,
author={Virginio, T. and Martins, L. and Rocha, L. and Santana, R. and Cruz, A. and Costa, H. and Machado, I.},
title={JNose: Java Test Smell Detector},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={564-569},
doi={10.1145/3422392.3422499},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099368163&doi=10.1145%2f3422392.3422499&partnerID=40&md5=cef336d5ec4591dbb5facbe0ab2312c2},
abstract={Several strategies have been proposed for test quality measurement and analysis. Code coverage is likely the most widely used one. It enables to verify the ability of a test case to cover as many source code branches as possible. Although code coverage has been widely used, novel strategies have been recently employed. It is the case of test smells analysis, which has been introduced as an affordable strategy to evaluate the quality of test code. Test smells are poor design choices in implementation, and their occurrence in test code might reduce the quality of test suites. Test smells identification is clearly dependent on tool support, otherwise it could become a cost-ineffective strategy. However, as far as we know, there is no tool that combines code coverage and test smells to address test quality measurement. In this work, we present the JNose Test, a tool aimed to analyze test suite quality in the perspective of test smells. JNose Test detects code coverage and software evolution metrics and a set of test smells throughout software versions. © 2020 ACM.},
author_keywords={Code Coverage;  Quality of Tests;  Test Smells;  Test Suite Evolution},
keywords={Java programming language;  Odors;  Quality control, Code coverage;  Novel strategies;  Software Evolution;  Software versions;  Source codes;  Test case;  Test quality;  Tool support, Software testing},
}

@ARTICLE{Andrade2020,
author={Andrade, L. and Machado, P. and Andrade, W.},
title={Can operational profile coverage explain post-release bug detection?},
journal={Software Testing Verification and Reliability},
year={2020},
volume={30},
number={4-5},
doi={10.1002/stvr.1735},
art_number={e1735},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087570144&doi=10.1002%2fstvr.1735&partnerID=40&md5=f76742b7507ec54fd131e128134a871e},
abstract={To deliver reliable software, developers may rely on the fault detection capability of test suites. To evaluate this capability, they can apply code coverage metrics before a software release. However, recent research results have shown that these metrics may not provide a solid basis for this evaluation. Moreover, the fixing of a fault has a cost, and not all faults have the same impact regarding software reliability. In this sense, operational testing aims at assessing parts of the system that are more valuable for users. The goal of this work is to investigate whether traditional code coverage and code coverage merged with operational information can be related to post-release bug detection. We focus on the scope of proprietary software under continuous delivery. We performed an exploratory case study where code branch and statement coverage metrics were collected for each version of a proprietary software together with real usage data of the system. We then measured the ability to explain the bug-fixing activity after version release using code coverage levels. We found that traditional statement coverage has a moderate negative correlation with bug-fixing activities, whereas statement coverage merged with the operational profile has a large negative correlation with higher confidence. Developers can consider operational information as an important factor of influence that should be analysed, among other factors, together with code coverage to assess the fault detection capability of a test suite. © 2020 John Wiley & Sons, Ltd.},
author_keywords={fault detection capability;  operational profile;  test coverage},
keywords={Codes (symbols);  Fault detection;  Software reliability, Detection capability;  Exploratory case studies;  Negative correlation;  Operational profile;  Operational testing;  Proprietary software;  Recent researches;  Statement coverage, Software testing},
}

@CONFERENCE{Krotkov2020280,
author={Krotkov, V. and Danilenko, A.},
title={Experience of creating a library for testing c# and c++ console applications},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2667},
pages={280-283},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092240595&partnerID=40&md5=23ff88b8a725b238efa9cbb3acf404fb},
abstract={The present work provides a description of functionality of library aimed at simplifying creation of console applications in and C++ languages by granting opportunities to use built-in adjusted menu of any level of enclosure, special means for control over variables of the program and tracking of a condition of program entities. The library contains functions for manual and automatic testing and the multilevel analysis of console application's performance. The library also includes auxiliary functionality of random or sample generation of user-defined or standardtyped test data and gives opportunities for exact identification of mistakes made by programmer during development. The library is powered with modern technologies of object-oriented programming and developed according to the advanced architectural and algorithmic concepts. Copyright © 2020 for this paper by its authors.},
author_keywords={C#;  C++;  Console applications;  Data generation;  Library of tools;  Manual and automatic testing;  Parser},
keywords={Application programs;  Automatic testing;  C++ (programming language);  Data Science;  Nanotechnology, C++ language;  Modern technologies;  Multi-level analysis;  Sample generations;  Test data, Object oriented programming},
}

@ARTICLE{Magalhães2020,
author={Magalhães, C. and Andrade, J. and Perrusi, L. and Mota, A. and Barros, F. and Maia, E.},
title={HSP: A hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study},
journal={Journal of Systems and Software},
year={2020},
volume={159},
doi={10.1016/j.jss.2019.110430},
art_number={110430},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072983204&doi=10.1016%2fj.jss.2019.110430&partnerID=40&md5=8beb16e9c6171b91d5742b4c511bc298},
abstract={The usual way to guarantee quality of software products is via testing. This paper presents a novel strategy for selection and prioritisation of Test Cases (TC) for Regression testing. In the lack of code artifacts from where to derive Test Plans, this work uses information conveyed by textual documents maintained by Industry, such as Change Requests. The proposed process is based on Information Retrieval techniques combined with indirect code coverage measures to select and prioritise TCs. The aim is to provide a high coverage Test Plan which would maximise the number of bugs found. This process was implemented as a prototype tool which was used in a case study with our industrial partner (Motorola Mobility). Experiments results revealed that the combined strategy provides better results than the use of information retrieval and code coverage independently. Yet, it is worth mentioning that any of these automated options performed better than the previous manual process deployed by our industrial partner to create test plans. © 2019},
author_keywords={Code coverage;  Information retrieval;  Regression testing;  Static analysis;  Test cases selection and prioritisation},
keywords={Codes (symbols);  Information retrieval;  Information use;  Regression analysis;  Static analysis;  Testing, Code coverage;  Industrial case study;  Industrial partners;  Novel strategies;  Quality of softwares;  Regression testing;  Selection and prioritisation;  Textual documents, Software testing},
}

@ARTICLE{Gómez-Abajo20201,
author={Gómez-Abajo, P. and Guerra, E. and de Lara, J. and Merayo, M.G.},
title={Systematic Engineering of Mutation Operators},
journal={Journal of Object Technology},
year={2020},
volume={19},
number={3},
pages={1-15},
doi={10.5381/jot.2020.19.3.a5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100137296&doi=10.5381%2fjot.2020.19.3.a5&partnerID=40&md5=bea62fdd68eff4bb817b97d2b39c9f60},
abstract={In the context of software engineering, mutation consists in injecting small changes in artefacts – like models, programs, or data – for purposes like (mutation) testing, test data generation, and all sorts of search-based methods. These tasks typically require defining sets of mutation operators, which are often built ad-hoc because there is currently poor support for their development and testing. To improve this situation, we propose a methodology and corresponding tool support for the proper engineering of mutation operators. Our proposal is model-based, representing the artefacts to be mutated as models. It includes a domain-specific language to describe the mutation operators, facilities to synthesize models that can be used to test the operators, different metrics to analyse operator coverage, and services to generate operators when the coverage is insufficient. We show automated support atop the WODEL tool, and illustrate its use by defining mutation operators for UML Class Diagrams. © 2020, Journal of Object Technology. All Rights Reserved.},
author_keywords={class diagrams;  metrics;  model mutation;  model synthesis;  Model-driven engineering;  WODEL},
}

@CONFERENCE{Godio201991,
author={Godio, A. and Bengolea, V. and Ponzio, P. and Aguirre, N. and Frias, M.F.},
title={Efficient test generation guided by field coverage criteria},
journal={Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
year={2019},
pages={91-101},
doi={10.1109/ASE.2019.00019},
art_number={8952481},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078946569&doi=10.1109%2fASE.2019.00019&partnerID=40&md5=4fc51df4634da4468ea829b99f1a1137},
abstract={Field-exhaustive testing is a testing criterion suitable for object-oriented code over complex, heap-allocated, data structures. It requires test suites to contain enough test inputs to cover all feasible values for the object's fields within a certain scope (input-size bound). While previous work shows that field-exhaustive suites can be automatically generated, the generation technique required a formal specification of the inputs that can be subject to SAT-based analysis. Moreover, the restriction of producing all feasible values for inputs' fields makes test generation costly. In this paper, we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score, by examining to what extent the values of inputs' fields are covered. In particular, we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field-exhaustive suites, using the Symbolic Pathfinder tool. To underapproximate these suites we use tranScoping, a technique that estimates characteristics of yet to be run analyses for large scopes, based on data obtained from analyses performed in small scopes. This provides us with a suitable condition to prematurely stop the symbolic execution. As we show, tranScoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time. All this while retaining the effectiveness of field exhaustive suites in terms of test suite quality. © 2019 IEEE.},
author_keywords={Field-based testing;  Field-exhaustive testing;  Symbolic execution;  Transcoping},
keywords={Model checking;  Object oriented programming, Automatically generated;  Exhaustive testing;  Generation techniques;  Object-oriented code;  Sat-based analysis;  Suitable conditions;  Symbolic execution;  Transcoping, Testing},
}

@ARTICLE{Gergely2019797,
author={Gergely, T. and Balogh, G. and Horváth, F. and Vancsics, B. and Beszédes, Á. and Gyimóthy, T.},
title={Differences between a static and a dynamic test-to-code traceability recovery method},
journal={Software Quality Journal},
year={2019},
volume={27},
number={2},
pages={797-822},
doi={10.1007/s11219-018-9430-x},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058641652&doi=10.1007%2fs11219-018-9430-x&partnerID=40&md5=0d4c283eb8d6a32793484f6a88fec86b},
abstract={Recovering test-to-code traceability links may be required in virtually every phase of development. This task might seem simple for unit tests thanks to two fundamental unit testing guidelines: isolation (unit tests should exercise only a single unit) and separation (they should be placed next to this unit). However, practice shows that recovery may be challenging because the guidelines typically cannot be fully followed. Furthermore, previous works have already demonstrated that fully automatic test-to-code traceability recovery for unit tests is virtually impossible in a general case. In this work, we propose a semi-automatic method for this task, which is based on computing traceability links using static and dynamic approaches, comparing their results and presenting the discrepancies to the user, who will determine the final traceability links based on the differences and contextual information. We define a set of discrepancy patterns, which can help the user in this task. Additional outcomes of analyzing the discrepancies are structural unit testing issues and related refactoring suggestions. For the static test-to-code traceability, we rely on the physical code structure, while for the dynamic, we use code coverage information. In both cases, we compute combined test and code clusters which represent sets of mutually traceable elements. We also present an empirical study of the method involving 8 non-trivial open source Java systems. © 2018, The Author(s).},
author_keywords={Code coverage;  Refactoring;  Structural test smells;  Test-to-code traceability;  Traceability link recovery;  Unit testing},
keywords={Codes (symbols);  Open source software;  Recovery;  Software testing, Code coverage;  Refactorings;  Structural tests;  Traceability links;  Unit testing, Open systems},
}

@ARTICLE{McMinn2019427,
author={McMinn, P. and Wright, C.J. and McCurdy, C.J. and Kapfhammer, G.M.},
title={Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas},
journal={IEEE Transactions on Software Engineering},
year={2019},
volume={45},
number={5},
pages={427-463},
doi={10.1109/TSE.2017.2786286},
art_number={8240964},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040086250&doi=10.1109%2fTSE.2017.2786286&partnerID=40&md5=40aed3c54e20bffa551a69f2e119c0cc},
abstract={Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding strength of a test suite. As with program mutation, however, relational database schema mutation results in many ineffective mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems-HyperSQL, PostgreSQL, and SQLite-the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with heavyweight DBMSs like PostgreSQL. © 1976-2012 IEEE.},
author_keywords={relational databases;  software quality;  software testing;  software tools},
keywords={Computer software;  Relational database systems;  Software testing;  Taxonomies;  Testing, Algorithm design and analysis;  Automatic Detection;  Equivalent Mutants;  Google;  Mutation analysis;  Relational Database;  Relational database management systems;  Relational database schemata, Quality control},
}

@CONFERENCE{Wang2019502,
author={Wang, P. and Bai, G.R. and Stolee, K.T.},
title={Exploring Regular Expression Evolution},
journal={SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
year={2019},
pages={502-513},
doi={10.1109/SANER.2019.8667972},
art_number={8667972},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064174666&doi=10.1109%2fSANER.2019.8667972&partnerID=40&md5=4cec9cff3af89379065c46dceb56c0ad},
abstract={Although there are tools to help developers understand the matching behaviors between a regular expression and a string, regular-expression related faults are still common. Learning developers' behavior through the change history of regular expressions can identify common edit patterns, which can inform the creation of mutation and repair operators to assist with testing and fixing regular expressions. In this work, we explore how regular expressions evolve over time, focusing on the characteristics of regular expression edits, the syntactic and semantic difference of the edits, and the feature changes of edits. Our exploration uses two datasets. First, we look at GitHub projects that have a regular expression in their current version and look back through the commit logs to collect the regular expressions' edit history. Second, we collect regular expressions composed by study participants during problem-solving tasks. Our results show that 1) 95% of the regular expressions from GitHub are not edited, 2) most edited regular expressions have a syntactic distance of 4-6 characters from their predecessors, 3) over 50% of the edits in GitHub tend to expand the scope of regular expression, and 4) the number of features used indicates the regular expression language usage increases over time. This work has implications for supporting regular expression repair and mutation to ensure test suite quality. © 2019 IEEE.},
author_keywords={empirical studies;  evolution;  Regular expressions},
keywords={Pattern matching;  Problem solving;  Reengineering;  Repair;  Semantics;  Syntactics, Change history;  Empirical studies;  evolution;  Feature changes;  Matching behavior;  Regular expressions;  Repair operator;  Semantic difference, Computer programming languages},
}

@CONFERENCE{Vancsics201917,
author={Vancsics, B.},
title={NFL: Neighbor-Based Fault Localization Technique},
journal={IBF 2019 - 2019 IEEE 1st International Workshop on Intelligent Bug Fixing},
year={2019},
pages={17-22},
doi={10.1109/IBF.2019.8665491},
art_number={8665491},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063956496&doi=10.1109%2fIBF.2019.8665491&partnerID=40&md5=01b95694cf22c6216cdce73fe3d4cc4f},
abstract={Fault localization (FL) is a much-researched area, there are a lot of techniques that help programmers to find the location of the error (or bug) as accurately as possible. However, these automatic procedures are either quite costly or, in some cases, inaccurate. There is no general method that would be recognized as the generally accepted method of FL.The aim of our research was to create an automatic FL algorithm that helps the user with good and balanced results in effective error detection. The presented Neighbor-based FL (NFL) is a graph-based algorithm which transposes the coverage matrix into a graph and prioritizes the methods based on their connection to the passed and failed tests. It also uses this information to specify the location of the bug as precisely as possible.We did an empirical evaluation on Defects4J and 6 additional fault localization metrics were used for quantification. Thus, the results obtained were objectively judged and comparisons could be made.The results show that, on average, NFL found the location of bugs most accurately, and the results compared to other metrics proved to be satisfactory. © 2019 IEEE.},
author_keywords={fault localization;  program debugging;  program spectra;  software testing},
keywords={Graphic methods;  Location;  Software testing, Automatic procedures;  Empirical evaluations;  Fault localization;  General method;  Graph-based algorithms;  Program spectra, Program debugging},
}

@ARTICLE{Vancsics2019372,
author={Vancsics, B.},
title={Graph-Based Fault Localization},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11622 LNCS},
pages={372-387},
doi={10.1007/978-3-030-24305-0_28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068603837&doi=10.1007%2f978-3-030-24305-0_28&partnerID=40&md5=db4a59b2f2dd6bf61f82f2e1f8e38a0a},
abstract={The subject of fault localization (FL) is a much-researched area, it has large literature. There are plenty of algorithms that try to identify the location of the bugs using different approaches. Debugging has a large resource requirement, therefore the bug’s location’s reflective identifiicaton greatly helps developers and testers to maintain the quality and reliability of the software. Our goal is to implement a graph-based Fl GFL approach that effectively finds the location of the bugs in the source code. In our research we performed an empirical evaluation using the Defects4J and the results were compared with six other algorithms accepted by the literature. The results show that our method finds the errors more effectively than the other presented procedures, thus speeding up the bug fixes. © 2019, Springer Nature Switzerland AG.},
author_keywords={Fault localization;  Program debugging;  Program spectra;  Software testing},
keywords={Graphic methods;  Location;  Software reliability;  Software testing, Bug fixes;  Empirical evaluations;  Fault localization;  Graph-based;  Program spectra;  Resource requirements;  Source codes, Program debugging},
}

@CONFERENCE{Vercammen201823,
author={Vercammen, S. and Ghafari, M. and Demeyer, S. and Borg, M.},
title={Goal-oriented mutation testing with focal methods},
journal={A-TEST 2018 - Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, Co-located with FSE 2018},
year={2018},
pages={23-30},
doi={10.1145/3278186.3278190},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061789604&doi=10.1145%2f3278186.3278190&partnerID=40&md5=bcdf0076ff1fbcf3c64ed9e89bc9facf},
abstract={Mutation testing is the state-of-the-art technique for assessing the fault-detection capacity of a test suite. Unfortunately, mutation testing consumes enormous computing resources because it runs the whole test suite for each and every injected mutant. In this paper we explore fine-grained traceability links at method level (named focal methods), to reduce the execution time of mutation testing and to verify the quality of the test cases for each individual method, instead of the usually verified overall test suite quality. Validation of our approach on the open source Apache Ant project shows a speed-up of 573.5x for the mutants located in focal methods with a quality score of 80%. © 2018 Copyright held by the owner/author(s)..},
author_keywords={Feasibility study;  Focal methods;  Mutation testing;  Software testing},
keywords={Fault detection;  Open source software;  Testing, Computing resource;  Feasibility studies;  Focal methods;  Goal-oriented;  Mutation testing;  Open sources;  State-of-the-art techniques;  Traceability links, Software testing},
}

@CONFERENCE{Zhu2018289,
author={Zhu, Q. and Zaidman, A.},
title={Mutation testing for physical computing},
journal={Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018},
year={2018},
pages={289-300},
doi={10.1109/QRS.2018.00042},
art_number={8424980},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052309792&doi=10.1109%2fQRS.2018.00042&partnerID=40&md5=69358288b9d1ba7d1ffd4a3b677d2c3e},
abstract={Physical computing, which builds interactive systems between the physical world and computers, has been widely used in a wide variety of domains and applications, e.g., the Internet of Things (IoT). Although physical computing has witnessed enormous realisations, testing these physical computing systems still face many challenges, such as potential circuit related bugs which are not part of the software problems, the timing issue which decreasing the testability, etc.; therefore, we proposed a mutation testing approach for physical computing systems to enable engineers to judge the quality of their tests in a more accurate way. The main focus is the communication between the software and peripherals. More particular, we first defined a set of mutation operators based on the common communication errors between the software and peripherals that could happen in the software. We conducted a preliminary experiment on nine physical computing projects based on the Raspberry Pi and Arduino platforms. The results show that our mutation testing method can assess the test suite quality effectively in terms of weakness and inadequacy. © 2018 IEEE.},
keywords={Computer software selection and evaluation;  Internet of things;  Program debugging;  Software reliability;  Testing, Arduino platforms;  Communication errors;  Interactive system;  Internet of thing (IOT);  Mutation operators;  Physical computing;  Physical computing systems;  Software problems, Software testing},
}

@BOOK{Lämmel20181,
author={Lämmel, R.},
title={Software languages: Syntax, semantics, and metaprogramming},
journal={Software Languages: Syntax, Semantics, and Metaprogramming},
year={2018},
pages={1-424},
doi={10.1007/978-3-319-90800-7},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037105274&doi=10.1007%2f978-3-319-90800-7&partnerID=40&md5=d64c02903bf91fd7140d190385307bb6},
abstract={This book identifies, defines and illustrates the fundamental concepts and engineering techniques relevant to applications of software languages in software development. It presents software languages primarily from a software engineering perspective, i.e., it addresses how to parse, analyze, transform, generate, format, and otherwise process software artifacts in different software languages, as they appear in software development. To this end, it covers a wide range of software languages - most notably programming languages, domain-specific languages, modeling languages, exchange formats, and specifically also language definition languages. Further, different languages are leveraged to illustrate software language engineering concepts and techniques. The functional programming language Haskell dominates the book, while the mainstream programming languages Python and Java are additionally used for illustration. By doing this, the book collects and organizes scattered knowledge from software language engineering, focusing on application areas such as software analysis (software reverse engineering), software transformation (software re-engineering), software composition (modularity), and domain-specific languages. It is designed as a textbook for independent study as well as for bachelor's (advanced level) or master's university courses in Computer Science. An additional website provides complementary material, for example, lecture slides and videos. This book is a valuable resource for anyone wanting to understand the fundamental concepts and important engineering principles underlying software languages, allowing them to acquire much of the operational intelligence needed for dealing with software languages in software development practice. This is an important skill set for software engineers, as languages are increasingly permeating software development. © Springer International Publishing AG, part of Springer Nature 2018. All right reserved.},
}

@CONFERENCE{Minhas201825,
author={Minhas, N.M. and Petersen, K. and Ali, N.B. and Wnuk, K.},
title={Regression testing goals-view of practitioners and researchers},
journal={Proceedings - 2017 24th Asia-Pacific Software Engineering Conference Workshops, APSECW 2017},
year={2018},
volume={2018-January},
pages={25-31},
doi={10.1109/APSECW.2017.23},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050614037&doi=10.1109%2fAPSECW.2017.23&partnerID=40&md5=88b112fd8eb0ea1a3befc24bddae8f1c},
abstract={Context: Regression testing is a well-researched area. However,the majority regression testing techniques proposed by theresearchers are not getting the attention of the practitioners. Communicationgaps between industry and academia and disparity in theregression testing goals are the main reasons. Close collaboration canhelp in bridging the communication gaps and resolving the disparities.Objective: The study aims at exploring the views of academicsand practitioners about the goals of regression testing. The purpose isto investigate the commonalities and differences in their viewpointsand defining some common goals for the success of regression testing.Method: We conducted a focus group study, with 7 testingexperts from industry and academia. 4 testing practitioners from 2companies and 3 researchers from 2 universities participated in thestudy. We followed GQM approach, to elicit the regression testinggoals, information needs, and measures.Results: 43 regression testing goals were identified by theparticipants, which were reduced to 10 on the basis of similarityamong the identified goals. Later during the priority assignmentprocess, 5 goals were discarded, because the priority assigned tothese goals was very low. Participants identified 47 informationneeds/questions required to evaluate the success of regression testingwith reference to goal G5 (confidence). Which were then reduced to10 on the basis of similarity. Finally, we identified measures to gaugethose information needs/questions, which were corresponding to thegoal (G5).Conclusions: We observed that participation level ofpractitioners and researchers during the elicitation of goals andquestions was same. We found a certain level of agreement betweenthe participants regarding the regression testing definitions and goals.But there was some level of disagreement regarding the prioritiesof the goals. We also identified the need to implement a regressiontesting evaluation framework in the participating companies. © 2017 IEEE.},
author_keywords={Focus group;  GQM;  Regression testing;  Regression testing goals},
keywords={Software testing, Communication gaps;  Evaluation framework;  Focus group studies;  Focus groups;  Regression testing;  Regression testing techniques, Regression analysis},
}

@CONFERENCE{Brünink2018150,
author={Brünink, M. and Rosenblum, D.S.},
title={Using Branch Frequency Spectra to Evaluate Operational Coverage},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2018},
volume={2017-December},
pages={150-159},
doi={10.1109/APSEC.2017.21},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045944394&doi=10.1109%2fAPSEC.2017.21&partnerID=40&md5=053e7115fcb43aa494b0cd8556053981},
abstract={Coverage metrics try to quantify how well a software artifact is tested. High coverage numbers instill confidence in the software and might even be necessary to obtain certification. Unfortunately, achieving high coverage numbers does not imply high quality of the test suite. One shortcoming is that coverage metrics do not measure how well test suites cover systems in production. We look at coverage from an operational perspective. We evaluate test suite quality by comparing runs executed during testing with runs executed in production. Branch frequency spectra are employed to capture the behavior during runtime. Differences in the branch frequency spectra between field executions and testing runs indicate test suite deficiencies. This post-release test suite quality assurance mechanism can be used to (1) build confidence by pooling coverage information from many execution sites and (2) guide test suite augmentation in order to prepare the test suite for the next release cycle. © 2017 IEEE.},
author_keywords={Code coverage;  Operational coverage;  Program spectra},
keywords={Quality assurance;  Quality control;  Spectroscopy;  Well testing, Code coverage;  Coverage metrics;  Frequency spectra;  High quality;  Operational coverage;  Program spectra;  Release cycles;  Software artifacts, Software testing},
}

@ARTICLE{RehmanKhan201811816,
author={Rehman Khan, S.U. and Lee, S.P. and Javaid, N. and Abdul, W.},
title={A Systematic Review on Test Suite Reduction: Approaches, Experiment's Quality Evaluation, and Guidelines},
journal={IEEE Access},
year={2018},
volume={6},
pages={11816-11841},
doi={10.1109/ACCESS.2018.2809600},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042848923&doi=10.1109%2fACCESS.2018.2809600&partnerID=40&md5=3060a13d850e9ea051d23ca67200e0ab},
abstract={Regression testing aims at testing a system under test (SUT) in the presence of changes. As a SUT changes, the number of test cases increases to handle the modifications, and ultimately, it becomes practically impossible to execute all of them within limited testing budget. Test suite reduction (TSR) approaches are widely used to improve the regression testing costs by selecting representative test suite without compromising effectiveness, such as fault-detection capability, within allowed time budget. The aim of this systematic review is to identify state-of-the-art TSR approaches categories, assess the quality of experiments reported on this subject, and provide a set of guidelines for conducting future experiments in this area of research. After applying a two-facet study selection procedure, we finalized 113 most relevant studies from an initial pool of 4230 papers published in the field of TSR between 1993 and 2016. The TSR approaches are broadly classified into four main categories based on the literature including greedy, clustering, search, and hybrid approaches. It is noted that majority of the experiments in TSR do not follow any specific guidelines for planning, conducting, and reporting the experiments, which may pose validity threats related to their results. Thus, we recommend conducting experiments that are better designed for the future. In this direction, an initial set of recommendations is provided that are useful for performing well-designed experiments in the field of TSR. Furthermore, we provide a number of future research directions based on current trends in this field of research. © 2013 IEEE.},
author_keywords={experiments;  guidelines;  regression testing;  Software testing;  test suite reduction},
keywords={Budget control;  Clustering algorithms;  Experiments;  Fault detection;  Optimization;  Reduction;  Regression analysis;  Testing;  Web services, Guidelines;  Regression testing;  Software systems;  Systematics;  Test suite reduction, Software testing},
}

@ARTICLE{Gergely2018903,
author={Gergely, T. and Balogh, G. and Horváth, F. and Vancsics, B. and Beszédes, Á. and Gyimóthy, T.},
title={Analysis of static and dynamic test-to-code traceability information},
journal={Acta Cybernetica},
year={2018},
volume={23},
number={3},
pages={903-919},
doi={10.14232/actacyb.23.3.2018.11},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052407712&doi=10.14232%2factacyb.23.3.2018.11&partnerID=40&md5=dcf86298b9df8ef6ed836326105f4340},
abstract={Unit test development has some widely accepted guidelines. Two of them concern the test and code relationship, namely isolation (unit tests should examine only a single unit) and separation (they should be placed next to this unit). These guidelines are not always kept by the developers. They can however be checked by investigating the relationship between tests and the source code, which is described by test-to-code traceability links. Still, these links perhaps cannot be inferred unambiguously from the test and production code. We developed a method that is based on the computation of traceability links for different aspects and report Structural Unit Test Smells where the traceability links for the different aspects do not match. The two aspects are the static structure of the code that reflects the intentions of the developers and testers and the dynamic coverage which reveals the actual behavior of the code during test execution. In this study, we investigated this method on real programs. We manually checked the reported Structural Unit Test Smells to find out whether they are real violations of the unit testing rules. Furthermore, the smells were analyzed to determine their root causes and possible ways of correction. Copyright © 2018 Institute of Informatics, University of Szeged. All rights reserved.},
author_keywords={Code coverage;  Refactoring;  Test smells;  Test-to-code traceability;  Unit testing},
keywords={Codes (symbols);  Odors, Code coverage;  Dynamic coverages;  Refactorings;  Static and dynamic tests;  Static structures;  Traceability information;  Traceability links;  Unit testing, Testing},
}

@CONFERENCE{Bach2017307,
author={Bach, T. and Andrzejak, A. and Pannemans, R. and Lo, D.},
title={The Impact of Coverage on Bug Density in a Large Industrial Software Project},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2017},
volume={2017-November},
pages={307-313},
doi={10.1109/ESEM.2017.44},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042384798&doi=10.1109%2fESEM.2017.44&partnerID=40&md5=926786df8be5dda473f3fd8671108953},
abstract={Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing. To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project. © 2017 IEEE.},
author_keywords={bug densitiy;  coverage;  empirical research;  industry project;  large real world project;  software quality},
keywords={Codes (symbols);  Computer software selection and evaluation;  Software engineering, bug densitiy;  coverage;  Empirical research;  Industry project;  Real world projects;  Software Quality, Software testing},
}

@CONFERENCE{Magalhães2017,
author={Magalhães, C. and Andrade, J. and Perrusi, L. and Mota, A.},
title={Evaluating an automatic text-based test case selection using a non-instrumented code coverage analysis},
journal={ACM International Conference Proceeding Series},
year={2017},
volume={Part F130656},
doi={10.1145/3128473.3128478},
art_number={5},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030473731&doi=10.1145%2f3128473.3128478&partnerID=40&md5=c23e6344b323e41b01849f1952a60f15},
abstract={During development, systems may be tested several times. In general, a system evolves from change requests, aiming at improving its behavior in terms of new features as well as fixing failures. Thus, selecting the best test plan in terms of the closeness between test cases and the changed code and its dependencies is pursued by industry and academia. In this paper we measure the coverage achieved by an automatic test case selection based on information retrieval that relates change requests and test cases. But instead of using off-the-shelf coverage tools, like JaCoCo, we propose a way of obtaining code coverage of Android apk's without instrumentation. This was a basic requirement of our industrial partner. We performed some experiments on this industrial partner and promising results were obtained. © 2017 Association for Computing Machinery.},
author_keywords={Code coverage;  Information Retrieval;  Test case selection and prioritization},
keywords={Codes (symbols);  Information retrieval, Code coverage;  Industrial partners;  Instrumented code;  Prioritization;  Test case;  Test case selection;  Test plan, Software testing},
}

@ARTICLE{Gopinath2017871,
author={Gopinath, R. and Ahmed, I. and Alipour, M.A. and Jensen, C. and Groce, A.},
title={Does choice of mutation tool matter?},
journal={Software Quality Journal},
year={2017},
volume={25},
number={3},
pages={871-920},
doi={10.1007/s11219-016-9317-7},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966667381&doi=10.1007%2fs11219-016-9317-7&partnerID=40&md5=b0e1bc973b3ce995d298e1c11603f2ba},
abstract={Though mutation analysis is the primary means of evaluating the quality of test suites, it suffers from inadequate standardization. Mutation analysis tools vary based on language, when mutants are generated (phase of compilation), and target audience. Mutation tools rarely implement the complete set of operators proposed in the literature and mostly implement at least a few domain-specific mutation operators. Thus different tools may not always agree on the mutant kills of a test suite. Few criteria exist to guide a practitioner in choosing the right tool for either evaluating effectiveness of a test suite or for comparing different testing techniques. We investigate an ensemble of measures for evaluating efficacy of mutants produced by different tools. These include the traditional difficulty of detection, strength of minimal sets, and the diversity of mutants, as well as the information carried by the mutants produced. We find that mutation tools rarely agree. The disagreement between scores can be large, and the variation due to characteristics of the project—even after accounting for difference due to test suites—is a significant factor. However, the mean difference between tools is very small, indicating that no single tool consistently skews mutation scores high or low for all projects. These results suggest that experiments yielding small differences in mutation score, especially using a single tool, or a small number of projects may not be reliable. There is a clear need for greater standardization of mutation analysis. We propose one approach for such a standardization. © 2016, Springer Science+Business Media New York.},
author_keywords={Empirical analysis;  Mutation analysis;  Software testing},
keywords={Quality control;  Standardization;  Testing, Domain specific;  Empirical analysis;  Mutation analysis;  Mutation operators;  Mutation score;  Target audience;  Testing technique, Software testing},
}

@ARTICLE{Gopinath2017854,
author={Gopinath, R. and Ahmed, I. and Alipour, M.A. and Jensen, C. and Groce, A.},
title={Mutation reduction strategies considered harmful},
journal={IEEE Transactions on Reliability},
year={2017},
volume={66},
number={3},
pages={854-874},
doi={10.1109/TR.2017.2705662},
art_number={7942146},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020385899&doi=10.1109%2fTR.2017.2705662&partnerID=40&md5=111ae5063220437a27e8ca4e1839db84},
abstract={Mutation analysis is a well known yet unfortunately costly method for measuring test suite quality. Researchers have proposed numerous mutation reduction strategies in order to reduce the high cost of mutation analysis, while preserving the representativeness of the original set of mutants. As mutation reduction is an area of active research, it is important to understand the limits of possible improvements. We theoretically and empirically investigate the limits of improvement in effectiveness from using mutation reduction strategies compared to random sampling. Using real-world open source programs as subjects, we find an absolute limit in improvement of effectiveness over random sampling - 13.078%. Given our findings with respect to absolute limits, one may ask: How effective are the extant mutation reduction strategies? We evaluate the effectiveness of multiple mutation reduction strategies in comparison to random sampling. We find that none of the mutation reduction strategies evaluated - many forms of operator selection, and stratified sampling (on operators or program elements) - produced an effectiveness advantage larger than 5% in comparison with random sampling. Given the poor performance of mutation selection strategies - they may have a negligible advantage at best, and often perform worse than random sampling - we caution practicing testers against applying mutation reduction strategies without adequate justification. © 1963-2012 IEEE.},
author_keywords={Mutation analysis;  software testing},
keywords={Software testing, Multiple mutations;  Mutation analysis;  Open source projects;  Operator selections;  Poor performance;  Program elements;  Reduction strategy;  Stratified sampling, Quality control},
}

@CONFERENCE{Bowes20179,
author={Bowes, D. and Hall, T. and Petrić, J. and Shippey, T. and Turhan, B.},
title={How Good Are My Tests?},
journal={International Workshop on Emerging Trends in Software Metrics, WETSoM},
year={2017},
pages={9-14},
doi={10.1109/WETSoM.2017.2},
art_number={7968009},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026860594&doi=10.1109%2fWETSoM.2017.2&partnerID=40&md5=34561c928d5976168d8bb9cc29353f13},
abstract={Background: Test quality is a prerequisite for achieving production system quality. While the concept of quality is multidimensional, most of the effort in testing context hasbeen channelled towards measuring test effectiveness. Objective: While effectiveness of tests is certainly important, we aim to identify a core list of testing principles that also address other quality facets of testing, and to discuss how they can be quantified as indicators of test quality. Method: We have conducted a two-day workshop with our industry partners to come up with a list of relevant principles and best practices expected to result in high quality tests. We then utilised our academic and industrial training materials together with recommendations in practitioner oriented testing books to refine the list. We surveyed existing literature for potential metrics to quantify identified principles. Results: We have identified a list of 15 testing principles to capture the essence of testing goals and best practices from quality perspective. Eight principles do not map toexisting test smells and we propose metrics for six of those. Further, we have identified additional potential metrics for the seven principles that partially map to test smells. Conclusion: We provide a core list of testing principles along with a discussion of possible ways to quantify them for assessing goodness of tests. We believe that our work would be useful for practitioners in assessing the quality of their tests from multiple perspectives including but not limited to maintainability, comprehension and simplicity. © 2017 IEEE.},
author_keywords={metrics;  test quality;  unit testing},
keywords={Materials testing;  Odors, Best practices;  High Quality Test;  Industrial training;  metrics;  Production system;  Test effectiveness;  Test quality;  Unit testing, Testing},
}

@CONFERENCE{Wang2017321,
author={Wang, Q. and Brun, Y. and Orso, A.},
title={Behavioral Execution Comparison: Are Tests Representative of Field Behavior?},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
year={2017},
pages={321-332},
doi={10.1109/ICST.2017.36},
art_number={7927986},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020753963&doi=10.1109%2fICST.2017.36&partnerID=40&md5=1e7452cb6ae8582674c935686b54d63d},
abstract={Software testing is the most widely used approach for assessing and improving software quality, but it is inherently incomplete and may not be representative of how the software is used in the field. This paper addresses the questions of to what extent tests represent how real users use software, and how to measure behavioral differences between test and field executions. We study four real-world systems, one used by endusers and three used by other (client) software, and compare test suites written by the systems' developers to field executions using four models of behavior: statement coverage, method coverage, mutation score, and a temporal-invariant-based model we developed. We find that developer-written test suites fail to accurately represent field executions: the tests, on average, miss 6.2% of the statements and 7.7% of the methods exercised in the field, the behavior exercised only in the field kills an extra 8.6% of the mutants, finally, the tests miss 52.6% of the behavioral invariants that occur in the field. In addition, augmenting the in-house test suites with automatically-generated tests by a tool targeting high code coverage only marginally improves the tests' behavioral representativeness. These differences between field and test executions - and in particular the finer-grained and more sophisticated ones that we measured using our invariantbased model - can provide insight for developers and suggest a better method for measuring test suite quality. © 2017 IEEE.},
author_keywords={Field data;  Model inference;  Software testing},
keywords={Automatic test pattern generation;  Computer software selection and evaluation;  Online systems;  Testing;  Verification, Automatically generated;  Field data;  Model inference;  Real-world system;  Software Quality;  Statement coverage;  Temporal invariants;  Test execution, Software testing},
}

@CONFERENCE{Felbinger2017171,
author={Felbinger, H. and Wotawa, F. and Nica, M.},
title={Mutation Score, Coverage, Model Inference: Quality Assessment for T-Way Combinatorial Test-Suites},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2017},
year={2017},
pages={171-180},
doi={10.1109/ICSTW.2017.36},
art_number={7899053},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018371142&doi=10.1109%2fICSTW.2017.36&partnerID=40&md5=d483a7a148df34b3a0338941e623e3d7},
abstract={In this paper we assess and evaluate the quality of t-way combinatorial test-suites using three different test-suite quality assessment methods. As t-way combinatorial test-suites reduce the input space of a program under test, we investigate how an increasing t affects the quality of the test-suite. There are some limitations of existing test-suite quality assessment methods e.g. the number of mutants is limited by execution time and code coverage measurement might be intrusive due to changes of the behavior of the program under test when instrumenting the code. Here we generate t-way combinatorial test-suites for Java programs of different size. We compute mutation score and code coverage for the generated test-suites, and apply additionally a new model inference based approach, that does not require to execute the program under test, to compare the generated test-suites with each other and assign a quality valuation to the test-suites. Our results show that an increasing t generally raises test-suite quality in terms of mutation score, coverage, and model inference. However, the model inference approach is only applicable, if the outcomes of the programs under test are discrete values, and if the number of discrete values is less than the test-suite size. © 2017 IEEE.},
author_keywords={Combinatorial testing;  Coverage;  Model inference;  Mutation score},
keywords={Codes (symbols);  Computer software;  Testing;  Verification, Code coverage;  Combinatorial testing;  Coverage;  Different sizes;  Discrete values;  Model inference;  Mutation score;  Quality assessment, Software testing},
}

@CONFERENCE{VanBeckhoven2017,
author={Van Beckhoven, P. and Oprescu, A. and Bruntink, M.},
title={Assessing test suite eectiveness using static metrics},
journal={CEUR Workshop Proceedings},
year={2017},
volume={2070},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045417698&partnerID=40&md5=454fa40a88fc0151a9da3597e418c942},
abstract={With the increasing amount of automated tests, we need ways to measure the test eectiveness. The state-of-the-art technique for assessing test eectiveness, mutation testing, is too slow and cumbersome to be used in large scale evolution studies or code audits by external companies. In this paper we investigated two alternatives, namely code coverage and assertion count. We discovered that code coverage outperforms assertion count by showing a relation with test suite eec-tiveness for all analysed project. Assertion count only displays such a relation in only one of the analysed projects. Further analysing this relationship between assertion count coverage and test eectiveness would allow to circumvent some of the problems of mutation testing. Copyright c by the paper's authors. Copying permitted for private and academic purposes.},
keywords={Codes (symbols);  Software testing, Automated test;  Code Audit;  Code coverage;  Large-scale evolution;  Mutation testing;  State-of-the-art techniques;  Static metrics;  Test suites, Testing},
}

@CONFERENCE{Ahmed2016547,
author={Ahmed, I. and Gopinath, R. and Brindescu, C. and Groce, A. and Jensen, C.},
title={Can testedness be effectively measured?},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={547-558},
doi={10.1145/2950290.2950324},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997418602&doi=10.1145%2f2950290.2950324&partnerID=40&md5=dda5f661050c3d8d817424775170a2d3},
abstract={Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and decid-ing when to stop testing. Test the least-Tested code, and stop when all code is well-Tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite qual-ity. The first measure is statement coverage, the simplest and best-known code coverage measure. The second mea-sure is mutation score, a supposedly more powerful, though expensive, measure. We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer fu-ture bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring tested-ness. Using a large number of open source Java programs from Github and Apache, we show that both statement cov-erage and mutation score have only a weak negative corre-lation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Pro-gram elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.},
author_keywords={Coverage criteria;  Mutation testing;  Sta-Tistical analysis;  Test suite evaluation},
keywords={Codes (symbols);  Computer software;  Java programming language;  Open source software;  Software engineering, Coverage criteria;  Mutation testing;  Program elements;  Research evaluation;  Software engineering practices;  Sta-Tistical analysis;  Statement coverage;  Strong correlation, Software testing},
}

@CONFERENCE{Parsai2016365,
author={Parsai, A. and Murgia, A. and Demeyer, S.},
title={A Model to Estimate First-Order Mutation Coverage from Higher-Order Mutation Coverage},
journal={Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016},
year={2016},
pages={365-373},
doi={10.1109/QRS.2016.48},
art_number={7589816},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995519530&doi=10.1109%2fQRS.2016.48&partnerID=40&md5=25ea0c780048802f0ff532d40d293b5b},
abstract={The test suite is essential for fault detection during software development. First-order mutation coverage is an accurate metric to quantify the quality of the test suite. However, it is computationally expensive. Hence, the adoption of this metric is limited. In this study, we address this issue by proposing a realistic model able to estimate first-order mutation coverage using only higher-order mutation coverage. Our study shows how the estimation evolves along with the order of mutation. We validate the model with an empirical study based on 17 open-source projects. © 2016 IEEE.},
author_keywords={Higher-order Mutation Testing;  Model;  Mutation Testing;  Software Testing;  Test Suite Quality},
keywords={Computer software selection and evaluation;  Fault detection;  Models;  Open source software;  Software design;  Software reliability, Empirical studies;  First order;  Higher-order;  Mutation testing;  Open source projects;  Realistic model, Software testing},
}

@CONFERENCE{Tengeri2016174,
author={Tengeri, D. and Vidács, L. and Beszédes, A. and Jász, J. and Balogh, G. and Vancsics, B. and Gyimóthy, T.},
title={Relating Code Coverage, Mutation Score and Test Suite Reducibility to Defect Density},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
year={2016},
pages={174-179},
doi={10.1109/ICSTW.2016.25},
art_number={7528960},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992222115&doi=10.1109%2fICSTW.2016.25&partnerID=40&md5=f7a1ebc4a978afcb286b920b20848b39},
abstract={Assessing the overall quality (adequacy for a particular purpose) of existing test suites is a complex task. Their code coverage is a simple yet powerful attribute for this purpose, so the additional benefits of mutation analysis may not always justify the comparably much higher costs and complexity of the computation. Mutation testing methods and tools slowly start to reach a maturity level at which their use in everyday industrial practice becomes possible, yet it is still not completely clear in which situations they provide additional insights into various quality attributes of the test suites. This paper reports on an experiment conducted on four open source systems' test suites to compare them from the viewpoints of code coverage, mutation score and test suite reducibility (the amount test adequacy is degraded in a reduced test suite). The purpose of the comparison is to find out when the different attributes provide additional insights with respect to defect density, a separately computed attribute for the estimation of real faults. We demonstrate that in some situations code coverage might be a sufficient indicator of the expected defect density, but mutation and reducibility are better in most of the cases. © 2016 IEEE.},
author_keywords={code coverage;  defect density;  Mutation analysis;  test adequacy criteria;  test suite reduction},
keywords={Codes (symbols);  Defect density;  Defects;  Open source software;  Open systems;  Testing;  Verification, Code coverage;  Industrial practices;  Mutation analysis;  Open source system;  Quality attributes;  Reduced test suites;  Test adequacy criteria;  Test suite reduction, Software testing},
}

@CONFERENCE{Holling2016152,
author={Holling, D. and Banescu, S. and Probst, M. and Petrovska, A. and Pretschner, A.},
title={Nequivack: Assessing Mutation Score Confidence},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
year={2016},
pages={152-161},
doi={10.1109/ICSTW.2016.29},
art_number={7528957},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992188057&doi=10.1109%2fICSTW.2016.29&partnerID=40&md5=69b302a57f396d9853d1c46c6ab910cc},
abstract={The mutation score is defined as the number of killed mutants divided by the number of non-equivalent mutants. However, whether a mutant is equivalent to the original program is undecidable in general. Thus, even when improving a test suite, a mutant score assessing this test suite may become worse during the development of a system, because of equivalent mutants introduced during mutant creation. This is a fundamental problem. Using static analysis and symbolic execution, we show how to establish non-equivalence or "don't know" among mutants. If the number of don't knows is small, this is a good indicator that a computed mutation score actually reflects its above definition. We can therefore have an increased confidence that mutation score trends correspond to actual improvements of a test suite's quality, and are not overly polluted by equivalent mutants. Using a set of 14 representative unit size programs, we show that for some, but not all, of these programs, the above confidence can indeed be established. We also evaluate the reproducibility, efficiency and effectiveness of our Nequivack tool. Our findings are that reproducibility is completely given. A single mutant analysis can be performed within 3 seconds on average, which is efficient for practical and industrial applications. © 2016 IEEE.},
author_keywords={equivalent mutant;  mutant score confidence;  mutation score;  non-equivalence checking},
keywords={Static analysis;  Verification, Equivalence checking;  Equivalent Mutants;  mutant score confidence;  Mutation score;  Reproducibilities;  Single mutant;  Symbolic execution, Software testing},
}

@CONFERENCE{Zhang2016342,
author={Zhang, J. and Wang, Z. and Zhang, L. and Hao, D. and Zang, L. and Cheng, S. and Zhang, L.},
title={Predictive mutation testing},
journal={ISSTA 2016 - Proceedings of the 25th International Symposium on Software Testing and Analysis},
year={2016},
pages={342-353},
doi={10.1145/2931037.2931038},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984865403&doi=10.1145%2f2931037.2931038&partnerID=40&md5=b1a090f70fa5481b41dd0c8289130599},
abstract={Mutation testing is a powerful methodology for evaluating test suite quality. In mutation testing, a large number of mutants are generated and executed against the test suite to check the ratio of killed mutants. Therefore, mutation testing is widely believed to be a computationally expensive technique. To alleviate the efficiency concern of mutation testing, in this paper, we propose predictive mutation testing (PMT), the first approach to predicting mutation testing results without mutant execution. In particular, the proposed approach constructs a classification model based on a series of features related to mutants and tests, and uses the classification model to predict whether a mutant is killed or survived without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (i.e., cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss when predicting mutant execution results, indicating a good tradeoff between efficiency and effectiveness of mutation testing. © 2016 ACM.},
author_keywords={Machine learning;  Mutation testing;  Software testing},
keywords={Artificial intelligence;  Efficiency;  Forecasting;  Learning systems, Accuracy loss;  Application scenario;  Classification models;  Mutation testing;  Real world projects, Software testing},
}

@CONFERENCE{Felbinger201643,
author={Felbinger, H. and Wotawa, F. and Nica, M.},
title={Empirical study of correlation between mutation score and model inference based test suite adequacy assessment},
journal={Proceedings - 11th International Workshop on Automation of Software Test, AST 2016},
year={2016},
pages={43-49},
doi={10.1145/2896921.2896923},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974652937&doi=10.1145%2f2896921.2896923&partnerID=40&md5=2e13e72dfaa17188d2617877d087c312},
abstract={In this paper we investigate a method for test suite evaluation that is based on an inferred model from the test suite. The idea is to use the similarity between the inferred model and the system under test as a measure of test suite adequacy, which is the ability of a test suite to expose errors in the system under test. We define similarity using the root mean squared error computed from the differences of the system under test output and the model output for certain inputs not used for model inference. In the paper we introduce the approach and provide results of an experimental evaluation where we compare the similarity with the mutation score. We used the Pearson Correlation coefficient to calculate whether a linear correlation between mutation score and root mean squared error exists. As a result we obtain that in certain cases the computed similarity strongly correlates with the mutation score. © 2016 ACM.},
author_keywords={Machine learning;  Mutation score;  Software test},
keywords={Artificial intelligence;  Correlation methods;  Errors;  Learning algorithms;  Learning systems;  Mean square error, Empirical studies;  Experimental evaluation;  Linear correlation;  Model inference;  Mutation score;  Pearson correlation coefficients;  Root mean squared errors;  System under test, Software testing},
}

@ARTICLE{Falah2015489,
author={Falah, B. and Akour, M. and Marchoum, N.E.},
title={Testing patterns in action: Designing a test-pattern-based suite},
journal={International Review on Computers and Software},
year={2015},
volume={10},
number={5},
pages={489-494},
doi={10.15866/irecos.v10i5.5963},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938498296&doi=10.15866%2firecos.v10i5.5963&partnerID=40&md5=077e9f4fde9e21a4db49d19865f63ac6},
abstract={Design patterns constitute a revolution in the field of software engineering, as they emphasize the importance of reuse and its impact on the software process and the software product quality. A special type of design patterns is testing patters; these can be used in the testing phase to reduce redundancy, save time and resources and provide an effective reuse mechanism for more coverage and better quality of service at the same time. Many design patterns exist to test different aspects of the implemented functionality separately. However, in this paper, we will suggest a new concept, which consists of incorporating different testing patterns into the same test suite to test different aspects through running one single test exactly once. It will also allow the users to track the performance of their test suite quality attributes using a simple representation. © 2015, Praise Worthy Prize S.r.l. - All rights reserved.},
author_keywords={Design patterns;  Quality metrics;  Software testing;  Test patterns;  Testing quality},
}

@ARTICLE{Fawaz2015171,
author={Fawaz, K. and Zaraket, F. and Masri, W. and Harkous, H.},
title={PBCOV: a property-based coverage criterion},
journal={Software Quality Journal},
year={2015},
volume={23},
number={1},
pages={171-202},
doi={10.1007/s11219-014-9237-3},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924222369&doi=10.1007%2fs11219-014-9237-3&partnerID=40&md5=db439c6545c725201184a9ba7af9d6d4},
abstract={Coverage criteria aim at satisfying test requirements and compute metrics values that quantify the adequacy of test suites at revealing defects in programs. Typically, a test requirement is a structural program element, and the coverage metric value represents the percentage of elements covered by a test suite. Empirical studies show that existing criteria might characterize a test suite as highly adequate, while it does not actually reveal some of the existing defects. In other words, existing structural coverage criteria are not always sensitive to the presence of defects. This paper presents PBCOV, a Property-Based COVerage criterion, and empirically demonstrates its effectiveness. Given a program with properties therein, static analysis techniques, such as model checking, leverage formal properties to find defects. PBCOV is a dynamic analysis technique that also leverages properties and is characterized by the following: (a) It considers the state space of first-order logic properties as the test requirements to be covered; (b) it uses logic synthesis to compute the state space; and (c) it is practical, i.e., computable, because it considers an over-approximation of the reachable state space using a cut-based abstraction.We evaluated PBCOV using programs with test suites comprising passing and failing test cases. First, we computed metrics values for PBCOV and structural coverage using the full test suites. Second, in order to quantify the sensitivity of the metrics to the absence of failing test cases, we computed the values for all considered metrics using only the passing test cases. In most cases, the structural metrics exhibited little or no decrease in their values, while PBCOV showed a considerable decrease. This suggests that PBCOV is more sensitive to the absence of failing test cases, i.e., it is more effective at characterizing test suite adequacy to detect defects, and at revealing deficiencies in test suites. © 2014, Springer Science+Business Media New York.},
author_keywords={Coverage criteria;  Logic synthesis;  Property-based coverage;  Reachability analysis;  Software testing;  Specification-based coverage;  State space coverage;  Test suite evaluation},
keywords={C (programming language);  Computer circuits;  Defects;  Logic Synthesis;  Model checking;  State space methods;  Static analysis;  Structural analysis;  Structural properties;  Testing, Analysis techniques;  Coverage criteria;  Dynamic analysis techniques;  First order logic;  Property-based;  Reachability analysis;  Structural metrics;  Structural programs, Software testing},
}

@CONFERENCE{Horváth201546,
author={Horváth, F. and Vancsics, B. and Vidács, L. and Beszédes, Á. and Tengeri, D. and Gergely, T. and Gyimóthy, T.},
title={Test suite evaluation using code coverage based metrics},
journal={CEUR Workshop Proceedings},
year={2015},
volume={1525},
pages={46-60},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962535014&partnerID=40&md5=0afbed6c96650e33de141304e883885a},
abstract={Regression test suites of evolving software systems are often crucial to maintaining software quality in the long term. They have to be effective in terms of detecting faults and helping their localization. However, to gain knowledge of such capabilities of test suites is usually difficult. We propose a method for deeper understanding of a test suite and its relation to the program code it is intended to test. The basic idea is to decompose the test suite and the program code into coherent logical groups which are easier to analyze and understand. Coverage and partition metrics are then extracted directly from code coverage information to characterize a test suite and its constituents. We also use heat-map tables for test suite assessment both at the system level and at the level of logical groups. We employ these metrics to analyze and evaluate the regression test suite of the WebKit system, an industrial size browser engine with an extensive set of 27,000 tests.},
author_keywords={Code coverage;  Regression testing;  Test metrics;  Test suite evaluation},
keywords={Codes (symbols);  Computational linguistics;  Computer software;  Computer software selection and evaluation;  Regression analysis;  Testing, Code coverage;  Industrial size;  Regression testing;  Regression tests;  Software Quality;  Software systems;  System levels;  Test metrics, Software testing},
}

@CONFERENCE{Tengeri2015,
author={Tengeri, D. and Beszédes, A. and Gergely, T. and Vidács, L. and Hávas, D. and Gyimóthy, T.},
title={Beyond code coverage - An approach for test suite assessment and improvement},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2015 - Proceedings},
year={2015},
doi={10.1109/ICSTW.2015.7107476},
art_number={7107476},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934301413&doi=10.1109%2fICSTW.2015.7107476&partnerID=40&md5=edf4f61994fcb6c29d60ef2f3d37a202},
abstract={Code coverage is successfully used to guide white box test design and evaluate the respective test completeness. However, simple overall coverage ratios are often not precise enough to effectively help when a (regression) test suite needs to be reassessed and evolved after software change. We present an approach for test suite assessment and improvement that utilizes code coverage information, but on a more detailed level and adds further evaluation aspects derived from the coverage. The main use of the method is to aid various test suite evolution situations such as removal, refactoring and extension of test cases as a result of code change or test suite efficiency enhancement. We define various metrics to express different properties of test suites beyond simple code coverage ratios, and present the assessment and improvement process as an iterative application of different improvement goals and more specific sub-activities. The method is demonstrated by applying it to improve the tests of one of our experimental systems. © 2015 IEEE.},
author_keywords={code coverage;  regression testing;  test suite evolution;  test suite quality;  test suite refactoring;  white box testing metrics},
}

@CONFERENCE{Felbinger2015,
author={Felbinger, H.},
title={Test suite quality assessment using model inference techniques},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015 - Proceedings},
year={2015},
doi={10.1109/ICST.2015.7102617},
art_number={7102617},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935093475&doi=10.1109%2fICST.2015.7102617&partnerID=40&md5=6aae7422e3dc3fd266a7744612bbebd2},
abstract={To state whether a System Under Test is sufficiently tested requires an assessment of the test suite quality. Existing methods to assess the quality of a test suite either are based on the structure of an implementation or determine the quality using mutation score. In this paper we introduce a method, which is based on inductive inference to assess the quality of a test suite and propose a method to augment a test suite depending on the quality assessment result. In this paper we provide a short glimpse on our objectives and show preliminary results of model inference of a test suite. © 2015 IEEE.},
keywords={Testing;  Verification, Inductive inference;  Model inference;  Mutation score;  Quality assessment;  System under test, Software testing},
}

@CONFERENCE{Inozemtseva2014435,
author={Inozemtseva, L. and Holmes, R.},
title={Coverage is not strongly correlated with test suite effectiveness},
journal={Proceedings - International Conference on Software Engineering},
year={2014},
number={1},
pages={435-445},
doi={10.1145/2568225.2568271},
note={cited By 219},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994086611&doi=10.1145%2f2568225.2568271&partnerID=40&md5=8124f10f05fc8239822e7289468833fe},
abstract={The coverage of a test suite is often used as a proxy for its ability to detect faults. However, previous studies that investigated the correlation between code coverage and test suite effectiveness have failed to reach a consensus about the nature and strength of the relationship between these test suite characteristics. Moreover, many of the studies were done with small or synthetic programs, making it unclear whether their results generalize to larger programs, and some of the studies did not account for the confounding influence of test suite size. In addition, most of the studies were done with adequate suites, which are are rare in practice, so the results may not generalize to typical test suites. We have extended these studies by evaluating the relationship between test suite size, coverage, and effectiveness for large Java programs. Our study is the largest to date in the literature: we generated 31,000 test suites for five systems consisting of up to 724,000 lines of source code. We measured the statement coverage, decision coverage, and modified condition coverage of these suites and used mutation testing to evaluate their fault detection effectiveness. We found that there is a low to moderate correlation between coverage and effectiveness when the number of test cases in the suite is controlled for. In addition, we found that stronger forms of coverage do not provide greater insight into the effectiveness of the suite. Our results suggest that coverage, while useful for identifying under-tested parts of a program, should not be used as a quality target because it is not a good indicator of test suite effectiveness. © 2014 ACM.},
author_keywords={Coverage;  test suite effectiveness;  test suite quality},
keywords={Computer software;  Fault detection;  Java programming language;  Software engineering, Code coverage;  Coverage;  Decision coverage;  Fault detection effectiveness;  Modified conditions;  Mutation testing;  Quality targets;  Statement coverage, Software testing},
}

@CONFERENCE{Kracht2014256,
author={Kracht, J.S. and Petrovic, J.Z. and Walcott-Justice, K.R.},
title={Empirically evaluating the quality of automatically generated and manually written test suites},
journal={Proceedings - International Conference on Quality Software},
year={2014},
pages={256-265},
doi={10.1109/QSIC.2014.33},
art_number={6958413},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912105788&doi=10.1109%2fQSIC.2014.33&partnerID=40&md5=9a52c359f5fb952b3a1f5a192022d161},
abstract={The creation, execution, and maintenance of tests are some of the most expensive tasks in software development. To help reduce the cost, automated test generation tools can be used to assist and guide developers in creating test cases. Yet, the tests that automated tools produce range from simple skeletons to fully executable test suites, hence their complexity and quality vary. This paper compares the complexity and quality of test suites created by sophisticated automated test generation tools to that of developer-written test suites. The empirical study in this paper examines ten real-world programs with existing test suites and applies two state-of-The-Art automated test generation tools. The study measures the resulting test suite quality in terms of code coverage and fault-finding capability. On average, manual tests covered 31.5% of the branches while the automated tools covered 31.8% of the branches. In terms of mutation score, the tests generated by automated tools had an average mutation score of 39.8% compared to the average mutation score of 42.1% for manually written tests. Even though automatically created tests often contain more lines of source code than those written by developers, this paper's empirical results reveal that test generation tools can provide value by creating high quality test suites while reducing the cost and effort needed for testing. © 2014 IEEE.},
keywords={Automation;  Cost reduction;  Software design;  Software testing;  Testing, Automated test generations;  Automated tools;  Automatically generated;  Empirical studies;  High Quality Test;  Mutation score;  Real world projects;  Test generations, Automatic test pattern generation},
}

@ARTICLE{Schuler2013531,
author={Schuler, D. and Zeller, A.},
title={Checked coverage: An indicator for oracle quality},
journal={Software Testing Verification and Reliability},
year={2013},
volume={23},
number={7},
pages={531-551},
doi={10.1002/stvr.1497},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885947629&doi=10.1002%2fstvr.1497&partnerID=40&md5=791a61c243449cccbe12f39c7768b843},
abstract={known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality and even more sensitive than mutation testing. Copyright © 2013 John Wiley & Sons, Ltd.},
author_keywords={coverage metrics;  dynamic slicing;  mutation testing;  test suite quality},
keywords={Coverage metrics;  Dynamic slicing;  Mutation testing;  Open source projects, Media streaming;  Software engineering, Software testing},
}

@CONFERENCE{Inozemtseva2013639,
author={Inozemtseva, L. and Hemmati, H. and Holmes, R.},
title={Using fault history to improve mutation reduction},
journal={2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings},
year={2013},
pages={639-642},
doi={10.1145/2491411.2494586},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883722857&doi=10.1145%2f2491411.2494586&partnerID=40&md5=4445c5597ca0ac7d8c98983944c2f41b},
abstract={Mutation testing can be used to measure test suite quality in two ways: by treating the kill score as a quality metric, or by treating each surviving, non-equivalent mutant as an indicator of an inadequacy in the test suite. The first technique relies on the assumption that the mutation score is highly correlated with the suite's real fault detection rate, which is not well supported by the literature. The second technique relies only on the weaker assumption that the "interesting" mutants (i.e., the ones that indicate an inadequacy in the suite) are in the set of surviving mutants. Using the second technique also makes improving the suite straightforward. Unfortunately, mutation testing has a performance problem. At least part of the test suite must be run on every mutant, meaning mutation testing can be too slow for practical use. Previous work has addressed this by reducing the number of mutants to evaluate in various ways, including selecting a random subset of them. However, reducing the set of mutants by random reduction is suboptimal for developers using the second technique described above, since random reduction will eliminate many of the interesting mutants. We propose a new reduction method that supports the use of the second technique by reducing the set of mutants to those generated by altering files that have contained many faults in the past. We performed a pilot study that suggests that this reduction method preferentially chooses mutants that will survive mutation testing; that is, it preserves a greater number of interesting mutants than random reduction does.},
author_keywords={Fault history;  Mutant reduction;  Mutation testing;  Test suite quality},
keywords={Fault detection rate;  Highly-correlated;  Mutation score;  Mutation testing;  Performance problems;  Quality metrices;  Random subsets;  Reduction method, Fault detection;  Software testing, Software engineering},
}

@CONFERENCE{Zhi2013382,
author={Zhi, J. and Garousi, V.},
title={On adequacy of assertions in automated test suites: An empirical investigation},
journal={Proceedings - IEEE 6th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2013},
year={2013},
pages={382-391},
doi={10.1109/ICSTW.2013.49},
art_number={6571656},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883330428&doi=10.1109%2fICSTW.2013.49&partnerID=40&md5=cbd09eac0a1c9ef41b8ab2ca883dc411},
abstract={An integral part of test case is the verification phase (also called 'test oracle'), which verifies program's state, output or behavior. In automated testing, the verification phase is often implemented using test assertions which are usually developed manually by testers. More precisely, assertions are used for checking the unit or the system's behavior (or output) which is reflected by the changes in the data fields of the class under test, or the output of the function under test. Originated from human (testers') error, test suites are prone to having inadequate assertions. The paper reports an empirical study on the Inadequate-Assertion (IA) problem in the context of automated test suites developed for open-source projects. In this study, test suites of three active open-source projects have been chosen. To investigate IA problem occurrence among the sampled test suites, we performed mutation analysis and coverage analysis. The results indicate that: (1) the IA problem is common among the sampled open-source projects, and the occurrence varies from project to project and from package to package, and (2) the occurrence rate of the IA problem is positively co-related with the complexity of test code. © 2013 IEEE.},
author_keywords={adequacy of test oracles;  automated testing;  empirical case-study;  mutation testing;  quality of test oracles;  state coverage;  Test assertions;  test-suite quality},
keywords={Automated testing;  empirical case-study;  Mutation testing;  State coverage;  Test assertions;  Test oracles, Testing, Software testing},
}

@ARTICLE{RealesMateo2013570,
author={Reales Mateo, P. and Polo Usaola, M. and Fernández Alemán, J.L.},
title={Validating second-order mutation at system level},
journal={IEEE Transactions on Software Engineering},
year={2013},
volume={39},
number={4},
pages={570-587},
doi={10.1109/TSE.2012.39},
art_number={6216382},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875720693&doi=10.1109%2fTSE.2012.39&partnerID=40&md5=5c4441f6cefbaa3689a83c6e2e63d765},
abstract={Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like (n)-order mutation where each mutant contains (n) artificial faults instead of one. There are two approaches to (n)-order mutation: increasing the effectiveness of mutation by searching for good (n)-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into (n)-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of (n)-order mutation at system level. © 1976-2012 IEEE.},
author_keywords={Empirical evaluation;  high-order mutation;  mutation testing},
keywords={Combination strategies;  Empirical evaluations;  Empirical studies;  High-order;  Mutation operators;  Mutation testing;  Program statements;  Software testing techniques, Cost reduction;  Risk perception;  Software testing;  Testing, Cost benefit analysis},
}

@CONFERENCE{Dulz201136,
author={Dulz, W.},
title={A comfortable TestPlayer for analyzing statistical usage testing strategies},
journal={Proceedings - International Conference on Software Engineering},
year={2011},
pages={36-42},
doi={10.1145/1982595.1982604},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959443087&doi=10.1145%2f1982595.1982604&partnerID=40&md5=6f14247b755c8a92a95da75662293727},
abstract={We will first give a brief introduction to a versatile modeling environment that allows the early validation of system specifications by a test-driven agile simulation approach. The main focus of our paper is on providing techniques for automated test case generation relying on statistical usage models. Based on the open source software R for statistical computing and graphics the easy to handle test case generation and analyzing TestPlayer© tool was developped. Starting from customer-specific graphical Markov chain usage models test cases are automatically generated and visualized by highlighting selected nodes and arcs in the model. In addition, various metrics and corresponding diagrams offer analytical techniques to assess the quality of the derived test suite. © 2011 ACM.},
author_keywords={graphical test suite evaluation;  markov chain usage model;  model-driven test case generation;  testplayer tool},
keywords={Automated test case generation;  Graphical test;  Markov Chain;  Model-driven;  Modeling environments;  Open Source Software;  Simulation approach;  Statistical computing;  Statistical usage testing;  System specification;  Test case generation;  testplayer tool;  Usage models, Computer software selection and evaluation;  Health care;  Markov processes;  Open systems;  Software engineering;  Specifications;  Testing, Software testing},
}

@CONFERENCE{Schuler201190,
author={Schuler, D. and Zeller, A.},
title={Assessing oracle quality with checked coverage},
journal={Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011},
year={2011},
pages={90-99},
doi={10.1109/ICST.2011.32},
art_number={5770598},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958743339&doi=10.1109%2fICST.2011.32&partnerID=40&md5=a3e9f7781fffc1efe9a94d69fe4a1d39},
abstract={A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative. © 2011 IEEE.},
author_keywords={coverage metrics;  dynamic slicing;  mutation testing;  test suite quality},
keywords={Coverage metrics;  dynamic slicing;  Mutation testing;  Open source projects;  test suite quality, Computer software selection and evaluation;  Verification, Software testing},
}

@ARTICLE{Popovic2010697,
author={Popovic, M. and Basicevic, I.},
title={Test case generation for the task tree type of architecture},
journal={Information and Software Technology},
year={2010},
volume={52},
number={6},
pages={697-706},
doi={10.1016/j.infsof.2010.03.001},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950458077&doi=10.1016%2fj.infsof.2010.03.001&partnerID=40&md5=43e01c9367f30a797b20115200a96cd4},
abstract={Context: Emerging multicores and clusters of multicores that may operate in parallel have set a new challenge - development of massively parallel software composed of thousands of loosely coupled or even completely independent threads/processes, such as MapReduce and Java 3.0 workers, or Erlang processes, respectively. Testing and verification is a critical phase in the development of such software products. Objective: Generating test cases based on operational profiles and certifying declared operational reliability figure of the given software product is a well-established process for the sequential type of software. This paper proposes an adaptation of that process for a class of massively parallel software - large-scale task trees. Method: The proposed method uses statistical usage testing and operational reliability estimation based on operational profiles and novel test suite quality indicators, namely the percentage of different task trees and the percentage of different paths. Results: As an example, the proposed method is applied to operational reliability certification of a parallel software infrastructure named the TaskTreeExecutor. The paper proposes an algorithm for generating random task trees to enable that application. Test runs in the experiments involved hundreds and thousands of Win32/Linux threads thus demonstrating scalability of the proposed approach. For practitioners, the most useful result presented is the method for determining the number of task trees and the number of paths, which are needed to certify the given operational reliability of a software product. The practitioners may also use the proposed coverage metrics to measure the quality of automatically generated test suite. Conclusion: This paper provides a useful solution for the test case generation that enables the operational reliability certification process for a class of massively parallel software called the large-scale task trees. The usefulness of this solution was demonstrated by a case study - operational reliability certification of the real parallel software product. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Massively parallel software;  Operational reliability;  Statistical usage testing;  Test case generation},
keywords={Automatically generated;  Certification process;  Coverage metrics;  Erlang process;  Operational profile;  Operational reliability;  Parallel software;  Quality indicators;  Software products;  Statistical usage testing;  Task tree;  Test case;  Test case generation;  Test runs, Automatic test pattern generation;  Computer software selection and evaluation;  Quality assurance;  Testing;  Trees (mathematics);  Verification, Software reliability},
}

@ARTICLE{Fraser20091403,
author={Fraser, G. and Wotawa, F. and Ammann, P.},
title={Issues in using model checkers for test case generation},
journal={Journal of Systems and Software},
year={2009},
volume={82},
number={9},
pages={1403-1418},
doi={10.1016/j.jss.2009.05.016},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-68949100459&doi=10.1016%2fj.jss.2009.05.016&partnerID=40&md5=10cb02a3f89ebe55aa1497253d1fb57b},
abstract={The use of model checkers for automated software testing has received some attention in the literature: It is convenient because it allows fully automated generation of test suites for many different test objectives. On the other hand, model checkers were not originally meant to be used this way but for formal verification, so using model checkers for testing is sometimes perceived as a "hack". Indeed, several drawbacks result from the use of model checkers for test case generation. If model checkers were designed or adapted to take into account the needs that result from the application to software testing, this could lead to significant improvements with regard to test suite quality and performance. In this paper we identify the drawbacks of current model checkers when used for testing. We illustrate techniques to overcome these problems, and show how they could be integrated into the model checking process. In essence, the described techniques can be seen as a general road map to turn model checkers into general purpose testing tools. © 2009 Elsevier Inc. All rights reserved.},
author_keywords={Automated software testing;  Automated test case generation;  Minimization;  Performance;  Testing with model checkers},
keywords={Automated generation;  Automated software testing;  Automated test case generation;  Current models;  Formal verifications;  General purpose;  Minimization;  Model checker;  Performance;  Road-maps;  Test case generation;  Testing tools;  Turn model, Automation;  Computer software selection and evaluation;  Software testing;  Testing, Model checking},
}
