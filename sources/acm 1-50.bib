@inproceedings{10.1145/3408877.3432417,
author = {Cordova, Lucas and Carver, Jeffrey and Gershmel, Noah and Walia, Gursimran},
title = {A Comparison of Inquiry-Based Conceptual Feedback vs. Traditional Detailed Feedback Mechanisms in Software Testing Education: An Empirical Investigation},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432417},
doi = {10.1145/3408877.3432417},
abstract = {The feedback provided by current testing education tools about the deficiencies in a student's test suite either mimics industry code coverage tools or lists specific instructor test cases that are missing from the student's test suite. While useful in some sense, these types of feedback are akin to revealing the solution to the problem, which can inadvertently encourage students to pursue a trial-and-error approach to testing, rather than using a more systematic approach that encourages learning. In addition to not teaching students why their test suite is inadequate, this type of feedback may motivate students to become dependent on the feedback rather than thinking for themselves. To address this deficiency, there is an opportunity to investigate alternative feedback mechanisms that include a positive reinforcement of testing concepts. We argue that using an inquiry-based learning approach is better than simply providing the answers. To facilitate this type of learning, we present Testing Tutor, a web-based assignment submission platform that supports different levels of testing pedagogy via a customizable feedback engine. We evaluated the impact of the different types of feedback through an empirical study in two sophomore-level courses. We use Testing Tutor to provide students with different types of feedback, either traditional detailed code coverage feedback or inquiry-based learning conceptual feedback, and compare the effects. The results show that students that receive conceptual feedback had higher code coverage (by different measures), fewer redundant test cases, and higher programming grades than the students who receive traditional code coverage feedback.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {87–93},
numpages = {7},
keywords = {education, testing, pedagogy, tools},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/3408877.3432411,
author = {Clegg, Benjamin Simon and McMinn, Phil and Fraser, Gordon},
title = {An Empirical Study to Determine If Mutants Can Effectively Simulate Students' Programming Mistakes to Increase Tutors' Confidence in Autograding},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432411},
doi = {10.1145/3408877.3432411},
abstract = {Automated grading often requires automated test suites to identify students' faults. However, tests may not detect some faults, limiting feedback, and providing inaccurate grades. This issue can be mitigated by first ensuring that tests can detect faults. Mutation analysis is a technique that generates artificial faulty variants of a program for this purpose, called mutants. Mutants that are not detected by tests reveal their inadequacies, providing knowledge on how they can be improved. By using mutants to improve test suites, tutors can gain the confidence that: a) generated grades will not be biased by unidentified faults, and b) students will receive appropriate feedback for their mistakes. Existing work has shown that mutants are suitable substitutes for faults in real world software, but no work has shown that this holds for students' faults. In this paper, we investigate whether mutants are capable of replicating mistakes made by students. We conducted a quantitative study on 197 Java classes written by students across three introductory programming assignments, and mutants generated from the assignments' model solutions. We found that generated mutants capture the observed faulty behaviour of students' solutions. We also found that mutants better assess test adequacy than code coverage in some cases. Our results indicate that tutors can use mutants to identify and remedy deficiencies in grading test suites.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1055–1061},
numpages = {7},
keywords = {mutation analysis, autograding, introductory programming},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/3368089.3409761,
author = {Wang, Zan and Yan, Ming and Chen, Junjie and Liu, Shuang and Zhang, Dongdi},
title = {Deep Learning Library Testing via Effective Model Generation},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409761},
doi = {10.1145/3368089.3409761},
abstract = {Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice. However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains. Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems. In this work, we propose a novel approach, LEMON, to testing DL libraries. In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries. We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet. The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers. Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {788–799},
numpages = {12},
keywords = {Library Testing, Deep Learning Testing, Search-based Software Testing, Mutation, Model Generation},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3368089.3409713,
author = {Wong, Chu-Pan and Meinicke, Jens and Chen, Leo and Diniz, Jo\~{a}o P. and K\"{a}stner, Christian and Figueiredo, Eduardo},
title = {Efficiently Finding Higher-Order Mutants},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409713},
doi = {10.1145/3368089.3409713},
abstract = {Higher-order mutation has the potential for improving major drawbacks of traditional first-order mutation, such as by simulating more realistic faults or improving test-optimization techniques. Despite interest in studying promising higher-order mutants, such mutants are difficult to find due to the exponential search space of mutation combinations. State-of-the-art approaches rely on genetic search, which is often incomplete and expensive due to its stochastic nature. First, we propose a novel way of finding a complete set of higher-order mutants by using variational execution, a technique that can, in many cases, explore large search spaces completely and often efficiently. Second, we use the identified complete set of higher-order mutants to study their characteristics. Finally, we use the identified characteristics to design and evaluate a new search strategy, independent of variational execution, that is highly effective at finding higher-order mutants even in large codebases.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1165–1177},
numpages = {13},
keywords = {higher-order mutant, mutation analysis, variational execution},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3368089.3409754,
author = {Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung},
title = {Is Neuron Coverage a Meaningful Measure for Testing Deep Neural Networks?},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409754},
doi = {10.1145/3368089.3409754},
abstract = {Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {851–862},
numpages = {12},
keywords = {Neuron Coverage, Software Engineering, Machine Learning, Adversarial Attack, Testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3422392.3422499,
author = {Virg\'{\i}nio, T\'{a}ssio and Martins, Luana and Rocha, Larissa and Santana, Railana and Cruz, Adriana and Costa, Heitor and Machado, Ivan},
title = {JNose: Java Test Smell Detector},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422499},
doi = {10.1145/3422392.3422499},
abstract = {Several strategies have been proposed for test quality measurement and analysis. Code coverage is likely the most widely used one. It enables to verify the ability of a test case to cover as many source code branches as possible. Although code coverage has been widely used, novel strategies have been recently employed. It is the case of test smells analysis, which has been introduced as an affordable strategy to evaluate the quality of test code. Test smells are poor design choices in implementation, and their occurrence in test code might reduce the quality of test suites. Test smells identification is clearly dependent on tool support, otherwise it could become a cost-ineffective strategy. However, as far as we know, there is no tool that combines code coverage and test smells to address test quality measurement. In this work, we present the JNose Test, a tool aimed to analyze test suite quality in the perspective of test smells. JNose Test detects code coverage and software evolution metrics and a set of test smells throughout software versions.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {564–569},
numpages = {6},
keywords = {Test Smells, Test Suite Evolution, Quality of Tests, Code Coverage},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3422392.3422407,
author = {Souza, Beatriz and Machado, Patr\'{\i}cia},
title = {A Large Scale Study On the Effectiveness of Manual and Automatic Unit Test Generation},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422407},
doi = {10.1145/3422392.3422407},
abstract = {Recently, an increasingly large amount of effort has been devoted to implementing tools to generate unit test suites automatically. Previous studies have investigated the effectiveness of these tools by comparing automatically generated test suites (ATSs) to manually written test suites (MTSs). Most of these studies report that ATSs can achieve higher code coverage, or even mutation coverage, than MTSs, particularly when suites are generated from defective code. However, these studies usually consider a limited amount of classes or subject programs, while the adoption of such tools in the industry is still low. This work aims to compare the effectiveness of ATSs and MTSs when applied as regression test suites. We conduct an empirical study, using ten programs (1368 classes), written in Java, that already have MTSs and apply two sophisticated tools that automatically generate test cases: Randoop and EvoSuite. To evaluate the test suites' effectiveness, we use line and mutation coverage. Our results indicate that MTSs are, in general, more effective than ATSs regarding the investigated metrics. Moreover, the number of generated test cases may not indicate test suites' effectiveness. Furthermore, there are situations when ATSs are more effective, and even when ATSs and MTSs can be complementary.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {253–262},
numpages = {10},
keywords = {mutation testing, automatic test generation, empirical studies},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1109/ASE.2019.00019,
author = {Godio, Ariel and Bengolea, Valeria and Ponzio, Pablo and Aguirre, Nazareno and Frias, Marcelo F.},
title = {Efficient Test Generation Guided by Field Coverage Criteria},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00019},
doi = {10.1109/ASE.2019.00019},
abstract = {Field-exhaustive testing is a testing criterion suitable for object-oriented code over complex, heap-allocated, data structures. It requires test suites to contain enough test inputs to cover all feasible values for the object's fields within a certain scope (input-size bound). While previous work shows that field-exhaustive suites can be automatically generated, the generation technique required a formal specification of the inputs that can be subject to SAT-based analysis. Moreover, the restriction of producing all feasible values for inputs' fields makes test generation costly.In this paper, we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score, by examining to what extent the values of inputs' fields are covered. In particular, we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field-exhaustive suites, using the Symbolic Pathfinder tool. To underapproximate these suites we use tranScoping, a technique that estimates characteristics of yet to be run analyses for large scopes, based on data obtained from analyses performed in small scopes. This provides us with a suitable condition to prematurely stop the symbolic execution.As we show, tranScoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time. All this while retaining the effectiveness of field exhaustive suites in terms of test suite quality.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {91–101},
numpages = {11},
keywords = {field-exhaustive testing, symbolic execution, field-based testing, transcoping},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/ASE.2019.00147,
author = {Soto, Mauricio},
title = {Improving Patch Quality by Enhancing Key Components of Automatic Program Repair},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00147},
doi = {10.1109/ASE.2019.00147},
abstract = {The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1230–1233},
numpages = {4},
keywords = {patch quality, Automatic Program Repair},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/ASE.2019.00109,
author = {Escobar-Velasquez, Camilo and Osorio-Ria\~{n}o, Michael and Linares-V\'{a}squez, Mario},
title = {MutAPK: Source-Codeless Mutant Generation for Android Apps},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00109},
doi = {10.1109/ASE.2019.00109},
abstract = {The amount of Android application is having a tremendous increasing trend, exerting pressure over practitioners and researchers around application quality, frequent releases, and quick fixing of bugs. This pressure leads practitioners to make usage of automated approaches based on using source-code as input. Nevertheless, third-party services are not able to use these approaches due to privacy factors. In this paper we present MutAPK, an open source mutation testing tool that enables the usage of Android Application Packages (APKs) as input for this task. MutAPK generates mutants without the need of having access to source code, because the mutations are done in an intermediate representation of the code (i.e., SMALI) that does not require compilation. MutAPK is publicly available at GitHub: https://bit.ly/2KYvgP9 VIDEO: https://bit.ly/2WOjiyy},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1090–1093},
numpages = {4},
keywords = {closed-source apps, Android, mutation testing},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3331447,
author = {Bertolino, Antonia and Angelis, Guglielmo De and Gallego, Micael and Garc\'{\i}a, Boni and Gort\'{a}zar, Francisco and Lonetti, Francesca and Marchetti, Eda},
title = {A Systematic Review on Cloud Testing},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3331447},
doi = {10.1145/3331447},
abstract = {A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012--2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an active research field, although not all topics have received enough attention and conclude by presenting the most relevant open research challenges for each area of the classification framework.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {93},
numpages = {42},
keywords = {systematic literature review, testing, Cloud computing}
}

@inproceedings{10.1145/3338906.3338957,
author = {Chen, Junjie and Han, Jiaqi and Sun, Peiyi and Zhang, Lingming and Hao, Dan and Zhang, Lu},
title = {Compiler Bug Isolation via Effective Witness Test Program Generation},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338957},
doi = {10.1145/3338906.3338957},
abstract = {Compiler bugs are extremely harmful, but are notoriously difficult to debug because compiler bugs usually produce few debugging information. Given a bug-triggering test program for a compiler, hundreds of compiler files are usually involved during compilation, and thus are suspect buggy files. Although there are lots of automated bug isolation techniques, they are not applicable to compilers due to the scalability or effectiveness problem. To solve this problem, in this paper, we transform the compiler bug isolation problem into a search problem, i.e., searching for a set of effective witness test programs that are able to eliminate innocent compiler files from suspects. Based on this intuition, we propose an automated compiler bug isolation technique, DiWi, which (1) proposes a heuristic-based search strategy to generate such a set of effective witness test programs via applying our designed witnessing mutation rules to the given failing test program, and (2) compares their coverage to isolate bugs following the practice of spectrum-based bug isolation. The experimental results on 90 real bugs from popular GCC and LLVM compilers show that DiWi effectively isolates 66.67%/78.89% bugs within Top-10/Top-20 compiler files, significantly outperforming state-of-the-art bug isolation techniques.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {223–234},
numpages = {12},
keywords = {Compiler Debugging, Bug Isolation, Test Program Generation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3293882.3330559,
author = {Ghanbari, Ali and Benton, Samuel and Zhang, Lingming},
title = {Practical Program Repair via Bytecode Mutation},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330559},
doi = {10.1145/3293882.3330559},
abstract = {Automated Program Repair (APR) is one of the most recent advances in automated debugging, and can directly fix buggy programs with minimal human intervention. Although various advanced APR techniques (including search-based or semantic-based ones) have been proposed, they mainly work at the source-code level and it is not clear how bytecode-level APR performs in practice. Also, empirical studies of the existing techniques on bugs beyond what has been reported in the original papers are rather limited. In this paper, we implement the first practical bytecode-level APR technique, PraPR, and present the first extensive study on fixing real-world bugs (e.g., Defects4J bugs) using JVM bytecode mutation. The experimental results show that surprisingly even PraPR with only the basic traditional mutators can produce genuine fixes for 17 bugs; with simple additional commonly used APR mutators, PraPR is able to produce genuine fixes for 43 bugs, significantly outperforming state-of-the-art APR, while being over 10X faster. Furthermore, we performed an extensive study of PraPR and other recent APR tools on a large number of additional real-world bugs, and demonstrated the overfitting problem of recent advanced APR tools for the first time. Lastly, PraPR has also successfully fixed bugs for other JVM languages (e.g., for the popular Kotlin language), indicating PraPR can greatly complement existing source-code-level APR.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {19–30},
numpages = {12},
keywords = {Program repair, JVM bytecode, Fault localization, Mutation testing},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3293882.3330569,
author = {Degott, Christian and Borges Jr., Nataniel P. and Zeller, Andreas},
title = {Learning User Interface Element Interactions},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330569},
doi = {10.1145/3293882.3330569},
abstract = {When generating tests for graphical user interfaces, one central problem is to identify how individual UI elements can be interacted with—clicking, long- or right-clicking, swiping, dragging, typing, or more. We present an approach based on reinforcement learning that automatically learns which interactions can be used for which elements, and uses this information to guide test generation. We model the problem as an instance of the multi-armed bandit problem (MAB problem) from probability theory, and show how its traditional solutions work on test generation, with and without relying on previous knowledge. The resulting guidance yields higher coverage. In our evaluation, our approach shows improvements in statement coverage between 18% (when not using any previous knowledge) and 20% (when reusing previously generated models).},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {296–306},
numpages = {11},
keywords = {Android, Test generation, Multi-Armed Bandit problem, User interactions},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3319008.3319022,
author = {Ardito, Luca and Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
title = {Espresso vs. EyeAutomate: An Experiment for the Comparison of Two Generations of Android GUI Testing},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319022},
doi = {10.1145/3319008.3319022},
abstract = {Context: Different approaches exist for automated GUI testing of Android applications, each with its peculiarities, advantages, and drawbacks. The most common are either based on the structure of the GUI or use visual recognition.Goal: In this paper, we present an empirical evaluation of two different GUI testing techniques with the use for each of a representative tool: (1) Visual GUI testing, with the use of EyeAutomate, and (2) Layout-based GUI testing, with the use of Espresso.Method: We conducted an experiment with a population of 78 graduate students. The participants of the study were asked to create the same test suite for a popular, open-source Android app (Omni-Notes) with both the tools, and to answer a survey about their preference to the one or the other, and the perceived difficulties when developing the test scripts.Results: By analyzing the outcomes of the delivered test suites (in terms of number of test scripts delivered and ratio of working ones) and the answers to the survey, we found that the participants showed similar productivity with both the tools, but the test suites developed with EyeAutomate were of higher quality (in terms of correctly working test scripts). The participants expressed a slight preference towards the EyeAutomate testing tool, reflecting a general complexity of Layout-based techniques -- represented by Espresso -- and some obstacles that may make the identification of components of the GUI quite a long and laborious task.Conclusions: The evidence we collected can provide useful hints for researchers aiming at making GUI testing techniques for mobile applications more usable and effective.},
booktitle = {Proceedings of the Evaluation and Assessment on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Mobile computing, Software Testing, Empirical Software Engineering},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/3278186.3278190,
author = {Vercammen, Sten and Ghafari, Mohammad and Demeyer, Serge and Borg, Markus},
title = {Goal-Oriented Mutation Testing with Focal Methods},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278190},
doi = {10.1145/3278186.3278190},
abstract = {Mutation testing is the state-of-the-art technique for assessing the fault-detection capacity of a test suite. Unfortunately, mutation testing consumes enormous computing resources because it runs the whole test suite for each and every injected mutant. In this paper we explore fine-grained traceability links at method level (named focal methods), to reduce the execution time of mutation testing and to verify the quality of the test cases for each individual method, instead of the usually verified overall test suite quality. Validation of our approach on the open source Apache Ant project shows a speed-up of 573.5x for the mutants located in focal methods with a quality score of 80%.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {23–30},
numpages = {8},
keywords = {Software testing, Feasibility study, Mutation testing, Focal methods},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/3238147.3238183,
author = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
title = {A Large-Scale Study of Test Coverage Evolution},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238183},
doi = {10.1145/3238147.3238183},
abstract = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {53–63},
numpages = {11},
keywords = {flaky tests, code coverage, Software testing, empirical study},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3180155.3182517,
author = {Yi, Jooyong and Tan, Shin Hwei and Mechtaev, Sergey and B\"{o}hme, Marcel and Roychoudhury, Abhik},
title = {A Correlation Study between Automated Program Repair and Test-Suite Metrics},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182517},
doi = {10.1145/3180155.3182517},
abstract = {Automated program repair has attracted attention due to its potential to reduce debugging cost. Prior works show the feasibility of automated repair, and the research focus is gradually shifting towards the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used. In this paper, 1we investigate the question: "Can traditional test-suite metrics used in software testing be used for automated program repair?". We empirically investigate the effectiveness of test-suite metrics (statement / branch coverage and mutation score) in controlling the reliability of repairs (the likelihood that repairs cause regressions). We conduct the largest-scale experiments to date with real-world software, and perform the first correlation study between test-suite metrics and the reliability of generated repairs. Our results show that by increasing test-suite metrics, the reliability of repairs tend to increase. Particularly, such trend is most strongly observed in statement coverage. This implies that traditional test-suite metrics used in software testing can also be used to improve the reliability of repairs in program repair.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {24},
numpages = {1},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3183440.3183485,
author = {Groce, Alex and Holmes, Josie and Marinov, Darko and Shi, August and Zhang, Lingming},
title = {An Extensible, Regular-Expression-Based Tool for Multi-Language Mutant Generation},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183485},
doi = {10.1145/3183440.3183485},
abstract = {Mutation testing is widely used in research (even if not in practice). Mutation testing tools usually target only one programming language and rely on parsing a program to generate mutants, or operate not at the source level but on compiled bytecode. Unfortunately, developing a robust mutation testing tool for a new language in this paradigm is a difficult and time-consuming undertaking. Moreover, bytecode/intermediate language mutants are difficult for programmers to read and understand. This paper presents a simple tool, called universalmutator, based on regular-expression-defined transformations of source code. The primary drawback of such an approach is that our tool can generate invalid mutants that do not compile, and sometimes fails to generate mutants that a parser-based tool would have produced. Additionally, it is incompatible with some approaches to improving the efficiency of mutation testing. However, the regexp-based approach provides multiple compensating advantages. First, our tool is easy to adapt to new languages; e.g., we present here the first mutation tool for Apple's Swift programming language. Second, the method makes handling multi-language programs and systems simple, because the same tool can support every language. Finally, our approach makes it easy for users to add custom, project-specific mutations.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {25–28},
numpages = {4},
keywords = {regular expressions, mutation testing, multi-language tools},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3202710.3203153,
author = {Tosun, Ayse and Ahmed, Muzamil and Turhan, Burak and Juristo, Natalia},
title = {On the Effectiveness of Unit Tests in Test-Driven Development},
year = {2018},
isbn = {9781450364591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202710.3203153},
doi = {10.1145/3202710.3203153},
abstract = {Background: Writing unit tests is one of the primary activities in test-driven development. Yet, the existing reviews report few evidence supporting or refuting the effect of this development approach on test case quality. Lack of ability and skills of developers to produce sufficiently good test cases are also reported as limitations of applying test-driven development in industrial practice. Objective: We investigate the impact of test-driven development on the effectiveness of unit test cases compared to an incremental test last development in an industrial context. Method: We conducted an experiment in an industrial setting with 24 professionals. Professionals followed the two development approaches to implement the tasks. We measure unit test effectiveness in terms of mutation score. We also measure branch and method coverage of test suites to compare our results with the literature. Results: In terms of mutation score, we have found that the test cases written for a test-driven development task have a higher defect detection ability than test cases written for an incremental test-last development task. Subjects wrote test cases that cover more branches on a test-driven development task compared to the other task. However, test cases written for an incremental test-last development task cover more methods than those written for the second task. Conclusion: Our findings are different from previous studies conducted at academic settings. Professionals were able to perform more effective unit testing with test-driven development. Furthermore, we observe that the coverage measure preferred in academic studies reveal different aspects of a development approach. Our results need to be validated in larger industrial contexts.},
booktitle = {Proceedings of the 2018 International Conference on Software and System Process},
pages = {113–122},
numpages = {10},
keywords = {test-driven development, empirical study, mutation score, code coverage, unit testing},
location = {Gothenburg, Sweden},
series = {ICSSP '18}
}

@inproceedings{10.1109/ESEM.2017.44,
author = {Bach, Thomas and Andrzejak, Artur and Pannemans, Ralf and Lo, David},
title = {The Impact of Coverage on Bug Density in a Large Industrial Software Project},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.44},
doi = {10.1109/ESEM.2017.44},
abstract = {Measuring quality of test suites is one of the major challenges of software testing. Code coverage identifies tested and untested parts of code and is frequently used to approximate test suite quality. Multiple previous studies have investigated the relationship between coverage ratio and test suite quality, without a clear consent in the results. In this work we study whether covered code contains a smaller number of future bugs than uncovered code (assuming appropriate scaling). If this correlation holds and bug density is lower in covered code, coverage can be regarded as a meaningful metric to estimate the adequacy of testing.To this end we analyse 16000 internal bug reports and bug-fixes of SAP HANA, a large industrial software project. We found that the above-mentioned relationship indeed holds, and is statistically significant. Contrary to most previous works our study uses real bugs and real bug-fixes. Furthermore, our data is derived from a complex and large industrial project.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {307–313},
numpages = {7},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3127041.3127049,
author = {Fellner, Andreas and Krenn, Willibald and Schlick, Rupert and Tarrach, Thorsten and Weissenbacher, Georg},
title = {Model-Based, Mutation-Driven Test Case Generation via Heuristic-Guided Branching Search},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3127049},
doi = {10.1145/3127041.3127049},
abstract = {This work introduces a heuristic-guided branching search algorithm for model-based, mutation-driven test case generation. The algorithm is designed towards the efficient and computationally tractable exploration of discrete, non-deterministic models with huge state spaces. Asynchronous parallel processing is a key feature of the algorithm. The algorithm is inspired by the successful path planning algorithm Rapidly exploring Random Trees (RRT). We adapt RRT in several aspects towards test case generation. Most notably, we introduce parametrized heuristics for start and successor state selection, as well as a mechanism to construct test cases from the data produced during search.We implemented our algorithm in the existing test case generation framework MoMuT. We present an extensive evaluation of our heuristics and parameters based on a diverse set of demanding models obtained in an industrial context. In total we continuously utilized 128 CPU cores on three servers for two weeks to gather the experimental data presented. Using statistical methods we determine which heuristics are performing well on all models. With our new algorithm, we are now able to process models consisting of over 2300 concurrent objects. To our knowledge there is no other mutation driven test case generation tool that is able to process models of this magnitude.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {56–66},
numpages = {11},
keywords = {parallel search, mutation testing, model-based testing, heuristics, test case generation, search-based testing},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/3106237.3106280,
author = {Brown, David Bingham and Vaughn, Michael and Liblit, Ben and Reps, Thomas},
title = {The Care and Feeding of Wild-Caught Mutants},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106280},
doi = {10.1145/3106237.3106280},
abstract = {Mutation testing of a test suite and a program provides a way to measure the quality of the test suite. In essence, mutation testing is a form of sensitivity testing: by running mutated versions of the program against the test suite, mutation testing measures the suite's sensitivity for detecting bugs that a programmer might introduce into the program. This paper introduces a technique to improve mutation testing that we call wild-caught mutants; it provides a method for creating potential faults that are more closely coupled with changes made by actual programmers. This technique allows the mutation tester to have more certainty that the test suite is sensitive to the kind of changes that have been observed to have been made by programmers in real-world cases.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {511–522},
numpages = {12},
keywords = {test suites, mutation testing, repository mining},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1145/3057269,
author = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
title = {Effective Regression Test Case Selection: A Systematic Literature Review},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057269},
doi = {10.1145/3057269},
abstract = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {29},
numpages = {32},
keywords = {Software testing, SLR, cost effectiveness, fault detection ability, coverage}
}

@inproceedings{10.1109/ICSE-NIER.2017.15,
author = {Gligoric, Milos and Khurshid, Sarfraz and Misailovic, Sasa and Shi, August},
title = {Mutation Testing Meets Approximate Computing},
year = {2017},
isbn = {9781538626757},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2017.15},
doi = {10.1109/ICSE-NIER.2017.15},
abstract = {One of the most widely studied techniques in software testing research is mutation testing - a technique for evaluating the quality of test suites. Despite over four decades of academic advances in this technique, mutation testing has not found its way to mainstream development. The key issue with mutation testing is its high computational cost: it requires running the test suite against not just the program under test but against typically thousands of mutants, i.e., syntactic variants, of the program. Our key insight is that exciting advances in the upcoming, yet unrelated, area of approximate computing allow us to define a principled approach that provides the benefits of traditional mutation testing at a fraction of its usually large cost.This paper introduces the idea of a novel approach, named ApproxiMut, that blends the power of mutation testing with the practicality of approximate computing. To demonstrate the potential of our approach, we present a concrete instantiation: rather than executing tests against each mutant on the exact program version, ApproxiMut obtains an approximate test/program version by applying approximate transformations and runs tests against each mutant on the approximated version. Our initial goal is to (1) measure the correlation between mutation scores on the exact and approximate program versions, (2) evaluate the relation among mutation operators and approximate transformations, (3) discover the best way to approximate a test and a program, and (4) evaluate the benefits of ApproxiMut. Our preliminary results show similar mutation scores on the exact and approximate program versions and uncovered a case when an approximated test was, to our surprise, better than the exact test.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track},
pages = {3–6},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {ICSE-NIER '17}
}

@inproceedings{10.1109/ICSE-SEIP.2017.15,
author = {Golagha, Mojdeh and Pretschner, Alexander and Fisch, Dominik and Nagy, Roman},
title = {Reducing Failure Analysis Time: An Industrial Evaluation},
year = {2017},
isbn = {9781538627174},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2017.15},
doi = {10.1109/ICSE-SEIP.2017.15},
abstract = {Testing and debugging automotive cyber physical systems are challenging. Developing and integrating cyber and physical components require extensive testing to ensure reliable and safe releases. One important cost factor in the debugging process is the time required to analyze failures. Since large number of failures usually happen due to a few underlying faults, clustering failures based on the responsible faults helps reduce analysis time. We focus on the software-in-the-loop and hardware-in-the-loop levels of testing where test execution times are high. We devise a methodology for adapting existing clustering techniques to a real context. We augment an existing clustering approach by a method for selecting representative tests. To analyze failures, rather than investigating all failing tests one by one, testers inspect only these representatives. We report on the results of a large scale industrial case study. We ran experiments on ca. 850 KLOC. Results show that utilizing our clustering tool, testers can reduce failure analysis time by more than 80%.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
pages = {293–302},
numpages = {10},
keywords = {HiL testing, SiL testing, automotive CPS, failure clustering, failure analysis},
location = {Buenos Aires, Argentina},
series = {ICSE-SEIP '17}
}

@inproceedings{10.1109/ICSE.2017.68,
author = {Rojas, Jos\'{e} Miguel and White, Thomas D. and Clegg, Benjamin S. and Fraser, Gordon},
title = {Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.68},
doi = {10.1109/ICSE.2017.68},
abstract = {Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program; automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {677–688},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2950290.2950324,
author = {Ahmed, Iftekhar and Gopinath, Rahul and Brindescu, Caius and Groce, Alex and Jensen, Carlos},
title = {Can Testedness Be Effectively Measured?},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950324},
doi = {10.1145/2950290.2950324},
abstract = { Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and deciding when to stop testing. Test the least-tested code, and stop when all code is well-tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite quality. The first measure is statement coverage, the simplest and best-known code coverage measure. The second measure is mutation score, a supposedly more powerful, though expensive, measure.  We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer future bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring testedness. Using a large number of open source Java programs from Github and Apache, we show that both statement coverage and mutation score have only a weak negative correlation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Program elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {547–558},
numpages = {12},
keywords = {statistical analysis, mutation testing, test suite evaluation, coverage criteria},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/2931037.2931038,
author = {Zhang, Jie and Wang, Ziyi and Zhang, Lingming and Hao, Dan and Zang, Lei and Cheng, Shiyang and Zhang, Lu},
title = {Predictive Mutation Testing},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931038},
doi = {10.1145/2931037.2931038},
abstract = { Mutation testing is a powerful methodology for evaluating test suite quality. In mutation testing, a large number of mutants are generated and executed against the test suite to check the ratio of killed mutants. Therefore, mutation testing is widely believed to be a computationally expensive technique. To alleviate the efficiency concern of mutation testing, in this paper, we propose predictive mutation testing (PMT), the first approach to predicting mutation testing results without mutant execution. In particular, the proposed approach constructs a classification model based on a series of features related to mutants and tests, and uses the classification model to predict whether a mutant is killed or survived without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (i.e., cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss when predicting mutant execution results, indicating a good tradeoff between efficiency and effectiveness of mutation testing. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {342–353},
numpages = {12},
keywords = {software testing, machine learning, mutation testing},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/2931037.2931067,
author = {Jabbarvand, Reyhaneh and Sadeghi, Alireza and Bagheri, Hamid and Malek, Sam},
title = {Energy-Aware Test-Suite Minimization for Android Apps},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931067},
doi = {10.1145/2931037.2931067},
abstract = { The rising popularity of mobile apps deployed on battery-constrained devices has motivated the need for effective energy-aware testing techniques. Energy testing is generally more labor intensive and expensive than functional testing, as tests need to be executed in the deployment environment and specialized equipment needs to be used to collect energy measurements. Currently, there is a dearth of automatic mobile testing techniques that consider energy as a program property of interest. This paper presents an energy-aware test-suite minimization approach to significantly reduce the number of tests needed to effectively test the energy properties of an Android app. It relies on an energy-aware coverage criterion that indicates the degree to which energy-greedy segments of a program are tested. We describe and evaluate two complementary algorithms for test-suite minimization. Experiments over test suites provided for real-world apps have corroborated our ability to reduce the test suite size by 84% on average, while maintaining the effectiveness of test suite in revealing the great majority of energy bugs. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {425–436},
numpages = {12},
keywords = {Green software engineering, Test-suite minimization, Coverage criterion, Android},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/2915970.2915999,
author = {Khalsa, Sunint Kaur and Labiche, Yvan and Nicoletta, Johanna},
title = {The Power of Single and Error Annotations in Category Partition Testing: An Experimental Evaluation},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915999},
doi = {10.1145/2915970.2915999},
abstract = {Category Partition (CP) is a black box testing technique that formalizes the specification of the input domain in a CP specification for the system under test. A CP specification is driven by tester's expertise and bundles parameters, categories (characteristics of parameters) and choices (acceptable values for categories) required for extensively testing the system. For completeness the choices correspond to permitted input values as well as some values to account for boundaries or robustness. These choices are then combined to form test frames on the basis of various criteria such as each choice or pairwise. To ensure that the combinations of choices are feasible and account for valid sets of user requirements, constraints are introduced to specify permitted combinations among choices, and to specify single or error choices. In a typical development environment where testing is driven by stringent deadlines a tester might have to decide how many constraints are enough to attain the maximum level of test completeness. The present work will assist a test engineer in making this decision. We conclude, on the basis of our experimental evaluation on academic and industrial case studies, that an equally effective test suite can be attained by meticulously defining error and single annotations in a CP specification while ignoring other constraints among choices.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {28},
numpages = {10},
keywords = {category partition, constraints, error annotations, single annotations, each choice, pairwise},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2915970.2915992,
author = {Parsai, Ali and Murgia, Alessandro and Demeyer, Serge},
title = {Evaluating Random Mutant Selection at Class-Level in Projects with Non-Adequate Test Suites},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915992},
doi = {10.1145/2915970.2915992},
abstract = {Mutation testing is a standard technique to evaluate the quality of a test suite. Due to its computationally intensive nature, many approaches have been proposed to make this technique feasible in real case scenarios. Among these approaches, uniform random mutant selection has been demonstrated to be simple and promising. However, works on this area analyze mutant samples at project level mainly on projects with adequate test suites. In this paper, we fill this lack of empirical validation by analyzing random mutant selection at class level on projects with non-adequate test suites. First, we show that uniform random mutant selection underachieves the expected results. Then, we propose a new approach named weighted random mutant selection which generates more representative mutant samples. Finally, we show that representative mutant samples are larger for projects with high test adequacy.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {11},
numpages = {10},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2896921.2896923,
author = {Felbinger, Hermann and Wotawa, Franz and Nica, Mihai},
title = {Empirical Study of Correlation between Mutation Score and Model Inference Based Test Suite Adequacy Assessment},
year = {2016},
isbn = {9781450341516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896921.2896923},
doi = {10.1145/2896921.2896923},
abstract = {In this paper we investigate a method for test suite evaluation that is based on an inferred model from the test suite. The idea is to use the similarity between the inferred model and the system under test as a measure of test suite adequacy, which is the ability of a test suite to expose errors in the system under test. We define similarity using the root mean squared error computed from the differences of the system under test output and the model output for certain inputs not used for model inference. In the paper we introduce the approach and provide results of an experimental evaluation where we compare the similarity with the mutation score. We used the Pearson Correlation coefficient to calculate whether a linear correlation between mutation score and root mean squared error exists. As a result we obtain that in certain cases the computed similarity strongly correlates with the mutation score.},
booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
pages = {43–49},
numpages = {7},
keywords = {software test, machine learning, mutation score},
location = {Austin, Texas},
series = {AST '16}
}

@article{10.1145/2660767,
author = {Gligoric, Milos and Groce, Alex and Zhang, Chaoqiang and Sharma, Rohan and Alipour, Mohammad Amin and Marinov, Darko},
title = {Guidelines for Coverage-Based Comparisons of Non-Adequate Test Suites},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2660767},
doi = {10.1145/2660767},
abstract = {A fundamental question in software testing research is how to compare test suites, often as a means for comparing test-generation techniques that produce those test suites. Researchers frequently compare test suites by measuring their coverage. A coverage criterion C provides a set of test requirements and measures how many requirements a given suite satisfies. A suite that satisfies 100% of the feasible requirements is called C-adequate. Previous rigorous evaluations of coverage criteria mostly focused on such adequate test suites: given two criteria C and C′, are C-adequate suites on average more effective than C′-adequate suites? However, in many realistic cases, producing adequate suites is impractical or even impossible.This article presents the first extensive study that evaluates coverage criteria for the common case of non-adequate test suites: given two criteria C and C′, which one is better to use to compare test suites? Namely, if suites T1, T2,…,Tn have coverage values c1, c2,…,cn for C and c1′, c2′,…,cn′ for C′, is it better to compare suites based on c1, c2,…,cn or based on c1′, c2′,…,cn′? We evaluate a large set of plausible criteria, including basic criteria such as statement and branch coverage, as well as stronger criteria used in recent studies, including criteria based on program paths, equivalence classes of covered statements, and predicate states. The criteria are evaluated on a set of Java and C programs with both manually written and automatically generated test suites. The evaluation uses three correlation measures. Based on these experiments, two criteria perform best: branch coverage and an intraprocedural acyclic path coverage. We provide guidelines for testing researchers aiming to evaluate test suites using coverage criteria as well as for other researchers evaluating coverage criteria for research use.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {22},
numpages = {33},
keywords = {non-adequate test suites, Coverage criteria}
}

@inproceedings{10.1145/2786805.2786825,
author = {Smith, Edward K. and Barr, Earl T. and Le Goues, Claire and Brun, Yuriy},
title = {Is the Cure Worse than the Disease? Overfitting in Automated Program Repair},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786825},
doi = {10.1145/2786805.2786825},
abstract = { Automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TrpAutoRepair, on a publicly available benchmark of bugs, each with a human-written patch. By evaluating patches using tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs that pass most tests, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair performs no worse than these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, and starting program quality, as well as the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with a test suite independent from the one used for patch generation. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {532–543},
numpages = {12},
keywords = {GenProg, automated program repair, independent evaluation, IntroClass, TrpAutoRepair, empirical evaluation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2771783.2771810,
author = {Clapp, Lazaro and Anand, Saswat and Aiken, Alex},
title = {Modelgen: Mining Explicit Information Flow Specifications from Concrete Executions},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771810},
doi = {10.1145/2771783.2771810},
abstract = { We present a technique to mine explicit information flow specifications from concrete executions. These specifications can be consumed by a static taint analysis, enabling static analysis to work even when method definitions are missing or portions of the program are too difficult to analyze statically (e.g., due to dynamic features such as reflection). We present an implementation of our technique for the Android platform. When compared to a set of manually written specifications for 309 methods across 51 classes, our technique is able to recover 96.36% of these manual specifications and produces many more correct annotations that our manual models missed. We incorporate the generated specifications into an existing static taint analysis system, and show that they enable it to find additional true flows. Although our implementation is Android-specific, our approach is applicable to other application frameworks. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {129–140},
numpages = {12},
keywords = {information flow, specification mining, Dynamic analysis},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.5555/2819009.2819196,
author = {Zhang, Jie},
title = {Scalability Studies on Selective Mutation Testing},
year = {2015},
publisher = {IEEE Press},
abstract = {Mutation testing is a test method which is designed to evaluate a test suite's quality. Due to the expensive cost of mutation testing, selective mutation testing was first proposed in 1991 by Mathur, in which a subset of mutants are selected aiming to achieve the same effectiveness as the whole set of mutants in evaluating the quality of test suites. Though selective mutation testing has been widely investigated in recent years, many people still doubt if it can suit well for large programs. Realizing that none of the existing work has systematically studied the scalability of selective mutation testing, I plan to work on the scalability of selective mutation testing through several studies.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {851–854},
numpages = {4},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.5555/2820282.2820298,
author = {Hauptmann, Benedikt and Juergens, Elmar and Woinke, Volkmar},
title = {Generating Refactoring Proposals to Remove Clones from Automated System Tests},
year = {2015},
publisher = {IEEE Press},
abstract = {Automated system tests often have many clones, which make them complex to understand and costly to maintain. Unfortunately, removing clones is challenging as there are numerous possibilities of how to refactor them to reuse components such as subroutines. Additionally, clones often overlap partly which makes it particularly difficult to decide which parts to extract. If done wrongly, reuse potential is not leveraged optimally and structures between tests and reuse components will become unnecessarily complex. We present a method to support test engineers in extracting overlapping clones. Using grammar inference algorithms, we generate a refactoring proposal that demonstrates test engineers how overlapping clones can be extracted. Furthermore, we visualize the generated refactoring proposal to make it easily understandable for test engineers. An industrial case study demonstrates that our approach helps test engineers to gain information of the reuse potential of test suites and guides them to perform refactorings.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {115–124},
numpages = {10},
keywords = {test clones, refactoring, automated testing},
location = {Florence, Italy},
series = {ICPC '15}
}

@inproceedings{10.5555/2819261.2819272,
author = {Khalili, Ali and Narizzano, Massimo and Tacchella, Armando and Giunchiglia, Enrico},
title = {Automatic Test-Pattern Generation for Grey-Box Programs},
year = {2015},
publisher = {IEEE Press},
abstract = {In the context of structural testing, automatic test-pattern generation (ATPG) may fail to provide suites covering 100% of the testing requirements for grey-box programs, i.e., applications wherein source code is available for some parts (white-box), but not for others (black-box). Furthermore, test suites based on abstract models may elicit behaviors on the actual program that diverge from the intended ones. In this paper, we present a new ATPG methodology to reduce divergence without increasing manual effort. This is achieved by (i) learning models of black-box components as finite-state machines, and (ii) composing the learnt models with the white-box components to generate test-suites for the grey-box program. Experiments with a prototypical implementation of our methodology show that it yields measurable improvements over two comparable state-of-the-art solutions.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {33–37},
numpages = {5},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.5555/2821464.2821469,
author = {Daoudagh, Said and Lonetti, Francesca and Marchetti, Eda},
title = {Assessment of Access Control Systems Using Mutation Testing},
year = {2015},
publisher = {IEEE Press},
abstract = {In modern pervasive applications, it is important to validate access control mechanisms that are usually defined by means of the standard XACML language. Mutation analysis has been applied on access control policies for measuring the adequacy of a test suite. In this paper, we present a testing framework aimed at applying mutation analysis at the level of the Java based policy evaluation engine. A set of Java based mutation operators is selected and applied to the code of the Policy Decision Point (PDP). A first experiment shows the effectiveness of the proposed framework in assessing the fault detection of XACML test suites and confirms the efficacy of the application of code-based mutation operators to the PDP.},
booktitle = {Proceedings of the First International Workshop on TEchnical and LEgal Aspects of Data PRIvacy},
pages = {8–13},
numpages = {6},
location = {Florence, Italy},
series = {TELERISE '15}
}

@inproceedings{10.1145/2676723.2677300,
author = {Shams, Zalia and Edwards, Stephen H.},
title = {Checked Coverage and Object Branch Coverage: New Alternatives for Assessing Student-Written Tests},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677300},
doi = {10.1145/2676723.2677300},
abstract = {Many educators currently use code coverage metrics to assess student-written software tests. While test adequacy criteria such as statement or branch coverage can also be used to measure the thoroughness of a test suite, they have limitations. Coverage metrics assess what percentage of code has been exercised, but do not depend on whether a test suite adequately checks that the expected behavior is achieved. This paper evaluates checked coverage, an alternative measure of test thoroughness aimed at overcoming this limitation, along with object branch coverage, a structure code coverage metric that has received little discussion in educational assessment. Checked coverage works backwards from behavioral assertions in test cases, measuring the dynamic slice of the executed code that actually influences the outcome of each assertion. Object branch coverage (OBC) is a stronger coverage criterion similar to weak variants of modified condition/decision coverage. We experimentally compare checked coverage and OBC against statement coverage, branch coverage, mutation analysis, and all-pairs testing to evaluate which is the best predictor of how likely a test suite is to detect naturally occurring defects. While checked coverage outperformed other coverage measures in our experiment, followed closely by OBC, both were only weakly correlated with a test suite's ability to detect naturally occurring defects produced by students in the final versions of their programs. Still, OBC appears to be an improved and practical alternative to existing statement and branch coverage measures, while achieving nearly the same benefits as checked coverage.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {534–539},
numpages = {6},
keywords = {checked coverage, statement coverage, branch coverage, software testing, mutation testing, automated grading, programming assignments, automated assessment, modified condition/decision coverage, test quality, test metrics, test coverage},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@article{10.1145/2693208.2693226,
author = {Bokil, Prasad and Krishnan, Padmanabhan and Venkatesh, R.},
title = {Achieving Effective Test Suites for Reactive Systems Using Specification Mining and Test Suite Reduction Techniques},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2693208.2693226},
doi = {10.1145/2693208.2693226},
abstract = {Failures in reactive embedded systems are often unacceptable. Moreover, effective testing of such systems to detect potential critical failures is a difficult task.We present an automated black box test suite generation technique for reactive systems. The technique is based on dynamic mining of specifications, in form of a finite state machine (FSM), from initial runs. The set of test cases thus produced contain several redundant test cases, many of which are eliminated by a simple greedy test suite reduction algorithm to give the final test suite. The effectiveness of tests generated by our technique was evaluated using five case studies from the reactive embedded domain. Results indicate that a test suite generated by our technique is promising in terms of effectiveness and scalability. While the test suite reduction algorithm removes redundant test cases, the change in effectiveness of test suites due to this reduction is examined in the experimentation.We present our specification mining based test suite generation technique, the test suite reduction technique and results on industrial case studies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–8},
numpages = {8},
keywords = {test suite reduction, specification mining, black box testing}
}

@article{10.1145/2656201,
author = {Li, Kaituo and Reichenbach, Christoph and Csallner, Christoph and Smaragdakis, Yannis},
title = {Residual Investigation: Predictive and Precise Bug Detection},
year = {2014},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2656201},
doi = {10.1145/2656201},
abstract = {We introduce the concept of residual investigation for program analysis. A residual investigation is a dynamic check installed as a result of running a static analysis that reports a possible program error. The purpose is to observe conditions that indicate whether the statically predicted program fault is likely to be realizable and relevant. The key feature of a residual investigation is that it has to be much more precise (i.e., with fewer false warnings) than the static analysis alone, yet significantly more general (i.e., reporting more errors) than the dynamic tests in the program's test suite that are pertinent to the statically reported error. That is, good residual investigations encode dynamic conditions that, when considered in conjunction with the static error report, increase confidence in the existence or severity of an error without needing to directly observe a fault resulting from the error.We enhance the static analyzer FindBugs with several residual investigations appropriately tuned to the static error patterns in FindBugs, and apply it to nine large open-source systems and their native test suites. The result is an analysis with a low occurrence of false warnings (false positives) while reporting several actual errors that would not have been detected by mere execution of a program's test suite.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {7},
numpages = {32},
keywords = {False warnings, existing test cases, RFBI}
}

@inproceedings{10.1145/2635868.2635929,
author = {Just, Ren\'{e} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon},
title = {Are Mutants a Valid Substitute for Real Faults in Software Testing?},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635929},
doi = {10.1145/2635868.2635929},
abstract = { A good test suite is one that detects real faults. Because the set of faults in a program is usually unknowable, this definition is not useful to practitioners who are creating test suites, nor to researchers who are creating and evaluating tools that generate test suites. In place of real faults, testing research often uses mutants, which are artificial faults -- each one a simple syntactic variation -- that are systematically seeded throughout the program under test. Mutation analysis is appealing because large numbers of mutants can be automatically-generated and used to compensate for low quantities or the absence of known real faults. Unfortunately, there is little experimental evidence to support the use of mutants as a replacement for real faults. This paper investigates whether mutants are indeed a valid substitute for real faults, i.e., whether a test suite’s ability to detect mutants is correlated with its ability to detect real faults that developers have fixed. Unlike prior studies, these investigations also explicitly consider the conflating effects of code coverage on the mutant detection rate. Our experiments used 357 real faults in 5 open-source applications that comprise a total of 321,000 lines of code. Furthermore, our experiments used both developer-written and automatically-generated test suites. The results show a statistically significant correlation between mutant detection and real fault detection, independently of code coverage. The results also give concrete suggestions on how to improve mutation analysis and reveal some inherent limitations. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {654–665},
numpages = {12},
keywords = {Test effectiveness, mutation analysis, code coverage, real faults},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2661136.2661157,
author = {Groce, Alex and Alipour, Mohammad Amin and Gopinath, Rahul},
title = {Coverage and Its Discontents},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661157},
doi = {10.1145/2661136.2661157},
abstract = {Everyone wants to know one thing about a test suite: will it detect enough bugs? Unfortunately, in most settings that matter, answering this question directly is impractical or impossible. Software engineers and researchers therefore tend to rely on various measures of code coverage (where mutation testing is considered a form of syntactic coverage). A long line of academic research efforts have attempted to determine whether relying on coverage as a substitute for fault detection is a reasonable solution to the problems of test suite evaluation. This essay argues that the profusion of coverage-related literature is in part a sign of an underlying uncertainty as to what exactly it is that measuring coverage should achieve, as well as how we would know if it can, in fact, achieve it. We propose some solutions and mitigations, but the primary focus of this essay is to clarify the state of current confusions regarding this key problem for effective software testing.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {255–268},
numpages = {14},
keywords = {testing, coverage, evaluation},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@inproceedings{10.1145/2652524.2652535,
author = {Herzig, Kim and Nagappan, Nachiappan},
title = {The Impact of Test Ownership and Team Structure on the Reliability and Effectiveness of Quality Test Runs},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652535},
doi = {10.1145/2652524.2652535},
abstract = {Context: Software testing is a crucial step in most software development processes. Testing software is a key component to manage and assess the risk of shipping quality products to customers. But testing is also an expensive process and changes to the system need to be tested thoroughly which may take time. Thus, the quality of a software product depends on the quality of its underlying testing process and on the effectiveness and reliability of individual test cases.Goal: In this paper, we investigate the impact of the organizational structure of test owners on the reliability and effectiveness of the corresponding test cases. Prior empirical research on organizational structure has focused only on developer activity. We expand the scope of empirical knowledge by assessing the impact of organizational structure on testing activities.Method: We performed an empirical study on the Windows build verification test suites (BVT) and relate effectiveness and reliability measures of each test run to the complexity and size of the organizational sub-structure that enclose all owners of test cases executed.Results: Our results show, that organizational structure impacts both test effectiveness and test execution reliability. We are also able to predict effectiveness and reliability with fairly high precision and recall values.Conclusion: We suggest to review test suites with respect to their organizational composition. As indicated by the results of this study, this would increase the effectiveness and reliability, development speed and developer satisfaction.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {2},
numpages = {10},
keywords = {software testing, reliability, effectiveness, empirical software engineering, organizational structure},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1145/2642937.2642940,
author = {Giannakopoulou, Dimitra and Howar, Falk and Isberner, Malte and Lauderdale, Todd and Rakamari\'{c}, Zvonimir and Raman, Vishwanath},
title = {Taming Test Inputs for Separation Assurance},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642940},
doi = {10.1145/2642937.2642940},
abstract = {The Next Generation Air Transportation System (NextGen) advocates the use of innovative algorithms and software to address the increasing load on air-traffic control. AutoResolver [12] is a large, complex NextGen component that provides separation assurance between multiple airplanes up to 20 minutes ahead of time. Our work targets the development of a light-weight, automated testing environment for AutoResolver. The input space of AutoResolver consists of airplane trajectories, each trajectory being a sequence of hundreds of points in the three-dimensional space. Generating meaningful test cases for AutoResolver that cover its behavioral space to a satisfactory degree is a major challenge. We discuss how we tamed this input space to make it amenable to test case generation techniques, as well as how we developed and validated an extensible testing environment around AutoResolver.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {373–384},
numpages = {12},
keywords = {test case generation, test coverage, air-traffic control},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/2610384.2610388,
author = {Just, Ren\'{e} and Ernst, Michael D. and Fraser, Gordon},
title = {Efficient Mutation Analysis by Propagating and Partitioning Infected Execution States},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610388},
doi = {10.1145/2610384.2610388},
abstract = { Mutation analysis evaluates a testing technique by measur- ing how well it detects seeded faults (mutants). Mutation analysis is hampered by inherent scalability problems — a test suite is executed for each of a large number of mutants. Despite numerous optimizations presented in the literature, this scalability issue remains, and this is one of the reasons why mutation analysis is hardly used in practice. Whereas most previous optimizations attempted to stati- cally reduce the number of executions or their computational overhead, this paper exploits information available only at run time to further reduce the number of executions. First, state infection conditions can reveal — with a single test execution of the unmutated program — which mutants would lead to a different state, thus avoiding unnecessary test executions. Second, determining whether an infected execution state propagates can further reduce the number of executions. Mutants that are embedded in compound expressions may infect the state locally without affecting the outcome of the compound expression. Third, those mutants that do infect the state can be partitioned based on the resulting infected state — if two mutants lead to the same infected state, only one needs to be executed as the result of the other can be inferred. We have implemented these optimizations in the Major mu- tation framework and empirically evaluated them on 14 open source programs. The optimizations reduced the mutation analysis time by 40% on average. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {315–326},
numpages = {12},
keywords = {software testing, Mutation analysis, dynamic analysis},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/2610384.2610406,
author = {Mirzaaghaei, Mehdi and Mesbah, Ali},
title = {DOM-Based Test Adequacy Criteria for Web Applications},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610406},
doi = {10.1145/2610384.2610406},
abstract = { To assess the quality of web application test cases, web developers currently measure code coverage. Although code coverage has traditionally been a popular test adequacy criterion, we believe it alone is not adequate for assessing the quality of web application test cases. We propose a set of novel DOM-based test adequacy criteria for web applications. These criteria aim at measuring coverage at two granularity levels, (1) the percentage of DOM states and transitions covered in the total state space of the web application under test, and (2) the percentage of elements covered in each particular DOM state. We present a technique and tool, called DomCovery, which automatically extracts and measures the proposed adequacy criteria and generates a visual DOM coverage report. Our evaluation shows that there is no correlation between code coverage and DOM coverage. A controlled experiment illustrates that participants using DomCovery completed coverage related tasks 22% more accurately and 66% faster. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {71–81},
numpages = {11},
keywords = {web applications, coverage, Test adequacy criteria, DOM},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

